# üöÄ AWS Cloud Workshop ‚Äî Data Pipeline & Serverless Architecture
### ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô LAB ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á 4 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå ¬∑ Free Tier ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

> **Prerequisite:** ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô EC2 ‡πÅ‡∏•‡∏∞ Lambda + API Gateway ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß

---

## üìã ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£

‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏û‡∏≤‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á **Data Pipeline ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥** ‡πÅ‡∏ö‡∏ö Serverless ‡πÄ‡∏ï‡πá‡∏°‡∏ï‡∏±‡∏ß ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ Server ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå ‚Üí ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‚Üí ‡∏Ñ‡∏¥‡∏ß ‚Üí Orchestrate ‚Üí ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô Project ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Ñ‡∏∑‡∏≠ **"CSV Report Processor"** ‚Äî ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡∏∂‡πâ‡∏ô S3 ‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏≤‡∏á Email ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

| ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå | ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ | Services ‡∏´‡∏•‡∏±‡∏Å | ‡πÄ‡∏ß‡∏•‡∏≤‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì |
|--------|--------|---------------|--------------|
| Week 1 | Event-Driven with S3 + Lambda | S3, Lambda, IAM | 2‚Äì3 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á |
| Week 2 | Message Queue with SQS | SQS, Lambda | 2‚Äì3 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á |
| Week 3 | Orchestration with Step Functions | Step Functions, Lambda | 2‚Äì3 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á |
| Week 4 | Notification & Recap | SNS, CloudWatch, Architecture Review | 2‚Äì3 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á |

### üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
- ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏°‡∏µ **Data Pipeline ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥** ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå ‚Üí ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‚Üí ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ Server ‡πÄ‡∏•‡∏¢
- ‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô **$0** ‡∏ï‡∏•‡∏≠‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ (AWS Free Tier ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Pattern ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: **Event-Driven, Decoupling, Orchestration, Observability**
- ‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏™‡∏π‡πà Production Pipeline ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏î‡πâ

### üõ†Ô∏è ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°
- AWS Account (Free Tier)
- Python 3.x ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
- VS Code ‡∏´‡∏£‡∏∑‡∏≠ Editor ‡∏ó‡∏µ‡πà‡∏ñ‡∏ô‡∏±‡∏î
- ‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö (‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢‡πÜ)
- Email ‡∏à‡∏£‡∏¥‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ö SNS Notification (Week 4)

### ‚ö†Ô∏è Free Tier Limits ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏£‡∏π‡πâ
| Service | Free Tier |
|---------|-----------|
| S3 | 5 GB storage, 20,000 GET, 2,000 PUT /‡πÄ‡∏î‡∏∑‡∏≠‡∏ô |
| Lambda | 1 million invocations, 400,000 GB-seconds /‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡∏ï‡∏•‡∏≠‡∏î‡∏ä‡∏µ‡∏û |
| SQS | 1 million requests /‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡∏ï‡∏•‡∏≠‡∏î‡∏ä‡∏µ‡∏û |
| Step Functions | 4,000 state transitions /‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡∏ï‡∏•‡∏≠‡∏î‡∏ä‡∏µ‡∏û |
| SNS | 1 million publishes, 1,000 email deliveries /‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡∏ï‡∏•‡∏≠‡∏î‡∏ä‡∏µ‡∏û |
| CloudWatch | 5 GB log ingest, 10 custom metrics /‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (12 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÅ‡∏£‡∏Å) |

---

## Architecture Overview (Final ‚Äî ‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏£‡∏ö 4 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå)

```
[User] ‚îÄ‚îÄUpload CSV‚îÄ‚îÄ> [S3: raw-uploads/]
                              ‚îÇ
                    S3 Event Trigger
                              ‚îÇ
                              ‚ñº
                     [Lambda: Validator]
                              ‚îÇ
                    ‡∏™‡πà‡∏á Message ‡πÄ‡∏Ç‡πâ‡∏≤ Queue
                              ‚îÇ
                              ‚ñº
                       [SQS Queue]
                              ‚îÇ
                  SQS Trigger (Polling)
                              ‚îÇ
                              ‚ñº
                  [Step Functions: Workflow]
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  1. Parse CSV    ‚îÇ
                    ‚îÇ  2. Transform    ‚îÇ
                    ‚îÇ  3. Save Result  ‚îÇ
                    ‚îÇ  4. Notify       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚ñº                   ‚ñº
           [S3: processed/]      [SNS Topic]
                                        ‚îÇ
                                        ‚ñº
                                 [Email / SMS]

[CloudWatch] ‚îÄ‚îÄ‚îÄ‚îÄ Monitors ‚îÄ‚îÄ‚îÄ‚îÄ> ‡∏ó‡∏∏‡∏Å Service
```

---

---

# üìÖ WEEK 1 ‚Äî Event-Driven with S3 + Lambda

> **‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Trigger ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÄ‡∏°‡∏∑‡πà‡∏≠ Upload ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏∂‡πâ‡∏ô S3

---

## üéì ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
- ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Event-Driven Architecture ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Polling ‡πÑ‡∏î‡πâ
- ‡∏™‡∏£‡πâ‡∏≤‡∏á S3 Bucket ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Event Notification ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Lambda
- ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Lambda ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö S3 Event ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå CSV
- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ IAM Role ‡πÉ‡∏´‡πâ Lambda ‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏° Least Privilege
- ‡∏î‡∏π CloudWatch Logs ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Debug Lambda

---

## üìñ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (Lecture ‚Äî 30 ‡∏ô‡∏≤‡∏ó‡∏µ)

### Event-Driven Architecture ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö "‡∏ñ‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡πÜ" ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏´‡∏° (Polling) Event-Driven Architecture ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö "‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô" ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ

**‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:**
- **Polling:** ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÇ‡∏ó‡∏£‡∏ñ‡∏≤‡∏°‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏∏‡∏Å 5 ‡∏ô‡∏≤‡∏ó‡∏µ‡∏ß‡πà‡∏≤‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏¢‡∏±‡∏á
- **Event-Driven:** ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡∏£‡πâ‡∏≤‡∏ô‡πÇ‡∏ó‡∏£‡∏´‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏û‡∏£‡πâ‡∏≠‡∏°

**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Event-Driven:**
- ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Resource ‚Äî ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
- Scale ‡πÑ‡∏î‡πâ‡∏î‡∏µ ‚Äî ‡∏´‡∏•‡∏≤‡∏¢ Event ‡πÄ‡∏Å‡∏¥‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ
- Loose Coupling ‚Äî ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏Å‡∏±‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
- ‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≥ ‚Äî Lambda ‡∏à‡πà‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á

### S3 Events ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?
S3 ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á Notification ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Lambda, SQS, ‡∏´‡∏£‡∏∑‡∏≠ SNS ‡πÑ‡∏î‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÄ‡∏°‡∏∑‡πà‡∏≠:
- `s3:ObjectCreated:*` ‚Äî ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å Upload
- `s3:ObjectRemoved:*` ‚Äî ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡∏•‡∏ö
- `s3:ObjectRestore:*` ‚Äî ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å Restore ‡∏à‡∏≤‡∏Å Glacier

### Project ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô Week 1

```
[‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ Upload sales_data.csv] 
        ‚îÇ
        ‚ñº
[S3 Bucket: raw-uploads/]
        ‚îÇ
  S3 Event Trigger (ObjectCreated)
        ‚îÇ
        ‚ñº
[Lambda: csv-validator]
   - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô .csv ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°
   - ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows
   - Log ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô CloudWatch
```

---

## üîß LAB Step-by-Step

---

### ‚úÖ STEP 1 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á S3 Buckets

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á 2 Buckets ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô:

**Bucket 1: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏î‡∏¥‡∏ö**

1. ‡πÄ‡∏Ç‡πâ‡∏≤ **AWS Console** ‚Üí ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ **S3** ‚Üí **Create bucket**
2. ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
   ```
   Bucket name: pipeline-raw-[‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì]-[random4digits]
   Region: ap-southeast-1 (Singapore)
   Block all public access: ‚úÖ ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (default)
   Versioning: Disable
   ```
3. ‡∏Å‡∏î **Create bucket**

**Bucket 2: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå**

1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Bucket ‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏≠‡∏±‡∏ô:
   ```
   Bucket name: pipeline-processed-[‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì]-[random4digits]
   Region: ap-southeast-1 (Singapore)
   (Settings ‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô)
   ```

**‡∏™‡∏£‡πâ‡∏≤‡∏á Folder Structure ‡πÉ‡∏ô Bucket ‡πÅ‡∏£‡∏Å:**

1. ‡∏Å‡∏î‡πÄ‡∏Ç‡πâ‡∏≤ `pipeline-raw-...`
2. ‡∏Å‡∏î **Create folder** ‚Üí ‡∏ä‡∏∑‡πà‡∏≠ `uploads/` ‚Üí Create
3. ‡∏Å‡∏î **Create folder** ‡∏≠‡∏µ‡∏Å‡∏≠‡∏±‡∏ô ‚Üí ‡∏ä‡∏∑‡πà‡∏≠ `failed/` ‚Üí Create

> üí° **‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏¢‡∏Å Bucket?** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Recursive Loop ‚Äî ‡∏ñ‡πâ‡∏≤ Lambda ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Bucket ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏ô Write ‡∏•‡∏á ‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î Event Loop ‡πÑ‡∏°‡πà‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏û‡∏∏‡πà‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ!

---

### ‚úÖ STEP 2 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á IAM Role ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Lambda

Lambda ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á S3 ‚Äî ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö Least Privilege

1. ‡πÄ‡∏Ç‡πâ‡∏≤ **IAM** ‚Üí **Roles** ‚Üí **Create role**
2. **Trusted entity:** AWS service ‚Üí **Lambda** ‚Üí Next
3. **‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Policy ‡∏Å‡πà‡∏≠‡∏ô** (‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ Managed Policy ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÜ):
   - ‡∏Ñ‡∏•‡∏¥‡∏Å **Create policy** (‡πÄ‡∏õ‡∏¥‡∏î Tab ‡πÉ‡∏´‡∏°‡πà)
   - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **JSON** tab ‚Üí ‡∏ß‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "ReadFromRawBucket",
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:HeadObject"
      ],
      "Resource": "arn:aws:s3:::pipeline-raw-[‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì]-*/*"
    },
    {
      "Sid": "WriteToProcessedBucket",
      "Effect": "Allow",
      "Action": [
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::pipeline-processed-[‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì]-*/*"
    },
    {
      "Sid": "CloudWatchLogs",
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "arn:aws:logs:*:*:*"
    }
  ]
}
```

4. Policy name: `pipeline-lambda-policy` ‚Üí **Create policy**
5. ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤ Create Role ‚Üí Refresh ‚Üí ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ `pipeline-lambda-policy` ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‚Üí Next
6. Role name: `pipeline-lambda-role` ‚Üí **Create role**

> üîí **Security Note:** ‡πÄ‡∏£‡∏≤‡∏£‡∏∞‡∏ö‡∏∏ Resource ARN ‡πÅ‡∏ö‡∏ö‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ `"Resource": "*"` ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ñ‡πâ‡∏≤ Lambda ‡∏ñ‡∏π‡∏Å Compromise ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤ Resource ‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏î‡πâ

---

### ‚úÖ STEP 3 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á Lambda Function

1. ‡πÄ‡∏Ç‡πâ‡∏≤ **Lambda** ‚Üí **Create function**
2. ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
   ```
   Function name: csv-validator
   Runtime: Python 3.12
   Architecture: x86_64
   Execution role: Use an existing role ‚Üí pipeline-lambda-role
   ```
3. ‡∏Å‡∏î **Create function**
4. ‡πÉ‡∏ô **Code** tab ‚Üí ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏î‡πâ‡∏ß‡∏¢:

```python
import json
import boto3
import csv
import io
import logging
from datetime import datetime

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Logger
logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client('s3')

def lambda_handler(event, context):
    """
    ‡∏£‡∏±‡∏ö S3 Event ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å Upload
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô CSV ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞ Log ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    """
    
    logger.info(f"Received event: {json.dumps(event)}")
    
    results = []
    
    for record in event['Records']:
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Event
        bucket_name = record['s3']['bucket']['name']
        object_key = record['s3']['object']['key']
        file_size = record['s3']['object']['size']
        event_time = record['eventTime']
        
        logger.info(json.dumps({
            "event": "file_received",
            "bucket": bucket_name,
            "key": object_key,
            "size_bytes": file_size,
            "timestamp": event_time
        }))
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏ü‡∏•‡πå
        if not object_key.lower().endswith('.csv'):
            logger.warning(f"File {object_key} is not a CSV file. Skipping.")
            results.append({
                "file": object_key,
                "status": "rejected",
                "reason": "Not a CSV file"
            })
            continue
        
        # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å S3
        try:
            response = s3.get_object(Bucket=bucket_name, Key=object_key)
            content = response['Body'].read().decode('utf-8')
            
            # Parse CSV
            csv_reader = csv.DictReader(io.StringIO(content))
            rows = list(csv_reader)
            headers = csv_reader.fieldnames
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤
            if len(rows) == 0:
                logger.warning(f"File {object_key} is empty (no data rows)")
                results.append({
                    "file": object_key,
                    "status": "rejected",
                    "reason": "Empty file"
                })
                continue
            
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå
            summary = {
                "event": "file_validated",
                "file": object_key,
                "bucket": bucket_name,
                "status": "valid",
                "total_rows": len(rows),
                "headers": list(headers) if headers else [],
                "file_size_bytes": file_size,
                "processed_at": datetime.now().isoformat()
            }
            
            logger.info(json.dumps(summary))
            
            results.append({
                "file": object_key,
                "status": "valid",
                "rows": len(rows),
                "headers": list(headers) if headers else []
            })
            
        except Exception as e:
            logger.error(json.dumps({
                "event": "processing_error",
                "file": object_key,
                "error": str(e)
            }))
            results.append({
                "file": object_key,
                "status": "error",
                "reason": str(e)
            })
    
    logger.info(f"Processing complete. Results: {json.dumps(results)}")
    
    return {
        "statusCode": 200,
        "body": json.dumps({
            "message": f"Processed {len(results)} file(s)",
            "results": results
        })
    }
```

5. ‡∏Å‡∏î **Deploy**

**‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Timeout:**
- **Configuration** tab ‚Üí **General configuration** ‚Üí **Edit**
- Timeout: **30 seconds** (default 3 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CSV ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà)
- Memory: **256 MB**
- ‡∏Å‡∏î **Save**

---

### ‚úÖ STEP 4 ‚Äî ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ S3 Event Trigger

1. ‡∏¢‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Lambda ‚Üí **Configuration** tab ‚Üí **Triggers** ‚Üí **Add trigger**
2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **S3**
3. ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
   ```
   Bucket: pipeline-raw-[‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì]
   Event types: All object create events
   Prefix: uploads/          ‚Üê ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏≤‡∏Å folder ‡∏ô‡∏µ‡πâ
   Suffix: .csv              ‚Üê ‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå .csv
   ```
4. ‚úÖ Acknowledge recursive warning ‚Üí **Add**

> ‚ö†Ô∏è **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å:** ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á Prefix `uploads/` ‡∏ó‡∏≥‡πÉ‡∏´‡πâ Lambda trigger ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô folder ‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Recursive Loop ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏¢‡∏±‡∏á Bucket ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

---

### ‚úÖ STEP 5 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏î‡∏™‡∏≠‡∏ö

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `sample_sales.csv` ‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á:

```csv
order_id,customer_name,product,quantity,price,date
ORD001,‡∏™‡∏°‡∏ä‡∏≤‡∏¢ ‡πÉ‡∏à‡∏î‡∏µ,Widget A,5,250.00,2024-01-15
ORD002,‡∏™‡∏°‡∏´‡∏ç‡∏¥‡∏á ‡∏£‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô,Widget B,2,450.00,2024-01-15
ORD003,‡∏ß‡∏¥‡∏ä‡∏±‡∏¢ ‡∏°‡∏≤‡∏ô‡∏∞,Widget A,10,250.00,2024-01-16
ORD004,‡∏ô‡∏¥‡∏î‡∏≤ ‡∏™‡∏∏‡∏Ç‡πÉ‡∏à,Widget C,1,1200.00,2024-01-16
ORD005,‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå ‡∏î‡∏µ‡∏á‡∏≤‡∏°,Widget B,3,450.00,2024-01-17
```

---

### ‚úÖ STEP 6 ‚Äî ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Pipeline

**‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå:**
1. ‡πÄ‡∏Ç‡πâ‡∏≤ S3 ‚Üí `pipeline-raw-...` ‚Üí ‡πÄ‡∏Ç‡πâ‡∏≤ folder `uploads/`
2. ‡∏Å‡∏î **Upload** ‚Üí Add files ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å `sample_sales.csv` ‚Üí **Upload**

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CloudWatch Logs:**
1. ‡πÄ‡∏Ç‡πâ‡∏≤ **CloudWatch** ‚Üí **Log groups** ‚Üí `/aws/lambda/csv-validator`
2. ‡∏Å‡∏î Log Stream ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
3. ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô Log ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ:

```json
{
  "event": "file_validated",
  "file": "uploads/sample_sales.csv",
  "status": "valid",
  "total_rows": 5,
  "headers": ["order_id", "customer_name", "product", "quantity", "price", "date"],
  "file_size_bytes": 312
}
```

**‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏£‡∏ì‡∏µ Error:**
1. ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà CSV ‡πÄ‡∏ä‡πà‡∏ô `test.txt` ‡πÑ‡∏õ‡∏ó‡∏µ‡πà `uploads/`
2. ‡∏î‡∏π Log ‚Äî ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô `"status": "rejected", "reason": "Not a CSV file"`

---

### ‚úÖ STEP 7 ‚Äî ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Lambda ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (Manual Test)

‡∏Å‡πà‡∏≠‡∏ô Trigger ‡∏à‡∏≤‡∏Å S3 ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Lambda ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏Å‡πà‡∏≠‡∏ô:

1. ‡πÉ‡∏ô Lambda ‚Üí **Test** tab ‚Üí **Create new event**
2. Event name: `test-s3-event`
3. ‡∏ß‡∏≤‡∏á JSON ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô Event:

```json
{
  "Records": [
    {
      "eventVersion": "2.1",
      "eventSource": "aws:s3",
      "eventTime": "2024-01-15T10:00:00.000Z",
      "eventName": "ObjectCreated:Put",
      "s3": {
        "bucket": {
          "name": "pipeline-raw-[‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì]-[random]"
        },
        "object": {
          "key": "uploads/sample_sales.csv",
          "size": 312
        }
      }
    }
  ]
}
```

4. ‡∏Å‡∏î **Test** ‚Üí ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô Execution results

---

### üìù ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° Week 1

1. **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Lambda** ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ Column `order_id` ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ Reject
2. **‡πÄ‡∏û‡∏¥‡πà‡∏° Metric:** ‡πÉ‡∏ä‡πâ `boto3` ‡∏™‡πà‡∏á Custom Metric ‡πÑ‡∏õ CloudWatch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà Valid/Invalid ‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô
3. **‡∏ó‡∏î‡∏•‡∏≠‡∏á** Upload ‡πÑ‡∏ü‡∏•‡πå CSV ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (1,000 rows) ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏π‡∏ß‡πà‡∏≤ Duration ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà

### ‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏¢ LAB Week 1

1. ‡∏ó‡∏≥‡πÑ‡∏° S3 Event Trigger ‡∏ñ‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏à‡∏∞ Trigger **‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß** ‡πÄ‡∏™‡∏°‡∏≠? ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Lambda ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Duplicate Events ‡πÑ‡∏î‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á? (Idempotency)
2. Lambda ‡∏°‡∏µ Concurrency Limit ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà‡πÉ‡∏ô Default? ‡∏ñ‡πâ‡∏≤ Upload ‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô 1,000 ‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡∏Ç‡∏∂‡πâ‡∏ô?
3. Prefix `uploads/` ‡πÉ‡∏ô S3 Trigger ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Recursive Loop ‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤? ‡∏°‡∏µ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏Å‡∏¥‡∏î Loop ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?

---

---

# üìÖ WEEK 2 ‚Äî Message Queue with SQS

> **‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠:** ‡πÄ‡∏û‡∏¥‡πà‡∏° SQS ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Decouple ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö High Volume

---

## üéì ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
- ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Decoupling ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Ç‡∏≠‡∏á Message Queue ‡πÑ‡∏î‡πâ
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Standard Queue ‡∏Å‡∏±‡∏ö FIFO Queue
- ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° Lambda (Producer) ‚Üí SQS ‚Üí Lambda (Consumer)
- ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Dead Letter Queue (DLQ) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Message ‡∏ó‡∏µ‡πà Process ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à SQS Visibility Timeout ‡πÅ‡∏•‡∏∞ Retry Mechanism

---

## üìñ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (Lecture ‚Äî 30 ‡∏ô‡∏≤‡∏ó‡∏µ)

### ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Queue?

‡∏•‡∏≠‡∏á‡∏ô‡∏∂‡∏Å‡∏ñ‡∏∂‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ô‡∏µ‡πâ: Upload ‡πÑ‡∏ü‡∏•‡πå 500 ‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô Lambda Validator ‡∏ñ‡∏π‡∏Å Trigger 500 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Service ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô ‚Üí Service ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏°‡πà‡πÑ‡∏´‡∏ß ‚Üí ‡∏£‡∏∞‡∏ö‡∏ö‡∏•‡πà‡∏°

**SQS ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢:**
```
500 files uploaded ‚Üí [SQS Queue] ‚Üí Consumer Lambda ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡∏•‡∏∞ 10 messages
                                    ‚Üí ‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ
                                    ‚Üí ‡πÑ‡∏°‡πà overflow ‡∏£‡∏∞‡∏ö‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
```

### Standard Queue vs FIFO Queue

| | Standard Queue | FIFO Queue |
|---|---|---|
| Ordering | Best-effort (‡πÑ‡∏°‡πà‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô) | ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô First-In-First-Out |
| Throughput | Unlimited | 300‚Äì3,000 msg/sec |
| Delivery | At least once (‡∏≠‡∏≤‡∏à duplicate) | Exactly once |
| ‡∏£‡∏≤‡∏Ñ‡∏≤ | ‡∏ñ‡∏π‡∏Å‡∏Å‡∏ß‡πà‡∏≤ | ‡πÅ‡∏û‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ |
| ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠ | ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç | ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö |

> ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Project ‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ **Standard Queue** ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ó‡∏≥ Idempotent Processing

### Dead Letter Queue (DLQ) ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

‡πÄ‡∏°‡∏∑‡πà‡∏≠ Consumer Lambda ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Message ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏ã‡πâ‡∏≥‡πÜ (‡πÄ‡∏Å‡∏¥‡∏ô maxReceiveCount) SQS ‡∏à‡∏∞‡∏¢‡πâ‡∏≤‡∏¢ Message ‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á DLQ ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏¥‡πâ‡∏á ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:
- Debug ‡∏ß‡πà‡∏≤ Message ‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
- ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á
- ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô Operator ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ Message ‡∏Ñ‡πâ‡∏≤‡∏á‡πÉ‡∏ô DLQ

### Architecture ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô Week 2

```
[S3 Event] ‚Üí [Lambda: csv-validator] ‚Üí [SQS: main-queue]
                                               ‚îÇ
                                   SQS Trigger (Batch)
                                               ‚îÇ
                                               ‚ñº
                                    [Lambda: csv-processor]
                                       - Parse CSV
                                       - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Summary
                                       - Save ‡πÑ‡∏õ‡∏¢‡∏±‡∏á S3 processed/
                                               ‚îÇ
                                     (‡∏ñ‡πâ‡∏≤ Error > 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
                                               ‚ñº
                                    [SQS: dead-letter-queue]
```

---

## üîß LAB Step-by-Step

---

### ‚úÖ STEP 1 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á Dead Letter Queue ‡∏Å‡πà‡∏≠‡∏ô

‡πÄ‡∏™‡∏°‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á DLQ ‡∏Å‡πà‡∏≠‡∏ô Main Queue:

1. ‡πÄ‡∏Ç‡πâ‡∏≤ **SQS** ‚Üí **Create queue**
2. ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
   ```
   Type: Standard
   Name: pipeline-dlq
   
   Configuration:
   Visibility timeout: 30 seconds
   Message retention period: 14 days   ‚Üê ‡πÄ‡∏Å‡πá‡∏ö‡∏ô‡∏≤‡∏ô ‡πÄ‡∏ú‡∏∑‡πà‡∏≠ Debug
   Maximum message size: 256 KB
   Delivery delay: 0
   ```
3. ‡∏Å‡∏î **Create queue**
4. ‡∏à‡∏î **Queue ARN** ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πà‡∏≠ (‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: `arn:aws:sqs:ap-southeast-1:123456789:pipeline-dlq`)

---

### ‚úÖ STEP 2 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á Main Queue

1. ‡∏Å‡∏î **Create queue** ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
2. ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
   ```
   Type: Standard
   Name: pipeline-main-queue
   
   Configuration:
   Visibility timeout: 60 seconds      ‚Üê ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ Lambda timeout
   Message retention period: 4 days
   Maximum message size: 256 KB
   Delivery delay: 0
   Receive message wait time: 20 seconds  ‚Üê Long Polling ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Cost
   ```
3. **Dead-letter queue section:**
   - Enabled: ‚úÖ
   - Dead-letter queue: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å `pipeline-dlq`
   - Maximum receives: `3` (‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏™‡πà‡∏á DLQ)
4. ‡∏Å‡∏î **Create queue**

> üí° **Long Polling ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?** ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà SQS ‡∏à‡∏∞‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏°‡∏µ Message" (Short Polling ‚Üí ‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡∏•‡πà‡∏≤) Long Polling ‡∏à‡∏∞‡∏£‡∏≠‡∏ô‡∏≤‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 20 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏≠‡∏ö ‚Äî ‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å

---

### ‚úÖ STEP 3 ‚Äî ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï IAM Policy

Lambda ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏™‡πà‡∏á/‡∏£‡∏±‡∏ö Message ‡∏à‡∏≤‡∏Å SQS:

1. ‡πÄ‡∏Ç‡πâ‡∏≤ **IAM** ‚Üí **Policies** ‚Üí ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ `pipeline-lambda-policy` ‚Üí **Edit**
2. ‡πÄ‡∏û‡∏¥‡πà‡∏° Statement ‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô JSON:

```json
{
  "Sid": "SQSPermissions",
  "Effect": "Allow",
  "Action": [
    "sqs:SendMessage",
    "sqs:ReceiveMessage",
    "sqs:DeleteMessage",
    "sqs:GetQueueAttributes",
    "sqs:ChangeMessageVisibility"
  ],
  "Resource": [
    "arn:aws:sqs:ap-southeast-1:[ACCOUNT-ID]:pipeline-main-queue",
    "arn:aws:sqs:ap-southeast-1:[ACCOUNT-ID]:pipeline-dlq"
  ]
}
```

3. ‡∏Å‡∏î **Save changes**

---

### ‚úÖ STEP 4 ‚Äî ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Lambda Validator ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á Message ‡πÑ‡∏õ SQS

‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç `csv-validator` Lambda:

```python
import json
import boto3
import csv
import io
import logging
from datetime import datetime

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client('s3')
sqs = boto3.client('sqs', region_name='ap-southeast-1')

# ‡πÉ‡∏™‡πà Queue URL ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
QUEUE_URL = 'https://sqs.ap-southeast-1.amazonaws.com/[ACCOUNT-ID]/pipeline-main-queue'
PROCESSED_BUCKET = 'pipeline-processed-[‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì]-[random]'

def lambda_handler(event, context):
    logger.info(f"Received {len(event['Records'])} S3 event(s)")
    
    results = []
    
    for record in event['Records']:
        bucket_name = record['s3']['bucket']['name']
        object_key = record['s3']['object']['key']
        file_size = record['s3']['object']['size']
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
        if not object_key.lower().endswith('.csv'):
            logger.warning(f"Skipping non-CSV file: {object_key}")
            continue
        
        try:
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå
            response = s3.get_object(Bucket=bucket_name, Key=object_key)
            content = response['Body'].read().decode('utf-8')
            
            csv_reader = csv.DictReader(io.StringIO(content))
            rows = list(csv_reader)
            headers = list(csv_reader.fieldnames) if csv_reader.fieldnames else []
            
            if len(rows) == 0:
                logger.warning(f"Empty file: {object_key}")
                continue
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Message ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SQS
            message = {
                "job_id": f"job-{datetime.now().strftime('%Y%m%d%H%M%S')}-{object_key.split('/')[-1]}",
                "source_bucket": bucket_name,
                "source_key": object_key,
                "destination_bucket": PROCESSED_BUCKET,
                "file_size_bytes": file_size,
                "row_count": len(rows),
                "headers": headers,
                "submitted_at": datetime.now().isoformat()
            }
            
            # ‡∏™‡πà‡∏á Message ‡πÑ‡∏õ SQS
            sqs_response = sqs.send_message(
                QueueUrl=QUEUE_URL,
                MessageBody=json.dumps(message),
                MessageAttributes={
                    'job_type': {
                        'DataType': 'String',
                        'StringValue': 'csv_processing'
                    }
                }
            )
            
            logger.info(json.dumps({
                "event": "message_queued",
                "job_id": message["job_id"],
                "message_id": sqs_response['MessageId'],
                "file": object_key,
                "rows": len(rows)
            }))
            
            results.append({
                "file": object_key,
                "status": "queued",
                "job_id": message["job_id"],
                "message_id": sqs_response['MessageId']
            })
            
        except Exception as e:
            logger.error(json.dumps({
                "event": "error",
                "file": object_key,
                "error": str(e),
                "error_type": type(e).__name__
            }))
            results.append({"file": object_key, "status": "error", "reason": str(e)})
    
    return {"statusCode": 200, "body": json.dumps({"results": results})}
```

‡∏Å‡∏î **Deploy**

---

### ‚úÖ STEP 5 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á Lambda Consumer (csv-processor)

1. **Lambda** ‚Üí **Create function**
2. ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
   ```
   Function name: csv-processor
   Runtime: Python 3.12
   Execution role: pipeline-lambda-role
   ```
3. ‡∏ß‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î:

```python
import json
import boto3
import csv
import io
import logging
from datetime import datetime
from collections import defaultdict

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client('s3')

def lambda_handler(event, context):
    """
    Consumer Lambda ‚Äî ‡∏≠‡πà‡∏≤‡∏ô Message ‡∏à‡∏≤‡∏Å SQS ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• CSV
    """
    logger.info(f"Processing {len(event['Records'])} SQS message(s)")
    
    processed = []
    failed = []
    
    for record in event['Records']:
        message_id = record['messageId']
        receipt_handle = record['receiptHandle']
        
        try:
            body = json.loads(record['body'])
            job_id = body['job_id']
            source_bucket = body['source_bucket']
            source_key = body['source_key']
            dest_bucket = body['destination_bucket']
            
            logger.info(json.dumps({
                "event": "processing_started",
                "job_id": job_id,
                "message_id": message_id
            }))
            
            # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å S3
            response = s3.get_object(Bucket=source_bucket, Key=source_key)
            content = response['Body'].read().decode('utf-8')
            
            # Parse CSV
            csv_reader = csv.DictReader(io.StringIO(content))
            rows = list(csv_reader)
            
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Summary Statistics
            summary = calculate_summary(rows, job_id, source_key)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á S3
            filename = source_key.split('/')[-1].replace('.csv', '')
            output_key = f"processed/{datetime.now().strftime('%Y/%m/%d')}/{filename}_summary.json"
            
            s3.put_object(
                Bucket=dest_bucket,
                Key=output_key,
                Body=json.dumps(summary, ensure_ascii=False, indent=2),
                ContentType='application/json'
            )
            
            logger.info(json.dumps({
                "event": "processing_complete",
                "job_id": job_id,
                "output_key": output_key,
                "total_rows": summary['total_rows'],
                "total_revenue": summary.get('total_revenue', 0)
            }))
            
            processed.append(job_id)
            
        except Exception as e:
            logger.error(json.dumps({
                "event": "processing_failed",
                "message_id": message_id,
                "error": str(e),
                "error_type": type(e).__name__
            }))
            # ‡∏ñ‡πâ‡∏≤ raise Exception ‚Üí SQS ‡∏à‡∏∞ retry ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á DLQ ‡∏´‡∏•‡∏±‡∏á 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
            failed.append(message_id)
            raise  # Re-raise ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ SQS ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ Message ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å Process ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    
    logger.info(f"Batch complete: {len(processed)} processed, {len(failed)} failed")
    
    return {
        "batchItemFailures": [
            {"itemIdentifier": mid} for mid in failed
        ]
    }


def calculate_summary(rows, job_id, source_key):
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Summary Statistics ‡∏à‡∏≤‡∏Å CSV rows"""
    
    if not rows:
        return {"job_id": job_id, "source": source_key, "total_rows": 0}
    
    summary = {
        "job_id": job_id,
        "source_file": source_key,
        "processed_at": datetime.now().isoformat(),
        "total_rows": len(rows),
        "columns": list(rows[0].keys())
    }
    
    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Column ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡∏•‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Stats
    numeric_columns = {}
    for col in rows[0].keys():
        try:
            values = [float(row[col]) for row in rows if row[col]]
            if values:
                numeric_columns[col] = {
                    "min": min(values),
                    "max": max(values),
                    "sum": round(sum(values), 2),
                    "avg": round(sum(values) / len(values), 2),
                    "count": len(values)
                }
        except (ValueError, TypeError):
            pass  # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ
    
    if numeric_columns:
        summary["numeric_stats"] = numeric_columns
    
    # ‡∏ô‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Column (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Categorical)
    categorical_stats = {}
    for col in rows[0].keys():
        if col not in numeric_columns:
            counts = defaultdict(int)
            for row in rows:
                counts[row[col]] += 1
            categorical_stats[col] = dict(counts)
    
    if categorical_stats:
        summary["categorical_stats"] = categorical_stats
    
    return summary
```

4. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤:
   - Timeout: **60 seconds**
   - Memory: **256 MB**
5. ‡∏Å‡∏î **Deploy**

---

### ‚úÖ STEP 6 ‚Äî ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° SQS ‡∏Å‡∏±‡∏ö Lambda Consumer

1. ‡πÉ‡∏ô Lambda `csv-processor` ‚Üí **Configuration** ‚Üí **Triggers** ‚Üí **Add trigger**
2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **SQS**
3. ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
   ```
   SQS queue: pipeline-main-queue
   Batch size: 5               ‚Üê ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏•‡∏∞ 5 messages
   Batch window: 10 seconds    ‚Üê ‡∏£‡∏≠‡∏™‡∏∞‡∏™‡∏° messages ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
   Report batch item failures: ‚úÖ Enable
   ```
4. ‡∏Å‡∏î **Add**

> üí° **Batch Item Failures ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?** ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡πâ‡∏≤ Message ‡πÉ‡∏î‡πÉ‡∏ô Batch ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏à‡∏∞ retry ‡∏ó‡∏±‡πâ‡∏á Batch ‡πÉ‡∏´‡∏°‡πà‡∏´‡∏°‡∏î ‡πÄ‡∏£‡∏≤‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Message ID ‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß SQS ‡∏à‡∏∞ retry ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏≠‡∏±‡∏ô ‡∏ô‡∏±‡πâ‡∏ô ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏°‡∏≤‡∏Å

---

### ‚úÖ STEP 7 ‚Äî ‡∏ó‡∏î‡∏™‡∏≠‡∏ö End-to-End Pipeline

1. **‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î** `sample_sales.csv` ‡πÑ‡∏õ‡∏¢‡∏±‡∏á `uploads/` ‡πÉ‡∏ô S3 Bucket ‡πÅ‡∏£‡∏Å
2. **‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö** SQS Console ‚Üí `pipeline-main-queue` ‚Üí Messages available ‡∏à‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
3. **‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö** CloudWatch Logs:
   - `/aws/lambda/csv-validator` ‚Üí ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô `"event": "message_queued"`
   - `/aws/lambda/csv-processor` ‚Üí ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô `"event": "processing_complete"`
4. **‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö** S3 `pipeline-processed-...` ‚Üí ‡πÄ‡∏Ç‡πâ‡∏≤ folder `processed/` ‚Üí ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå `..._summary.json`
5. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå JSON ‡∏°‡∏≤‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

---

### ‚úÖ STEP 8 ‚Äî ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Dead Letter Queue

‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ DLQ ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á:

1. ‡πÅ‡∏Å‡πâ `csv-processor` ‡πÉ‡∏´‡πâ Raise Error ‡πÄ‡∏™‡∏°‡∏≠‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß:
```python
def lambda_handler(event, context):
    raise Exception("Simulated failure for DLQ testing")
```
2. Deploy ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡πÉ‡∏´‡∏°‡πà
3. ‡∏£‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 2‚Äì3 ‡∏ô‡∏≤‡∏ó‡∏µ (SQS ‡∏à‡∏∞ retry 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡∏≤‡∏° maxReceiveCount)
4. ‡πÄ‡∏Ç‡πâ‡∏≤ SQS ‚Üí `pipeline-dlq` ‚Üí **Messages available** ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô
5. ‡∏Å‡∏î **Send and receive messages** ‚Üí **Poll for messages** ‚Üí ‡∏î‡∏π‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ Message
6. ‡πÅ‡∏Å‡πâ‡πÇ‡∏Ñ‡πâ‡∏î Lambda ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥‡πÅ‡∏•‡πâ‡∏ß Deploy

---

### üìù ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° Week 2

1. **‡πÄ‡∏û‡∏¥‡πà‡∏° CloudWatch Alarm** ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ `pipeline-dlq` ‡∏°‡∏µ Message ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 0 (‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏°‡∏µ Processing Error)
2. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö High Volume:** ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Script ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î CSV 20 ‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô ‡∏î‡∏π‡∏ß‡πà‡∏≤ SQS ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Queue ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
3. **‡πÄ‡∏û‡∏¥‡πà‡∏° Message Deduplication ID:** ‡πÅ‡∏Å‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏ã‡πâ‡∏≥ (Idempotency Key)

### ‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏¢ LAB Week 2

1. ‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á Visibility Timeout ‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤ Lambda Timeout ‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡∏Ç‡∏∂‡πâ‡∏ô?
2. SQS Standard Queue ‡∏≠‡∏≤‡∏à Deliver Message ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 1 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‚Äî ‡πÄ‡∏£‡∏≤‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö `csv-processor` ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á? (Hint: ‡∏î‡∏π‡∏ó‡∏µ‡πà output_key)
3. Long Polling ‡∏Å‡∏±‡∏ö Short Polling ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á‡πÉ‡∏ô‡πÅ‡∏á‡πà Cost ‡πÅ‡∏•‡∏∞ Latency?

---

---

# üìÖ WEEK 3 ‚Äî Orchestration with Step Functions

> **‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠:** ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà Chain Lambda ‡∏ï‡∏£‡∏á‡πÜ ‡∏î‡πâ‡∏ß‡∏¢ State Machine ‡∏ó‡∏µ‡πà‡∏î‡∏π‡πÅ‡∏•‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢

---

## üéì ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
- ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Orchestration vs Choreography ‡πÉ‡∏ô Microservices ‡πÑ‡∏î‡πâ
- ‡∏™‡∏£‡πâ‡∏≤‡∏á Step Functions State Machine ‡∏î‡πâ‡∏ß‡∏¢ Amazon States Language (ASL)
- ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° Lambda ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤ Workflow ‡∏ó‡∏µ‡πà‡∏°‡∏µ Error Handling
- ‡πÉ‡∏ä‡πâ Parallel State ‡πÅ‡∏•‡∏∞ Choice State
- Monitor Execution History ‡πÉ‡∏ô Step Functions Console

---

## üìñ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (Lecture ‚Äî 30 ‡∏ô‡∏≤‡∏ó‡∏µ)

### Orchestration vs Choreography

**Choreography (‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏ô Week 1‚Äì2):**
```
Lambda A ‚îÄ‚îÄ‚îÄ‚îÄ‡∏™‡πà‡∏á Event‚îÄ‚îÄ‚îÄ‚îÄ> SQS ‚îÄ‚îÄ‚îÄ‚îÄtrigger‚îÄ‚îÄ‚îÄ‚îÄ> Lambda B
Lambda B ‚îÄ‚îÄ‚îÄ‚îÄ‡∏™‡πà‡∏á Event‚îÄ‚îÄ‚îÄ‚îÄ> SNS ‚îÄ‚îÄ‚îÄ‚îÄtrigger‚îÄ‚îÄ‚îÄ‚îÄ> Lambda C
```
- ‡πÅ‡∏ï‡πà‡∏•‡∏∞ Service ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏Å‡∏±‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
- ‡∏¢‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏à‡∏∞ Debug ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ("‡πÑ‡∏ü‡∏•‡πà‡∏≠‡∏á‡∏´‡∏ô‡πÑ‡∏õ‡πÑ‡∏´‡∏ô?")
- ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô Flow ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

**Orchestration (Step Functions):**
```
Step Functions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Lambda A
              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Lambda B (‡∏ñ‡πâ‡∏≤ A ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à)
              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Lambda C (‡∏ñ‡πâ‡∏≤ B ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à)
              ‚îÄ‚îÄError‚îÄ‚îÄ‚îÄ> Notify (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Error)
```
- ‡∏°‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° Flow ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
- ‡πÄ‡∏´‡πá‡∏ô Visual Diagram ‡πÅ‡∏•‡∏∞ Execution History
- Error Handling ‡πÅ‡∏•‡∏∞ Retry ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
- Timeout ‡πÅ‡∏ï‡πà‡∏•‡∏∞ Step ‡πÑ‡∏î‡πâ

### Amazon States Language (ASL)

ASL ‡πÄ‡∏õ‡πá‡∏ô JSON-based Language ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î State Machine:

```json
{
  "Comment": "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á State Machine",
  "StartAt": "Step1",
  "States": {
    "Step1": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:...:function:my-lambda",
      "Next": "Step2",
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError"
        }
      ]
    },
    "Step2": {
      "Type": "Task",
      "Resource": "...",
      "End": true
    },
    "HandleError": {
      "Type": "Fail",
      "Error": "ProcessingFailed"
    }
  }
}
```

### State Types ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

| State Type | ‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£ | ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á |
|-----------|-----------|---------|
| Task | ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Lambda / Service | ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå |
| Choice | ‡πÅ‡∏¢‡∏Å‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç | ‡∏ñ‡πâ‡∏≤ rows > 1000 ‡∏ó‡∏≥ A ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô‡∏ó‡∏≥ B |
| Parallel | ‡∏ó‡∏≥‡∏´‡∏•‡∏≤‡∏¢ Tasks ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô | Transform + Validate ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô |
| Wait | ‡∏£‡∏≠‡πÄ‡∏ß‡∏•‡∏≤ | ‡∏£‡∏≠ 5 ‡∏ô‡∏≤‡∏ó‡∏µ‡∏Å‡πà‡∏≠‡∏ô Retry |
| Succeed | ‡∏à‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à | End State |
| Fail | ‡∏à‡∏ö‡πÅ‡∏ö‡∏ö Error | Error State |
| Pass | ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£ | Debug / Testing |

### Architecture ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô Week 3

```
[SQS Consumer Lambda] ‚îÄstarts‚îÄ> [Step Functions: ProcessCSVWorkflow]
                                          ‚îÇ
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚ñº             ‚ñº             ‚ñº
                      [Parse CSV]  [Validate Schema] [Check Size]
                      (Parallel States ‚Äî ‡∏ó‡∏≥‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô)
                            ‚îÇ
                            ‚ñº
                      [Choice: Valid?]
                         /         \
                    Yes              No
                     ‚îÇ               ‚îÇ
              [Transform Data]  [Move to failed/]
                     ‚îÇ
                     ‚ñº
               [Save Results]
                     ‚îÇ
                     ‚ñº
                [Notify SNS]   ‚Üê ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏ô Week 4
                     ‚îÇ
                     ‚ñº
                  [Success]
```

---

## üîß LAB Step-by-Step

---

### ‚úÖ STEP 1 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á Lambda Functions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Step

‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Lambda 4 ‡∏ï‡∏±‡∏ß‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Single Responsibility):

**Lambda 1: parse-csv**

1. **Lambda** ‚Üí **Create function** ‚Üí `parse-csv` ‚Üí Python 3.12 ‚Üí `pipeline-lambda-role`

```python
import json
import boto3
import csv
import io
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)
s3 = boto3.client('s3')

def lambda_handler(event, context):
    """
    Step 1: ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞ Parse ‡πÑ‡∏ü‡∏•‡πå CSV ‡∏à‡∏≤‡∏Å S3
    Input: {"source_bucket": "...", "source_key": "...", "job_id": "..."}
    Output: ‡πÄ‡∏û‡∏¥‡πà‡∏° rows ‡πÅ‡∏•‡∏∞ headers ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô event
    """
    source_bucket = event['source_bucket']
    source_key = event['source_key']
    job_id = event['job_id']
    
    logger.info(f"Parsing {source_key} for job {job_id}")
    
    response = s3.get_object(Bucket=source_bucket, Key=source_key)
    content = response['Body'].read().decode('utf-8')
    
    csv_reader = csv.DictReader(io.StringIO(content))
    rows = list(csv_reader)
    headers = list(csv_reader.fieldnames) if csv_reader.fieldnames else []
    
    logger.info(f"Parsed {len(rows)} rows, {len(headers)} columns")
    
    # ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Step ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    return {
        **event,  # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏ï‡πà‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        "rows": rows,
        "headers": headers,
        "row_count": len(rows),
        "parse_status": "success"
    }
```

**Lambda 2: validate-schema**

1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Lambda `validate-schema` ‚Üí Python 3.12 ‚Üí `pipeline-lambda-role`

```python
import json
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Schema ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
REQUIRED_COLUMNS = ['order_id', 'customer_name', 'product', 'quantity', 'price', 'date']

def lambda_handler(event, context):
    """
    Step 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ CSV ‡∏°‡∏µ Column ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏ï‡∏≤‡∏° Schema
    """
    headers = event.get('headers', [])
    job_id = event['job_id']
    
    missing_columns = [col for col in REQUIRED_COLUMNS if col not in headers]
    extra_columns = [col for col in headers if col not in REQUIRED_COLUMNS]
    
    is_valid = len(missing_columns) == 0
    
    logger.info(json.dumps({
        "job_id": job_id,
        "is_valid": is_valid,
        "missing_columns": missing_columns,
        "extra_columns": extra_columns,
        "headers_found": headers
    }))
    
    return {
        **event,
        "schema_valid": is_valid,
        "missing_columns": missing_columns,
        "extra_columns": extra_columns,
        "validation_status": "passed" if is_valid else "failed"
    }
```

**Lambda 3: transform-data**

1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Lambda `transform-data` ‚Üí Python 3.12 ‚Üí `pipeline-lambda-role`

```python
import json
import boto3
import logging
from datetime import datetime
from collections import defaultdict

logger = logging.getLogger()
logger.setLevel(logging.INFO)
s3 = boto3.client('s3')

def lambda_handler(event, context):
    """
    Step 3: ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    """
    rows = event['rows']
    job_id = event['job_id']
    dest_bucket = event['destination_bucket']
    source_key = event['source_key']
    
    logger.info(f"Transforming {len(rows)} rows for job {job_id}")
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Revenue
    total_revenue = 0
    product_summary = defaultdict(lambda: {"quantity": 0, "revenue": 0, "orders": 0})
    
    transformed_rows = []
    for row in rows:
        try:
            qty = float(row.get('quantity', 0))
            price = float(row.get('price', 0))
            revenue = qty * price
            total_revenue += revenue
            
            product = row.get('product', 'Unknown')
            product_summary[product]['quantity'] += qty
            product_summary[product]['revenue'] += revenue
            product_summary[product]['orders'] += 1
            
            transformed_rows.append({
                **row,
                'revenue': round(revenue, 2),
                'processed_at': datetime.now().isoformat()
            })
        except (ValueError, TypeError) as e:
            logger.warning(f"Error processing row {row}: {e}")
    
    result = {
        "job_id": job_id,
        "source_file": source_key,
        "processed_at": datetime.now().isoformat(),
        "summary": {
            "total_rows": len(rows),
            "total_revenue": round(total_revenue, 2),
            "average_order_value": round(total_revenue / len(rows), 2) if rows else 0,
            "product_breakdown": {k: {**v, "revenue": round(v["revenue"], 2)} 
                                  for k, v in product_summary.items()}
        },
        "data": transformed_rows
    }
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡∏¢‡∏±‡∏á S3
    filename = source_key.split('/')[-1].replace('.csv', '')
    output_key = f"processed/{datetime.now().strftime('%Y/%m/%d')}/{filename}_result.json"
    
    s3.put_object(
        Bucket=dest_bucket,
        Key=output_key,
        Body=json.dumps(result, ensure_ascii=False, indent=2),
        ContentType='application/json'
    )
    
    logger.info(json.dumps({
        "event": "transform_complete",
        "job_id": job_id,
        "total_revenue": result['summary']['total_revenue'],
        "output_key": output_key
    }))
    
    return {
        **event,
        "transform_status": "success",
        "output_key": output_key,
        "total_revenue": result['summary']['total_revenue'],
        "summary": result['summary']
    }
```

**Lambda 4: move-to-failed**

1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Lambda `move-to-failed` ‚Üí Python 3.12 ‚Üí `pipeline-lambda-role`

```python
import json
import boto3
import logging
from datetime import datetime

logger = logging.getLogger()
logger.setLevel(logging.INFO)
s3 = boto3.client('s3')

def lambda_handler(event, context):
    """
    ‡πÄ‡∏°‡∏∑‡πà‡∏≠ Schema Invalid ‚Äî ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á failed/ folder ‡∏û‡∏£‡πâ‡∏≠‡∏° error log
    """
    source_bucket = event['source_bucket']
    source_key = event['source_key']
    job_id = event['job_id']
    missing_columns = event.get('missing_columns', [])
    
    # Copy ‡πÑ‡∏õ‡∏¢‡∏±‡∏á failed/ folder
    filename = source_key.split('/')[-1]
    failed_key = f"failed/{datetime.now().strftime('%Y/%m/%d')}/{filename}"
    
    s3.copy_object(
        Bucket=source_bucket,
        CopySource={'Bucket': source_bucket, 'Key': source_key},
        Key=failed_key,
        Metadata={
            'job_id': job_id,
            'reason': f"Missing columns: {', '.join(missing_columns)}",
            'failed_at': datetime.now().isoformat()
        },
        MetadataDirective='REPLACE'
    )
    
    logger.warning(json.dumps({
        "event": "file_moved_to_failed",
        "job_id": job_id,
        "original_key": source_key,
        "failed_key": failed_key,
        "reason": f"Missing columns: {missing_columns}"
    }))
    
    return {
        **event,
        "failed_key": failed_key,
        "move_status": "success"
    }
```

Deploy ‡∏ó‡∏∏‡∏Å Lambda

---

### ‚úÖ STEP 2 ‚Äî ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï IAM Policy

Lambda ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Step Functions ‡πÅ‡∏•‡∏∞ Copy Object ‡πÉ‡∏ô S3:

‡πÄ‡∏û‡∏¥‡πà‡∏° Statement ‡πÉ‡∏ô `pipeline-lambda-policy`:

```json
{
  "Sid": "S3CopyObject",
  "Effect": "Allow",
  "Action": ["s3:CopyObject"],
  "Resource": "arn:aws:s3:::pipeline-raw-[‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì]-*/*"
},
{
  "Sid": "StepFunctionsStart",
  "Effect": "Allow",
  "Action": ["states:StartExecution"],
  "Resource": "*"
}
```

**‡∏™‡∏£‡πâ‡∏≤‡∏á IAM Role ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Step Functions:**

1. **IAM** ‚Üí **Roles** ‚Üí **Create role**
2. Trusted entity: **AWS service** ‚Üí ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏•‡∏á‡∏´‡∏≤ **Step Functions** ‚Üí Next
3. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ `AWSLambdaRole` (‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ Step Functions ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Lambda) ‚Üí ‡πÄ‡∏û‡∏¥‡πà‡∏°
4. Role name: `pipeline-stepfunctions-role` ‚Üí Create

---

### ‚úÖ STEP 3 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á State Machine

1. ‡πÄ‡∏Ç‡πâ‡∏≤ **Step Functions** ‚Üí **State machines** ‚Üí **Create state machine**
2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **Write your workflow in code**
3. Type: **Standard** (Express ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö High Volume / Short Duration)
4. ‡∏ß‡∏≤‡∏á ASL JSON ‡∏ô‡∏µ‡πâ:

```json
{
  "Comment": "CSV Processing Pipeline ‚Äî Week 3 LAB",
  "StartAt": "ParseCSV",
  "States": {
    "ParseCSV": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:parse-csv",
      "TimeoutSeconds": 60,
      "Retry": [
        {
          "ErrorEquals": ["Lambda.ServiceException", "Lambda.AWSLambdaException", "Lambda.SdkClientException"],
          "IntervalSeconds": 2,
          "MaxAttempts": 3,
          "BackoffRate": 2
        }
      ],
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError",
          "ResultPath": "$.error"
        }
      ],
      "Next": "ValidateSchema"
    },
    
    "ValidateSchema": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:validate-schema",
      "TimeoutSeconds": 30,
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError",
          "ResultPath": "$.error"
        }
      ],
      "Next": "CheckValidation"
    },
    
    "CheckValidation": {
      "Type": "Choice",
      "Choices": [
        {
          "Variable": "$.schema_valid",
          "BooleanEquals": true,
          "Next": "TransformData"
        },
        {
          "Variable": "$.schema_valid",
          "BooleanEquals": false,
          "Next": "MoveToFailed"
        }
      ],
      "Default": "HandleError"
    },
    
    "TransformData": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:transform-data",
      "TimeoutSeconds": 120,
      "Retry": [
        {
          "ErrorEquals": ["States.ALL"],
          "IntervalSeconds": 5,
          "MaxAttempts": 2,
          "BackoffRate": 2
        }
      ],
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError",
          "ResultPath": "$.error"
        }
      ],
      "Next": "ProcessingComplete"
    },
    
    "MoveToFailed": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:move-to-failed",
      "TimeoutSeconds": 30,
      "Next": "ValidationFailed"
    },
    
    "ProcessingComplete": {
      "Type": "Succeed",
      "Comment": "Pipeline completed successfully"
    },
    
    "ValidationFailed": {
      "Type": "Fail",
      "Error": "ValidationFailed",
      "Cause": "CSV schema validation failed ‚Äî file moved to failed folder"
    },
    
    "HandleError": {
      "Type": "Fail",
      "Error": "ProcessingError",
      "Cause": "An error occurred during pipeline processing"
    }
  }
}
```

5. **‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà `[ACCOUNT-ID]`** ‡∏î‡πâ‡∏ß‡∏¢ AWS Account ID ‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å AWS Console ‡∏ö‡∏ô‡∏Ç‡∏ß‡∏≤)
6. ‡∏Å‡∏î **Next** ‚Üí State machine name: `ProcessCSVWorkflow`
7. Execution role: `pipeline-stepfunctions-role`
8. ‡∏Å‡∏î **Create state machine**

---

### ‚úÖ STEP 4 ‚Äî ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï SQS Consumer ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏° Step Functions

‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Lambda `csv-processor` (‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô Week 2) ‡πÉ‡∏´‡πâ Start Step Functions Execution ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏á:

```python
import json
import boto3
import logging
from datetime import datetime

logger = logging.getLogger()
logger.setLevel(logging.INFO)

sfn = boto3.client('stepfunctions', region_name='ap-southeast-1')

# ‡πÉ‡∏™‡πà State Machine ARN ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
STATE_MACHINE_ARN = 'arn:aws:states:ap-southeast-1:[ACCOUNT-ID]:stateMachine:ProcessCSVWorkflow'

def lambda_handler(event, context):
    """
    ‡∏≠‡πà‡∏≤‡∏ô Message ‡∏à‡∏≤‡∏Å SQS ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏° Step Functions Execution
    """
    logger.info(f"Processing {len(event['Records'])} SQS message(s)")
    
    batch_failures = []
    
    for record in event['Records']:
        message_id = record['messageId']
        
        try:
            body = json.loads(record['body'])
            job_id = body['job_id']
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Input ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Step Functions
            sfn_input = {
                "job_id": job_id,
                "source_bucket": body['source_bucket'],
                "source_key": body['source_key'],
                "destination_bucket": body['destination_bucket'],
                "submitted_at": body['submitted_at'],
                "row_count_estimate": body.get('row_count', 0)
            }
            
            # Start Step Functions Execution
            response = sfn.start_execution(
                stateMachineArn=STATE_MACHINE_ARN,
                name=f"execution-{job_id}",  # ‡∏ï‡πâ‡∏≠‡∏á Unique
                input=json.dumps(sfn_input)
            )
            
            logger.info(json.dumps({
                "event": "execution_started",
                "job_id": job_id,
                "execution_arn": response['executionArn'],
                "message_id": message_id
            }))
            
        except Exception as e:
            logger.error(f"Failed to start execution for message {message_id}: {e}")
            batch_failures.append({"itemIdentifier": message_id})
    
    return {"batchItemFailures": batch_failures}
```

‡∏Å‡∏î **Deploy**

---

### ‚úÖ STEP 5 ‚Äî ‡∏ó‡∏î‡∏™‡∏≠‡∏ö State Machine

**‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏à‡∏≤‡∏Å Step Functions Console:**

1. ‡πÄ‡∏Ç‡πâ‡∏≤ **Step Functions** ‚Üí `ProcessCSVWorkflow` ‚Üí **Start execution**
2. Input:
```json
{
  "job_id": "test-job-001",
  "source_bucket": "pipeline-raw-[‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì]-[random]",
  "source_key": "uploads/sample_sales.csv",
  "destination_bucket": "pipeline-processed-[‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì]-[random]",
  "submitted_at": "2024-01-15T10:00:00"
}
```
3. ‡∏Å‡∏î **Start execution**

**‡∏î‡∏π Visual Execution:**
- ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô Diagram ‡πÅ‡∏™‡∏î‡∏á State ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á Execute ‡∏≠‡∏¢‡∏π‡πà (‡∏™‡∏µ‡∏ü‡πâ‡∏≤ = ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô, ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß = ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à, ‡∏™‡∏µ‡πÅ‡∏î‡∏á = ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß)
- ‡∏Å‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞ State ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π Input/Output

**‡∏ó‡∏î‡∏™‡∏≠‡∏ö Validation Failed Path:**
1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `bad_schema.csv` ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ Column `order_id`:
```csv
name,product,qty,amount
Alice,Widget A,5,250
```
2. Upload ‡∏Ç‡∏∂‡πâ‡∏ô S3 `uploads/`
3. Start execution ‡∏î‡πâ‡∏ß‡∏¢ `source_key: "uploads/bad_schema.csv"`
4. ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï Flow ‡∏à‡∏∞‡πÑ‡∏õ‡πÄ‡∏™‡πâ‡∏ô `MoveToFailed` ‚Üí `ValidationFailed`

---

### üìù ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° Week 3

1. **‡πÄ‡∏û‡∏¥‡πà‡∏° Parallel State:** ‡∏£‡∏±‡∏ô `validate-schema` ‡πÅ‡∏•‡∏∞ **‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö File Size** ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô (‡∏™‡∏£‡πâ‡∏≤‡∏á Lambda `check-file-size` ‡∏á‡πà‡∏≤‡∏¢‡πÜ)
2. **‡πÄ‡∏û‡∏¥‡πà‡∏° Wait State:** ‡∏ñ‡πâ‡∏≤ File Size > 1 MB ‡πÉ‡∏´‡πâ‡∏£‡∏≠ 30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏Å‡πà‡∏≠‡∏ô Transform (‡∏à‡∏≥‡∏•‡∏≠‡∏á Rate Limiting)
3. **Map State:** ‡∏ñ‡πâ‡∏≤ CSV ‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ Products ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞ Product ‡πÅ‡∏ö‡∏ö Parallel ‡∏î‡πâ‡∏ß‡∏¢ Map State

### ‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏¢ LAB Week 3

1. Standard Workflow ‡∏Å‡∏±‡∏ö Express Workflow ‡πÉ‡∏ô Step Functions ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á? ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà?
2. Retry ‡πÉ‡∏ô ASL ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á? `BackoffRate: 2` ‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏≠‡∏∞‡πÑ‡∏£?
3. ‡∏ñ‡πâ‡∏≤ Execution ‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å ‡∏à‡∏∞ Cancel ‡πÑ‡∏î‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á? ‡∏°‡∏µ Side Effects ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?

---

---

# üìÖ WEEK 4 ‚Äî Notification & Recap

> **‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠:** ‡πÄ‡∏û‡∏¥‡πà‡∏° SNS ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô + CloudWatch Dashboard + ‡∏™‡∏£‡∏∏‡∏õ Architecture ‡πÅ‡∏•‡∏∞ Cost

---

## üéì ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
- ‡∏™‡∏£‡πâ‡∏≤‡∏á SNS Topic ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á Email/SMS Notification
- ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° SNS ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö Step Functions (‡∏à‡∏ö Pipeline ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô)
- ‡∏™‡∏£‡πâ‡∏≤‡∏á CloudWatch Dashboard ‡πÅ‡∏™‡∏î‡∏á Metrics ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å Service
- ‡∏ï‡∏±‡πâ‡∏á CloudWatch Alarm ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ Pipeline Failure
- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Architecture ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏•‡∏∞ Cost Estimation

---

## üìñ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (Lecture ‚Äî 20 ‡∏ô‡∏≤‡∏ó‡∏µ)

### Amazon SNS (Simple Notification Service) ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

SNS ‡πÄ‡∏õ‡πá‡∏ô Pub/Sub Messaging Service ‚Äî ‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á (Publisher) ‡∏™‡πà‡∏á Message ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Topic ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÉ‡∏Ñ‡∏£‡∏£‡∏±‡∏ö ‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö (Subscriber) ‡∏£‡∏±‡∏ö Message ‡∏à‡∏≤‡∏Å Topic ‡∏ó‡∏µ‡πà Subscribe ‡πÑ‡∏ß‡πâ

```
[Lambda ‡∏™‡πà‡∏á‡∏ú‡∏•] ‚îÄ‚îÄpublish‚îÄ‚îÄ> [SNS Topic: pipeline-notifications]
                                          ‚îÇ
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚ñº                ‚ñº                ‚ñº
                    [Email Sub]     [SMS Sub]      [Lambda Sub]
                  (‡∏™‡πà‡∏á Email)      (‡∏™‡πà‡∏á SMS)     (trigger ‡∏≠‡∏∑‡πà‡∏ô)
```

**SNS vs SQS:**
| | SNS | SQS |
|---|---|---|
| Pattern | Push (Pub/Sub) | Pull (Queue) |
| Delivery | Fan-out (‡∏ó‡∏∏‡∏Å Subscriber) | One consumer per message |
| Retention | ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö | ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ |
| Use case | Notification, Fan-out | Work Queue, Decoupling |

### Architecture ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô Week 4

```
[Step Functions: TransformData] 
              ‚îÇ
              ‚ñº
     [Lambda: send-notification]
              ‚îÇ
              ‚ñº
       [SNS Topic: pipeline-notifications]
         ‚îÇ              ‚îÇ
         ‚ñº              ‚ñº
      [Email]         [SQS]  ‚Üê ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Integration ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï

[CloudWatch] ‚îÄ‚îÄ‚îÄ‚îÄ Monitor ‚îÄ‚îÄ‚îÄ‚îÄ> ‡∏ó‡∏∏‡∏Å Service ‡πÉ‡∏ô Pipeline
                                 + Dashboard + Alarms
```

---

## üîß LAB Step-by-Step

---

### ‚úÖ STEP 1 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á SNS Topic

1. ‡πÄ‡∏Ç‡πâ‡∏≤ **SNS** ‚Üí **Topics** ‚Üí **Create topic**
2. ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
   ```
   Type: Standard
   Name: pipeline-notifications
   Display name: CSV Pipeline
   ```
3. ‡∏Å‡∏î **Create topic**
4. ‡∏à‡∏î **Topic ARN** ‡πÑ‡∏ß‡πâ (‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: `arn:aws:sns:ap-southeast-1:...:pipeline-notifications`)

**‡∏™‡∏£‡πâ‡∏≤‡∏á Email Subscription:**
1. ‡∏Å‡∏î Topic ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á ‚Üí **Create subscription**
2. ‡∏Å‡∏£‡∏≠‡∏Å:
   ```
   Protocol: Email
   Endpoint: [‡πÉ‡∏™‡πà Email ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì]
   ```
3. ‡∏Å‡∏î **Create subscription**
4. ‡πÄ‡∏ä‡πá‡∏Ñ Email ‚Üí ‡∏Å‡∏î **Confirm subscription** ‡πÉ‡∏ô Email ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö
5. ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏ó‡∏µ‡πà SNS Console ‚Üí Status ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô `Confirmed`

---

### ‚úÖ STEP 2 ‚Äî ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï IAM Policy ‡πÄ‡∏û‡∏¥‡πà‡∏° SNS

‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô `pipeline-lambda-policy`:

```json
{
  "Sid": "SNSPublish",
  "Effect": "Allow",
  "Action": ["sns:Publish"],
  "Resource": "arn:aws:sns:ap-southeast-1:[ACCOUNT-ID]:pipeline-notifications"
}
```

---

### ‚úÖ STEP 3 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á Lambda Notification

1. **Lambda** ‚Üí **Create function** ‚Üí `send-notification` ‚Üí Python 3.12 ‚Üí `pipeline-lambda-role`

```python
import json
import boto3
import logging
from datetime import datetime

logger = logging.getLogger()
logger.setLevel(logging.INFO)

sns = boto3.client('sns', region_name='ap-southeast-1')

SNS_TOPIC_ARN = 'arn:aws:sns:ap-southeast-1:[ACCOUNT-ID]:pipeline-notifications'

def lambda_handler(event, context):
    """
    ‡∏™‡πà‡∏á Notification ‡πÄ‡∏°‡∏∑‡πà‡∏≠ Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô (‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß)
    """
    job_id = event.get('job_id', 'unknown')
    status = event.get('transform_status', event.get('move_status', 'unknown'))
    source_key = event.get('source_key', 'unknown')
    summary = event.get('summary', {})
    
    is_success = status == 'success'
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Message
    if is_success:
        subject = f"‚úÖ Pipeline ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‚Äî {source_key.split('/')[-1]}"
        message = build_success_message(job_id, source_key, summary, event)
    else:
        subject = f"‚ùå Pipeline ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‚Äî {source_key.split('/')[-1]}"
        message = build_failure_message(job_id, source_key, event)
    
    # ‡∏™‡πà‡∏á SNS
    response = sns.publish(
        TopicArn=SNS_TOPIC_ARN,
        Subject=subject[:100],  # Subject ‡∏à‡∏≥‡∏Å‡∏±‡∏î 100 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
        Message=message,
        MessageAttributes={
            'job_id': {'DataType': 'String', 'StringValue': job_id},
            'status': {'DataType': 'String', 'StringValue': 'success' if is_success else 'failed'}
        }
    )
    
    logger.info(json.dumps({
        "event": "notification_sent",
        "job_id": job_id,
        "status": "success" if is_success else "failed",
        "message_id": response['MessageId']
    }))
    
    return {
        **event,
        "notification_status": "sent",
        "notification_message_id": response['MessageId']
    }


def build_success_message(job_id, source_key, summary, event):
    filename = source_key.split('/')[-1]
    total_rows = summary.get('total_rows', event.get('row_count', 0))
    total_revenue = summary.get('total_revenue', 0)
    output_key = event.get('output_key', 'N/A')
    
    return f"""
üéâ CSV Pipeline ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!
{'='*50}

üìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {filename}
üÜî Job ID: {job_id}
‚è∞ ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏°‡∏∑‡πà‡∏≠: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC

üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:
   - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß: {total_rows:,} ‡πÅ‡∏ñ‡∏ß
   - ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏£‡∏ß‡∏°: {total_revenue:,.2f} ‡∏ö‡∏≤‡∏ó

üíæ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà:
   {output_key}

---
AWS Data Pipeline Notification
    """.strip()


def build_failure_message(job_id, source_key, event):
    filename = source_key.split('/')[-1]
    missing_cols = event.get('missing_columns', [])
    failed_key = event.get('failed_key', 'N/A')
    
    return f"""
‚ö†Ô∏è CSV Pipeline ‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤

üìÅ ‡πÑ‡∏ü‡∏•‡πå: {filename}
üÜî Job ID: {job_id}
‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC

‚ùå ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏: Schema ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
   Column ‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢: {', '.join(missing_cols) if missing_cols else 'N/A'}

üìÇ ‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡∏ó‡∏µ‡πà: {failed_key}

---
‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏´‡∏°‡πà
AWS Data Pipeline Notification
    """.strip()
```

‡∏Å‡∏î **Deploy**

---

### ‚úÖ STEP 4 ‚Äî ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï State Machine ‡πÄ‡∏û‡∏¥‡πà‡∏° Notification Step

‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç State Machine `ProcessCSVWorkflow` ‚Äî ‡πÄ‡∏û‡∏¥‡πà‡∏° `SendNotification` ‡πÅ‡∏•‡∏∞ `SendFailureNotification`:

1. ‡πÄ‡∏Ç‡πâ‡∏≤ **Step Functions** ‚Üí `ProcessCSVWorkflow` ‚Üí **Edit**
2. ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà ASL ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà:

```json
{
  "Comment": "CSV Processing Pipeline ‚Äî Week 4 (with Notifications)",
  "StartAt": "ParseCSV",
  "States": {
    "ParseCSV": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:parse-csv",
      "TimeoutSeconds": 60,
      "Retry": [
        {
          "ErrorEquals": ["Lambda.ServiceException", "Lambda.AWSLambdaException"],
          "IntervalSeconds": 2,
          "MaxAttempts": 3,
          "BackoffRate": 2
        }
      ],
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError",
          "ResultPath": "$.error"
        }
      ],
      "Next": "ValidateSchema"
    },

    "ValidateSchema": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:validate-schema",
      "TimeoutSeconds": 30,
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError",
          "ResultPath": "$.error"
        }
      ],
      "Next": "CheckValidation"
    },

    "CheckValidation": {
      "Type": "Choice",
      "Choices": [
        {
          "Variable": "$.schema_valid",
          "BooleanEquals": true,
          "Next": "TransformData"
        },
        {
          "Variable": "$.schema_valid",
          "BooleanEquals": false,
          "Next": "MoveToFailed"
        }
      ],
      "Default": "HandleError"
    },

    "TransformData": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:transform-data",
      "TimeoutSeconds": 120,
      "Retry": [
        {
          "ErrorEquals": ["States.ALL"],
          "IntervalSeconds": 5,
          "MaxAttempts": 2,
          "BackoffRate": 2
        }
      ],
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError",
          "ResultPath": "$.error"
        }
      ],
      "Next": "SendSuccessNotification"
    },

    "SendSuccessNotification": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:send-notification",
      "TimeoutSeconds": 30,
      "Next": "ProcessingComplete"
    },

    "MoveToFailed": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:move-to-failed",
      "TimeoutSeconds": 30,
      "Next": "SendFailureNotification"
    },

    "SendFailureNotification": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:send-notification",
      "TimeoutSeconds": 30,
      "Next": "ValidationFailed"
    },

    "ProcessingComplete": {
      "Type": "Succeed"
    },

    "ValidationFailed": {
      "Type": "Fail",
      "Error": "ValidationFailed",
      "Cause": "Schema validation failed"
    },

    "HandleError": {
      "Type": "Fail",
      "Error": "ProcessingError",
      "Cause": "Unexpected error during pipeline processing"
    }
  }
}
```

3. ‡∏Å‡∏î **Save** ‚Üí **Save anyway** (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ warning)

---

### ‚úÖ STEP 5 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á CloudWatch Dashboard

1. ‡πÄ‡∏Ç‡πâ‡∏≤ **CloudWatch** ‚Üí **Dashboards** ‚Üí **Create dashboard**
2. Name: `csv-pipeline-dashboard` ‚Üí **Create dashboard**
3. ‡πÄ‡∏û‡∏¥‡πà‡∏° Widgets ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö:

**Widget 1: Lambda Invocations Overview (Line Chart)**
- Add widget ‚Üí Line
- Metric: Lambda ‚Üí By Function Name
- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: `csv-validator`, `csv-processor`, `parse-csv`, `transform-data`, `send-notification`
- Metric name: Invocations
- Title: "Lambda Invocations"

**Widget 2: Lambda Errors (Line Chart)**
- ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡πÅ‡∏ï‡πà Metric: Errors
- Title: "Lambda Errors"

**Widget 3: SQS Queue Depth (Line Chart)**
- Metric: SQS ‚Üí ApproximateNumberOfMessagesVisible
- Queue: pipeline-main-queue
- Title: "SQS Queue Depth"

**Widget 4: DLQ Messages (Number)**
- Metric: SQS ‚Üí ApproximateNumberOfMessagesVisible
- Queue: pipeline-dlq
- Title: "Dead Letter Queue"

**Widget 5: Step Functions Executions (Line Chart)**
- Metric: Step Functions ‚Üí ExecutionsStarted, ExecutionsSucceeded, ExecutionsFailed
- Title: "Step Functions Executions"

4. ‡∏Å‡∏î **Save dashboard**

---

### ‚úÖ STEP 6 ‚Äî ‡∏ï‡∏±‡πâ‡∏á CloudWatch Alarms

**Alarm 1: Lambda Error Rate ‡∏™‡∏π‡∏á**

1. **CloudWatch** ‚Üí **Alarms** ‚Üí **Create alarm**
2. Select metric ‚Üí Lambda ‚Üí By Function Name ‚Üí `transform-data` ‚Üí Errors
3. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î:
   ```
   Statistic: Sum
   Period: 5 minutes
   Threshold: Greater than 2
   Datapoints to alarm: 1 out of 1
   ```
4. Notification: ‡∏™‡πà‡∏á‡πÑ‡∏õ SNS Topic `pipeline-notifications`
5. Alarm name: `pipeline-transform-errors`

**Alarm 2: DLQ ‡∏°‡∏µ Messages**

1. Create alarm ‚Üí SQS ‚Üí `pipeline-dlq` ‚Üí ApproximateNumberOfMessagesVisible
2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î:
   ```
   Statistic: Maximum
   Period: 1 minute  
   Threshold: Greater than 0
   ```
3. Notification: ‡∏™‡πà‡∏á‡πÑ‡∏õ SNS Topic ‡πÄ‡∏î‡∏¥‡∏°
4. Alarm name: `pipeline-dlq-messages`

**Alarm 3: Step Functions Failures**

1. Create alarm ‚Üí Step Functions ‚Üí `ProcessCSVWorkflow` ‚Üí ExecutionsFailed
2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î:
   ```
   Statistic: Sum
   Period: 5 minutes
   Threshold: Greater than 0
   ```
3. Alarm name: `pipeline-execution-failures`

---

### ‚úÖ STEP 7 ‚Äî End-to-End Test ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå

‡∏ó‡∏î‡∏™‡∏≠‡∏ö Full Pipeline ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:

**Test Case 1: Happy Path (‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)**
1. ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î `sample_sales.csv` ‡πÑ‡∏õ‡∏ó‡∏µ‡πà `uploads/`
2. ‡∏£‡∏≠ ~30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:
   - [ ] CloudWatch Logs: `csv-validator` ‚Üí queued
   - [ ] CloudWatch Logs: `csv-processor` ‚Üí execution started
   - [ ] Step Functions: Execution ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
   - [ ] S3 `processed/`: ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå `_result.json`
   - [ ] Email: ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö Notification ‚úÖ

**Test Case 2: Schema Error**
1. ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î CSV ‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î Column
2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:
   - [ ] Step Functions: ‡πÑ‡∏õ‡πÄ‡∏™‡πâ‡∏ô `MoveToFailed`
   - [ ] S3: ‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ `failed/`
   - [ ] Email: ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö Notification ‚ùå ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏

**Test Case 3: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö DLQ**
1. ‡∏ó‡∏≥‡πÉ‡∏´‡πâ `parse-csv` Lambda Error ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
2. ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå
3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:
   - [ ] SQS DLQ: ‡∏°‡∏µ Message
   - [ ] CloudWatch Alarm: ‡∏ñ‡∏π‡∏Å Trigger
   - [ ] Email Alert: ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö Alarm notification

---

### üìù ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° Week 4

1. **‡πÄ‡∏û‡∏¥‡πà‡∏° SMS Notification:** ‡∏™‡∏£‡πâ‡∏≤‡∏á SNS Subscription ‡πÅ‡∏ö‡∏ö SMS (‡πÉ‡∏ä‡πâ‡πÄ‡∏ö‡∏≠‡∏£‡πå‡∏°‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠‡∏à‡∏£‡∏¥‡∏á)
2. **‡∏™‡∏£‡πâ‡∏≤‡∏á CloudWatch Metric Filter:** ‡∏î‡∏∂‡∏á Custom Metric ‡∏à‡∏≤‡∏Å Log ‡πÄ‡∏ä‡πà‡∏ô ‡∏ô‡∏±‡∏ö `"event": "file_validated"` ‡∏ï‡πà‡∏≠‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
3. **‡πÄ‡∏û‡∏¥‡πà‡∏° S3 Lifecycle Policy:** ‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô `failed/` ‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏á 30 ‡∏ß‡∏±‡∏ô

### ‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏¢ LAB Week 4

1. SNS Subscription ‡πÅ‡∏ö‡∏ö Email ‡∏Å‡∏±‡∏ö‡πÅ‡∏ö‡∏ö SQS ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á‡πÉ‡∏ô‡πÅ‡∏á‡πà Reliability? ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏ñ‡πâ‡∏≤ Email Server Down?
2. CloudWatch Alarm ‡∏°‡∏µ‡∏Å‡∏µ‡πà State? ‡πÅ‡∏ï‡πà‡∏•‡∏∞ State ‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏≠‡∏∞‡πÑ‡∏£?
3. ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Monitor Latency ‡∏Ç‡∏≠‡∏á Pipeline ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà Upload ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á Email ‡∏™‡πà‡∏á) ‡∏à‡∏∞‡∏ß‡∏±‡∏î‡πÑ‡∏î‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á?

---

---

# üèóÔ∏è Final Architecture & Summary

---

## Architecture Diagram (‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AWS Data Pipeline Architecture                ‚îÇ
‚îÇ                      (All Serverless, Free Tier)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[User]
  ‚îÇ  Upload CSV
  ‚ñº
[S3: raw-uploads/]                         [CloudWatch]
  ‚îÇ  ObjectCreated Event                        ‚îÇ
  ‚îÇ                                    Monitors everything
  ‚ñº                                             ‚îÇ
[Lambda: csv-validator]  ‚îÄ‚îÄLogs‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ
  ‚îÇ  Validate + Send Message                    ‚îÇ
  ‚ñº                                             ‚îÇ
[SQS: pipeline-main-queue]  ‚îÄMetrics‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ
  ‚îÇ  (DLQ: pipeline-dlq)                        ‚îÇ
  ‚îÇ  SQS Trigger                                ‚îÇ
  ‚ñº                                             ‚îÇ
[Lambda: csv-processor]  ‚îÄ‚îÄLogs‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇ
  ‚îÇ  Start Execution                            ‚îÇ
  ‚ñº                                             ‚îÇ
[Step Functions: ProcessCSVWorkflow]  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Execution Metrics
  ‚îÇ
  ‚îú‚îÄ[Lambda: parse-csv]
  ‚îÇ
  ‚îú‚îÄ[Lambda: validate-schema]
  ‚îÇ         ‚îÇ
  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  Valid    Invalid
  ‚îÇ    ‚îÇ         ‚îÇ
  ‚îÇ    ‚ñº         ‚ñº
  ‚îÇ [Lambda:  [Lambda:
  ‚îÇ  transform] move-to-failed]
  ‚îÇ    ‚îÇ         ‚îÇ
  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚îÇ         ‚îÇ
  ‚ñº         ‚ñº
[Lambda: send-notification]
  ‚îÇ
  ‚ñº
[SNS: pipeline-notifications]
  ‚îÇ              ‚îÇ
  ‚ñº              ‚ñº
[Email]       [SMS / Lambda / SQS ...]

Output: [S3: processed/YYYY/MM/DD/*.json]
Failed: [S3: raw-uploads/failed/YYYY/MM/DD/*.csv]
```

---

## Services ‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏±‡πâ‡∏á 4 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå

| AWS Service | ‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£ | ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå | Free Tier |
|-------------|-----------|---------|-----------|
| S3 | ‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏î‡∏¥‡∏ö, ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå | Week 1 | 5 GB / ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô |
| Lambda | Business Logic ‡∏ó‡∏∏‡∏Å Step | Week 1‚Äì4 | 1M invocations / ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô |
| IAM | ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå (Least Privilege) | Week 1 | ‡∏ü‡∏£‡∏µ‡πÄ‡∏™‡∏°‡∏≠ |
| SQS | Decouple + Queue | Week 2 | 1M requests / ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô |
| Step Functions | Orchestrate Workflow | Week 3 | 4,000 transitions / ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô |
| SNS | Notification | Week 4 | 1M publishes / ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô |
| CloudWatch | Monitor + Alert | Week 4 | 5 GB logs / ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô |

---

## Cost Estimation (‡∏´‡∏•‡∏±‡∏á Free Tier ‡∏´‡∏°‡∏î)

‡∏™‡∏°‡∏°‡∏ï‡∏¥ Pipeline ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå 100 ‡πÑ‡∏ü‡∏•‡πå/‡∏ß‡∏±‡∏ô:

| Service | Estimated Usage | Estimated Cost |
|---------|----------------|----------------|
| S3 | 1 GB storage + transfers | ~$0.02/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô |
| Lambda | 5 functions √ó 100 files √ó 30 days = 15,000 invocations | ‡∏ü‡∏£‡∏µ (‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 1M) |
| SQS | 100 messages/‡∏ß‡∏±‡∏ô = 3,000/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô | ‡∏ü‡∏£‡∏µ (‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 1M) |
| Step Functions | 100 executions √ó 7 transitions = 700/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô | ‡∏ü‡∏£‡∏µ (‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 4,000) |
| SNS | 100 emails/‡∏ß‡∏±‡∏ô = 3,000/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô | ‡∏ü‡∏£‡∏µ (‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 1,000 ‚Äî ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô ~$0.002/notification) |
| CloudWatch | Logs + Metrics | ~$0.50/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô |
| **‡∏£‡∏ß‡∏°** | | **< $1/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô** |

> üí∞ Pipeline ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏°‡∏≤‡∏Å‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Serverless ‚Äî ‡∏à‡πà‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ Server ‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤

---

## Design Patterns ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏°‡∏≤‡∏ï‡∏•‡∏≠‡∏î 4 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå

### 1. Event-Driven Pattern (Week 1)
```
Trigger ‚îÄ‚îÄ> Function ‚îÄ‚îÄ> Output
```
‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠: ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

### 2. Message Queue Pattern (Week 2)
```
Producer ‚îÄ‚îÄ> Queue ‚îÄ‚îÄ> Consumer
```
‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠: ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Decouple ‡∏£‡∏∞‡∏ö‡∏ö ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Load ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠

### 3. Orchestration Pattern (Week 3)
```
Orchestrator ‚îÄ‚îÄ> Step A ‚îÄ‚îÄ> Step B ‚îÄ‚îÄ> Step C
```
‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠: ‡∏°‡∏µ Workflow ‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Error Handling ‡∏£‡∏ß‡∏°‡∏®‡∏π‡∏ô‡∏¢‡πå

### 4. Fan-out Notification Pattern (Week 4)
```
Event ‚îÄ‚îÄ> SNS ‚îÄ‚îÄ> [Email, SMS, Lambda, SQS, ...]
```
‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠: ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏´‡∏•‡∏≤‡∏¢ Channel ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô

---

## üöÄ Next Steps ‚Äî ‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô

- **EventBridge:** ‡πÅ‡∏ó‡∏ô S3 Events ‡∏î‡πâ‡∏ß‡∏¢ EventBridge Rules ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Multi-Source Events
- **Kinesis Data Streams:** ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Real-time Stream (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà File-based)
- **Glue + Athena:** Query ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô S3 ‡∏î‡πâ‡∏ß‡∏¢ SQL ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Load ‡πÄ‡∏Ç‡πâ‡∏≤ Database
- **DynamoDB:** ‡πÄ‡∏Å‡πá‡∏ö Job Status ‡πÅ‡∏•‡∏∞ Metadata ‡πÉ‡∏´‡πâ Query ‡πÑ‡∏î‡πâ
- **API Gateway:** ‡πÄ‡∏û‡∏¥‡πà‡∏° REST API ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Trigger Pipeline ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏π‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Job
- **CDK (Cloud Development Kit):** ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Infrastructure ‡πÄ‡∏õ‡πá‡∏ô Code ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏¥‡∏Å Console
- **X-Ray:** Distributed Tracing ‡∏î‡∏π Latency ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Step

---

## üèÜ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏£‡∏ö 4 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå

- ‚úÖ ‡∏°‡∏µ Data Pipeline ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ 100% Serverless ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ Server ‡∏î‡∏π‡πÅ‡∏•‡πÄ‡∏•‡∏¢
- ‚úÖ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à 4 Design Patterns ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö Distributed
- ‚úÖ ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Lambda Python ‡∏ó‡∏µ‡πà‡∏°‡∏µ Structured Logging ‡πÅ‡∏•‡∏∞ Error Handling
- ‚úÖ ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö IAM Policy ‡πÅ‡∏ö‡∏ö Least Privilege ‡πÑ‡∏î‡πâ
- ‚úÖ Debug ‡∏î‡πâ‡∏ß‡∏¢ CloudWatch Logs ‡πÅ‡∏•‡∏∞ Step Functions Visual Execution
- ‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≠‡∏ö **AWS Cloud Practitioner** ‡∏´‡∏£‡∏∑‡∏≠ **AWS Solutions Architect Associate**

---

*‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô AWS Cloud ¬∑ Free Tier ¬∑ 4 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå*
