# ICT24467 â€” à¸à¸²à¸£à¸à¸±à¸’à¸™à¸²à¸‹à¸­à¸Ÿà¸•à¹Œà¹à¸§à¸£à¹Œà¸£à¸°à¸šà¸šà¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸„à¸¥à¸²à¸§à¸”à¹Œà¹à¸¥à¸°à¸„à¸§à¸²à¸¡à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥

## à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¸›à¸£à¸°à¸à¸­à¸šà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸à¸²à¸£à¸ªà¸­à¸™: 
# ğŸš€ AWS Cloud Workshop â€” Data Pipeline & Serverless Architecture
### à¹à¸œà¸™à¸à¸²à¸£à¸ªà¸­à¸™ LAB à¸•à¹ˆà¸­à¹€à¸™à¸·à¹ˆà¸­à¸‡ 4 à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ (Week8, 9, 10, 11) Â· AWS Free Tier à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”

---

> **à¸„à¸“à¸°à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µà¸ªà¸²à¸£à¸ªà¸™à¹€à¸—à¸¨ (School of Information Technology)**  
> à¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢à¸¨à¸£à¸µà¸›à¸—à¸¸à¸¡ Â· Sripatum University  
>
> | | |
> |---|---|
> | **à¸£à¸²à¸¢à¸§à¸´à¸Šà¸²** | ICT24467 â€” Cloud Computing Software Development and Data Security |
> | **à¸ à¸²à¸„à¸à¸²à¸£à¸¨à¸¶à¸à¸©à¸²** | 2 / 2568 |
> | **à¸­à¸²à¸ˆà¸²à¸£à¸¢à¹Œà¸œà¸¹à¹‰à¸ªà¸­à¸™** | à¸­.à¸­à¸³à¸™à¸²à¸ˆ à¸„à¸‡à¹€à¸ˆà¸£à¸´à¸à¸–à¸´à¹ˆà¸™ |
> | **à¸£à¸°à¸”à¸±à¸šà¸Šà¸±à¹‰à¸™** | à¸™à¸±à¸à¸¨à¸¶à¸à¸©à¸²à¸›à¸£à¸´à¸à¸à¸²à¸•à¸£à¸µ à¸„à¸“à¸° ICT |
> | **à¸£à¸°à¸¢à¸°à¹€à¸§à¸¥à¸²** | 4 à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ Â· à¹ƒà¸Šà¹‰ AWS Free Tier à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” |

> **Prerequisite:** à¸™à¸±à¸à¹€à¸£à¸µà¸¢à¸™à¸œà¹ˆà¸²à¸™à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™ EC2 à¹à¸¥à¸° Lambda + API Gateway à¹€à¸šà¸·à¹‰à¸­à¸‡à¸•à¹‰à¸™à¸¡à¸²à¹à¸¥à¹‰à¸§ (Week6-7)

---

## ğŸ“‘ à¸ªà¸²à¸£à¸šà¸±à¸

| | à¸«à¸±à¸§à¸‚à¹‰à¸­ |
|---|---|
| **à¸ à¸²à¸à¸£à¸§à¸¡** | |
| | [à¸ à¸²à¸à¸£à¸§à¸¡à¸«à¸¥à¸±à¸à¸ªà¸¹à¸•à¸£](#-à¸ à¸²à¸à¸£à¸§à¸¡à¸«à¸¥à¸±à¸à¸ªà¸¹à¸•à¸£) |
| | [Architecture Overview](#architecture-overview-final--à¸«à¸¥à¸±à¸‡à¸—à¸³à¸„à¸£à¸š-4-à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ) |
| **Week 1** | [Event-Driven with S3 + Lambda](#-week-1--event-driven-with-s3--lambda) |
| | [STEP 1 â€” à¸ªà¸£à¹‰à¸²à¸‡ S3 Buckets](#-step-1--à¸ªà¸£à¹‰à¸²à¸‡-s3-buckets) |
| | [STEP 2 â€” à¸ªà¸£à¹‰à¸²à¸‡ IAM Role à¸ªà¸³à¸«à¸£à¸±à¸š Lambda](#-step-2--à¸ªà¸£à¹‰à¸²à¸‡-iam-role-à¸ªà¸³à¸«à¸£à¸±à¸š-lambda) |
| | [STEP 3 â€” à¸ªà¸£à¹‰à¸²à¸‡ Lambda Function](#-step-3--à¸ªà¸£à¹‰à¸²à¸‡-lambda-function) |
| | [STEP 4 â€” à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² S3 Event Trigger](#-step-4--à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²-s3-event-trigger) |
| | [STEP 5 â€” à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œ CSV à¸—à¸”à¸ªà¸­à¸š](#-step-5--à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œ-csv-à¸—à¸”à¸ªà¸­à¸š) |
| | [STEP 6 â€” à¸—à¸”à¸ªà¸­à¸š Pipeline](#-step-6--à¸—à¸”à¸ªà¸­à¸š-pipeline) |
| | [STEP 7 â€” à¸—à¸”à¸ªà¸­à¸š Lambda à¹‚à¸”à¸¢à¸•à¸£à¸‡ (Manual Test)](#-step-7--à¸—à¸”à¸ªà¸­à¸š-lambda-à¹‚à¸”à¸¢à¸•à¸£à¸‡-manual-test) |
| | [à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡ Week 1](#-à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡-week-1) |
| **Week 2** | [Message Queue with SQS](#-week-2--message-queue-with-sqs) |
| | [STEP 1 â€” à¸ªà¸£à¹‰à¸²à¸‡ Dead Letter Queue à¸à¹ˆà¸­à¸™](#-step-1--à¸ªà¸£à¹‰à¸²à¸‡-dead-letter-queue-à¸à¹ˆà¸­à¸™) |
| | [STEP 2 â€” à¸ªà¸£à¹‰à¸²à¸‡ Main Queue](#-step-2--à¸ªà¸£à¹‰à¸²à¸‡-main-queue) |
| | [STEP 3 â€” à¸­à¸±à¸›à¹€à¸”à¸• IAM Policy](#-step-3--à¸­à¸±à¸›à¹€à¸”à¸•-iam-policy) |
| | [STEP 4 â€” à¸­à¸±à¸›à¹€à¸”à¸• Lambda Validator à¹ƒà¸«à¹‰à¸ªà¹ˆà¸‡ Message à¹„à¸› SQS](#-step-4--à¸­à¸±à¸›à¹€à¸”à¸•-lambda-validator-à¹ƒà¸«à¹‰à¸ªà¹ˆà¸‡-message-à¹„à¸›-sqs) |
| | [STEP 5 â€” à¸ªà¸£à¹‰à¸²à¸‡ Lambda Consumer (csv-processor)](#-step-5--à¸ªà¸£à¹‰à¸²à¸‡-lambda-consumer-csv-processor) |
| | [STEP 6 â€” à¹€à¸Šà¸·à¹ˆà¸­à¸¡ SQS à¸à¸±à¸š Lambda Consumer](#-step-6--à¹€à¸Šà¸·à¹ˆà¸­à¸¡-sqs-à¸à¸±à¸š-lambda-consumer) |
| | [STEP 7 â€” à¸—à¸”à¸ªà¸­à¸š End-to-End Pipeline](#-step-7--à¸—à¸”à¸ªà¸­à¸š-end-to-end-pipeline) |
| | [STEP 8 â€” à¸—à¸”à¸ªà¸­à¸š Dead Letter Queue](#-step-8--à¸—à¸”à¸ªà¸­à¸š-dead-letter-queue) |
| | [à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡ Week 2](#-à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡-week-2) |
| **Week 3** | [Orchestration with Step Functions](#-week-3--orchestration-with-step-functions) |
| | [STEP 1 â€” à¸ªà¸£à¹‰à¸²à¸‡ Lambda Functions à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸•à¹ˆà¸¥à¸° Step](#-step-1--à¸ªà¸£à¹‰à¸²à¸‡-lambda-functions-à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸•à¹ˆà¸¥à¸°-step) |
| | [STEP 2 â€” à¸­à¸±à¸›à¹€à¸”à¸• IAM Policy](#-step-2--à¸­à¸±à¸›à¹€à¸”à¸•-iam-policy) |
| | [STEP 3 â€” à¸ªà¸£à¹‰à¸²à¸‡ State Machine](#-step-3--à¸ªà¸£à¹‰à¸²à¸‡-state-machine) |
| | [STEP 4 â€” à¸­à¸±à¸›à¹€à¸”à¸• SQS Consumer à¹ƒà¸«à¹‰à¹€à¸£à¸´à¹ˆà¸¡ Step Functions](#-step-4--à¸­à¸±à¸›à¹€à¸”à¸•-sqs-consumer-à¹ƒà¸«à¹‰à¹€à¸£à¸´à¹ˆà¸¡-step-functions) |
| | [STEP 5 â€” à¸—à¸”à¸ªà¸­à¸š State Machine](#-step-5--à¸—à¸”à¸ªà¸­à¸š-state-machine) |
| | [à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡ Week 3](#-à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡-week-3) |
| **Week 4** | [Notification & Recap](#-week-4--notification--recap) |
| | [STEP 1 â€” à¸ªà¸£à¹‰à¸²à¸‡ SNS Topic](#-step-1--à¸ªà¸£à¹‰à¸²à¸‡-sns-topic) |
| | [STEP 2 â€” à¸­à¸±à¸›à¹€à¸”à¸• IAM Policy à¹€à¸à¸´à¹ˆà¸¡ SNS](#-step-2--à¸­à¸±à¸›à¹€à¸”à¸•-iam-policy-à¹€à¸à¸´à¹ˆà¸¡-sns) |
| | [STEP 3 â€” à¸ªà¸£à¹‰à¸²à¸‡ Lambda Notification](#-step-3--à¸ªà¸£à¹‰à¸²à¸‡-lambda-notification) |
| | [STEP 4 â€” à¸­à¸±à¸›à¹€à¸”à¸• Step Functions State Machine](#-step-4--à¸­à¸±à¸›à¹€à¸”à¸•-step-functions-state-machine) |
| | [STEP 5 â€” à¸ªà¸£à¹‰à¸²à¸‡ CloudWatch Dashboard](#-step-5--à¸ªà¸£à¹‰à¸²à¸‡-cloudwatch-dashboard) |
| | [STEP 6 â€” à¸•à¸±à¹‰à¸‡ CloudWatch Alarms](#-step-6--à¸•à¸±à¹‰à¸‡-cloudwatch-alarms) |
| | [STEP 7 â€” End-to-End Test à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ](#-step-7--end-to-end-test-à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ) |
| | [à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡ Week 4](#-à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡-week-4) |
| **à¸ªà¸£à¸¸à¸›** | [Final Architecture & Summary](#ï¸-final-architecture--summary) |
| | [Services à¸ªà¸£à¸¸à¸›à¸—à¸±à¹‰à¸‡ 4 à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ](#services-à¸ªà¸£à¸¸à¸›à¸—à¸±à¹‰à¸‡-4-à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ) |
| | [Design Patterns à¸—à¸µà¹ˆà¹€à¸£à¸µà¸¢à¸™à¸¡à¸²à¸•à¸¥à¸­à¸” 4 à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ](#design-patterns-à¸—à¸µà¹ˆà¹€à¸£à¸µà¸¢à¸™à¸¡à¸²à¸•à¸¥à¸­à¸”-4-à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ) |
| | [Next Steps â€” à¸•à¹ˆà¸­à¸¢à¸­à¸”à¸ˆà¸²à¸à¸—à¸µà¹ˆà¹€à¸£à¸µà¸¢à¸™](#-next-steps--à¸•à¹ˆà¸­à¸¢à¸­à¸”à¸ˆà¸²à¸à¸—à¸µà¹ˆà¹€à¸£à¸µà¸¢à¸™) |

---

## ğŸ“‹ à¸ à¸²à¸à¸£à¸§à¸¡à¸«à¸¥à¸±à¸à¸ªà¸¹à¸•à¸£

à¸«à¸¥à¸±à¸à¸ªà¸¹à¸•à¸£à¸™à¸µà¹‰à¸ˆà¸°à¸à¸²à¸™à¸±à¸à¹€à¸£à¸µà¸¢à¸™à¸ªà¸£à¹‰à¸²à¸‡ **Data Pipeline à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´** à¹à¸šà¸š Serverless à¹€à¸•à¹‡à¸¡à¸•à¸±à¸§ à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸¡à¸µ Server à¹€à¸”à¸µà¸¢à¸§ à¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆà¸£à¸±à¸šà¹„à¸Ÿà¸¥à¹Œ â†’ à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ â†’ à¸„à¸´à¸§ â†’ Orchestrate â†’ à¹à¸ˆà¹‰à¸‡à¹€à¸•à¸·à¸­à¸™ à¸—à¸¸à¸à¸­à¸¢à¹ˆà¸²à¸‡à¸•à¹ˆà¸­à¸à¸±à¸™à¹€à¸›à¹‡à¸™ Project à¹€à¸”à¸µà¸¢à¸§à¸„à¸·à¸­ **"CSV Report Processor"** â€” à¸£à¸°à¸šà¸šà¸—à¸µà¹ˆà¸£à¸±à¸šà¹„à¸Ÿà¸¥à¹Œ CSV à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¸‚à¸¶à¹‰à¸™ S3 à¹à¸¥à¹‰à¸§à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹à¸¥à¸°à¸ªà¹ˆà¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸—à¸²à¸‡ Email à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´

| à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ | à¸«à¸±à¸§à¸‚à¹‰à¸­ | Services à¸«à¸¥à¸±à¸ | à¹€à¸§à¸¥à¸²à¹‚à¸”à¸¢à¸›à¸£à¸°à¸¡à¸²à¸“ |
|--------|--------|---------------|--------------|
| Week 1 | Event-Driven with S3 + Lambda | S3, Lambda, IAM | 2â€“3 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡ |
| Week 2 | Message Queue with SQS | SQS, Lambda | 2â€“3 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡ |
| Week 3 | Orchestration with Step Functions | Step Functions, Lambda | 2â€“3 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡ |
| Week 4 | Notification & Recap | SNS, CloudWatch, Architecture Review | 2â€“3 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡ |

### ğŸ¯ à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢à¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡
- à¸™à¸±à¸à¹€à¸£à¸µà¸¢à¸™à¸¡à¸µ **Data Pipeline à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´** à¸—à¸µà¹ˆà¸£à¸±à¸šà¹„à¸Ÿà¸¥à¹Œ â†’ à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ â†’ à¹à¸ˆà¹‰à¸‡à¹€à¸•à¸·à¸­à¸™ à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸¡à¸µ Server à¹€à¸¥à¸¢
- à¸•à¹‰à¸™à¸—à¸¸à¸™ **$0** à¸•à¸¥à¸­à¸”à¸«à¸¥à¸±à¸à¸ªà¸¹à¸•à¸£ (AWS Free Tier à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”)
- à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆ Pattern à¸ªà¸³à¸„à¸±à¸: **Event-Driven, Decoupling, Orchestration, Observability**
- à¸•à¹ˆà¸­à¸¢à¸­à¸”à¸ªà¸¹à¹ˆ Production Pipeline à¸ˆà¸£à¸´à¸‡à¹„à¸”à¹‰

### ğŸ› ï¸ à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¹€à¸•à¸£à¸µà¸¢à¸¡à¸à¹ˆà¸­à¸™à¹€à¸£à¸´à¹ˆà¸¡
- AWS Account (Free Tier)
- Python 3.x à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹ƒà¸™à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡
- VS Code à¸«à¸£à¸·à¸­ Editor à¸—à¸µà¹ˆà¸–à¸™à¸±à¸”
- à¹„à¸Ÿà¸¥à¹Œ CSV à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸”à¸ªà¸­à¸š (à¸ªà¸£à¹‰à¸²à¸‡à¹€à¸­à¸‡à¹„à¸”à¹‰à¸‡à¹ˆà¸²à¸¢à¹†)
- Email à¸ˆà¸£à¸´à¸‡à¸ªà¸³à¸«à¸£à¸±à¸šà¸£à¸±à¸š SNS Notification (Week 4)

### âš ï¸ Free Tier Limits à¸—à¸µà¹ˆà¸„à¸§à¸£à¸£à¸¹à¹‰
| Service | Free Tier |
|---------|-----------|
| S3 | 5 GB storage, 20,000 GET, 2,000 PUT /à¹€à¸”à¸·à¸­à¸™ |
| Lambda | 1 million invocations, 400,000 GB-seconds /à¹€à¸”à¸·à¸­à¸™ à¸•à¸¥à¸­à¸”à¸Šà¸µà¸ |
| SQS | 1 million requests /à¹€à¸”à¸·à¸­à¸™ à¸•à¸¥à¸­à¸”à¸Šà¸µà¸ |
| Step Functions | 4,000 state transitions /à¹€à¸”à¸·à¸­à¸™ à¸•à¸¥à¸­à¸”à¸Šà¸µà¸ |
| SNS | 1 million publishes, 1,000 email deliveries /à¹€à¸”à¸·à¸­à¸™ à¸•à¸¥à¸­à¸”à¸Šà¸µà¸ |
| CloudWatch | 5 GB log ingest, 10 custom metrics /à¹€à¸”à¸·à¸­à¸™ (12 à¹€à¸”à¸·à¸­à¸™à¹à¸£à¸) |

---

## Architecture Overview (Final â€” à¸«à¸¥à¸±à¸‡à¸—à¸³à¸„à¸£à¸š 4 à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ)

```
[User] â”€â”€Upload CSVâ”€â”€> [S3: raw-uploads/]
                              â”‚
                    S3 Event Trigger
                              â”‚
                              â–¼
                     [Lambda: Validator]
                              â”‚
                    à¸ªà¹ˆà¸‡ Message à¹€à¸‚à¹‰à¸² Queue
                              â”‚
                              â–¼
                       [SQS Queue]
                              â”‚
                  SQS Trigger (Polling)
                              â”‚
                              â–¼
                  [Step Functions: Workflow]
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  1. Parse CSV    â”‚
                    â”‚  2. Transform    â”‚
                    â”‚  3. Save Result  â”‚
                    â”‚  4. Notify       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                   â–¼
           [S3: processed/]      [SNS Topic]
                                        â”‚
                                        â–¼
                                 [Email / SMS]

[CloudWatch] â”€â”€â”€â”€ Monitors â”€â”€â”€â”€> à¸—à¸¸à¸ Service
```

---

---

# ğŸ“… WEEK 1 â€” Event-Driven with S3 + Lambda

> **à¸«à¸±à¸§à¸‚à¹‰à¸­:** à¸ªà¸£à¹‰à¸²à¸‡ Trigger à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´à¹€à¸¡à¸·à¹ˆà¸­ Upload à¹„à¸Ÿà¸¥à¹Œà¸‚à¸¶à¹‰à¸™ S3

---

## ğŸ“ à¸§à¸±à¸•à¸–à¸¸à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰
- à¸­à¸˜à¸´à¸šà¸²à¸¢ Event-Driven Architecture à¹à¸¥à¸°à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œà¹€à¸—à¸µà¸¢à¸šà¸à¸±à¸š Polling à¹„à¸”à¹‰
- à¸ªà¸£à¹‰à¸²à¸‡ S3 Bucket à¹à¸¥à¸°à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Event Notification à¹„à¸›à¸¢à¸±à¸‡ Lambda
- à¹€à¸‚à¸µà¸¢à¸™ Lambda à¸—à¸µà¹ˆà¸£à¸±à¸š S3 Event à¹à¸¥à¸°à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸Ÿà¸¥à¹Œ CSV
- à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² IAM Role à¹ƒà¸«à¹‰ Lambda à¸¡à¸µà¸ªà¸´à¸—à¸˜à¸´à¹Œà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸•à¸²à¸¡ Least Privilege
- à¸”à¸¹ CloudWatch Logs à¹€à¸à¸·à¹ˆà¸­ Debug Lambda

---

## ğŸ“– à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸à¸·à¹‰à¸™à¸à¸²à¸™ (Lecture â€” 30 à¸™à¸²à¸—à¸µ)

### Event-Driven Architecture à¸„à¸·à¸­à¸­à¸°à¹„à¸£?

à¹à¸—à¸™à¸—à¸µà¹ˆà¸ˆà¸°à¹ƒà¸«à¹‰à¸£à¸°à¸šà¸š "à¸–à¸²à¸¡à¸‹à¹‰à¸³à¹†" à¸§à¹ˆà¸²à¸¡à¸µà¸‡à¸²à¸™à¹ƒà¸«à¸¡à¹ˆà¹„à¸«à¸¡ (Polling) Event-Driven Architecture à¹ƒà¸«à¹‰à¸£à¸°à¸šà¸š "à¸•à¸­à¸šà¸ªà¸™à¸­à¸‡à¹€à¸¡à¸·à¹ˆà¸­à¸¡à¸µà¹€à¸«à¸•à¸¸à¸à¸²à¸£à¸“à¹Œà¹€à¸à¸´à¸”à¸‚à¸¶à¹‰à¸™" à¸—à¸±à¸™à¸—à¸µ

**à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š:**
- **Polling:** à¹€à¸«à¸¡à¸·à¸­à¸™à¹‚à¸—à¸£à¸–à¸²à¸¡à¸£à¹‰à¸²à¸™à¸­à¸²à¸«à¸²à¸£à¸—à¸¸à¸ 5 à¸™à¸²à¸—à¸µà¸§à¹ˆà¸²à¸­à¸²à¸«à¸²à¸£à¸à¸£à¹‰à¸­à¸¡à¸¢à¸±à¸‡
- **Event-Driven:** à¹€à¸«à¸¡à¸·à¸­à¸™à¹ƒà¸«à¹‰à¸£à¹‰à¸²à¸™à¹‚à¸—à¸£à¸«à¸²à¹€à¸¡à¸·à¹ˆà¸­à¸­à¸²à¸«à¸²à¸£à¸à¸£à¹‰à¸­à¸¡

**à¸‚à¹‰à¸­à¸”à¸µà¸‚à¸­à¸‡ Event-Driven:**
- à¸›à¸£à¸°à¸«à¸¢à¸±à¸” Resource â€” à¸—à¸³à¸‡à¸²à¸™à¹€à¸‰à¸à¸²à¸°à¹€à¸¡à¸·à¹ˆà¸­à¸ˆà¸³à¹€à¸›à¹‡à¸™
- Scale à¹„à¸”à¹‰à¸”à¸µ â€” à¸«à¸¥à¸²à¸¢ Event à¹€à¸à¸´à¸”à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™à¹„à¸”à¹‰
- Loose Coupling â€” à¹à¸•à¹ˆà¸¥à¸°à¸ªà¹ˆà¸§à¸™à¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¸ˆà¸±à¸à¸à¸±à¸™à¹‚à¸”à¸¢à¸•à¸£à¸‡
- à¸•à¹‰à¸™à¸—à¸¸à¸™à¸•à¹ˆà¸³ â€” Lambda à¸ˆà¹ˆà¸²à¸¢à¸•à¸²à¸¡à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸ˆà¸£à¸´à¸‡

### S3 Events à¸„à¸·à¸­à¸­à¸°à¹„à¸£?
S3 à¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¹ˆà¸‡ Notification à¹„à¸›à¸¢à¸±à¸‡ Lambda, SQS, à¸«à¸£à¸·à¸­ SNS à¹„à¸”à¹‰à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´à¹€à¸¡à¸·à¹ˆà¸­:
- `s3:ObjectCreated:*` â€” à¸¡à¸µà¹„à¸Ÿà¸¥à¹Œà¸–à¸¹à¸ Upload
- `s3:ObjectRemoved:*` â€” à¸¡à¸µà¹„à¸Ÿà¸¥à¹Œà¸–à¸¹à¸à¸¥à¸š
- `s3:ObjectRestore:*` â€” à¸¡à¸µà¹„à¸Ÿà¸¥à¹Œà¸–à¸¹à¸ Restore à¸ˆà¸²à¸ Glacier

### Project à¸—à¸µà¹ˆà¸ˆà¸°à¸ªà¸£à¹‰à¸²à¸‡à¹ƒà¸™ Week 1

```
[à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰ Upload sales_data.csv] 
        â”‚
        â–¼
[S3 Bucket: raw-uploads/]
        â”‚
  S3 Event Trigger (ObjectCreated)
        â”‚
        â–¼
[Lambda: csv-validator]
   - à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ .csv à¸ˆà¸£à¸´à¸‡à¹„à¸«à¸¡
   - à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™ rows
   - Log à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¹ƒà¸™ CloudWatch
```

---

## ğŸ”§ LAB Step-by-Step

---

### âœ… STEP 1 â€” à¸ªà¸£à¹‰à¸²à¸‡ S3 Buckets

à¹€à¸£à¸²à¸ˆà¸°à¸ªà¸£à¹‰à¸²à¸‡ 2 Buckets à¹à¸¢à¸à¸à¸±à¸™à¸Šà¸±à¸”à¹€à¸ˆà¸™:

**Bucket 1: à¸ªà¸³à¸«à¸£à¸±à¸šà¸£à¸±à¸šà¹„à¸Ÿà¸¥à¹Œà¸”à¸´à¸š**

1. à¹€à¸‚à¹‰à¸² **AWS Console** â†’ à¸„à¹‰à¸™à¸«à¸² **S3** â†’ **Create bucket**
2. à¸à¸£à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:
   ```
   Bucket name: pipeline-raw-[à¸Šà¸·à¹ˆà¸­à¸„à¸¸à¸“]-[random4digits]
   Region: ap-southeast-1 (Singapore)
   Block all public access: âœ… à¹€à¸›à¸´à¸”à¹„à¸§à¹‰à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” (default)
   Versioning: Disable
   ```
3. à¸à¸” **Create bucket**

**Bucket 2: à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸šà¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ**

1. à¸ªà¸£à¹‰à¸²à¸‡ Bucket à¹ƒà¸«à¸¡à¹ˆà¸­à¸µà¸à¸­à¸±à¸™:
   ```
   Bucket name: pipeline-processed-[à¸Šà¸·à¹ˆà¸­à¸„à¸¸à¸“]-[random4digits]
   Region: ap-southeast-1 (Singapore)
   (Settings à¸­à¸·à¹ˆà¸™à¹€à¸«à¸¡à¸·à¸­à¸™à¸à¸±à¸™)
   ```

**à¸ªà¸£à¹‰à¸²à¸‡ Folder Structure à¹ƒà¸™ Bucket à¹à¸£à¸:**

1. à¸à¸”à¹€à¸‚à¹‰à¸² `pipeline-raw-...`
2. à¸à¸” **Create folder** â†’ à¸Šà¸·à¹ˆà¸­ `uploads/` â†’ Create
3. à¸à¸” **Create folder** à¸­à¸µà¸à¸­à¸±à¸™ â†’ à¸Šà¸·à¹ˆà¸­ `failed/` â†’ Create

> ğŸ’¡ **à¸—à¸³à¹„à¸¡à¸•à¹‰à¸­à¸‡à¹à¸¢à¸ Bucket?** à¹€à¸à¸·à¹ˆà¸­à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ Recursive Loop â€” à¸–à¹‰à¸² Lambda à¸­à¹ˆà¸²à¸™à¸ˆà¸²à¸ Bucket à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸šà¸—à¸µà¹ˆà¸¡à¸±à¸™ Write à¸¥à¸‡ à¸ˆà¸°à¹€à¸à¸´à¸” Event Loop à¹„à¸¡à¹ˆà¸ªà¸´à¹‰à¸™à¸ªà¸¸à¸” à¹à¸¥à¸°à¸„à¹ˆà¸²à¹ƒà¸Šà¹‰à¸ˆà¹ˆà¸²à¸¢à¸à¸¸à¹ˆà¸‡à¸—à¸±à¸™à¸—à¸µ!

---

### âœ… STEP 2 â€” à¸ªà¸£à¹‰à¸²à¸‡ IAM Role à¸ªà¸³à¸«à¸£à¸±à¸š Lambda

Lambda à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸ªà¸´à¸—à¸˜à¸´à¹Œà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¹ƒà¸™à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡ S3 â€” à¹€à¸£à¸²à¸ˆà¸°à¸ªà¸£à¹‰à¸²à¸‡à¹à¸šà¸š Least Privilege

1. à¹€à¸‚à¹‰à¸² **IAM** â†’ **Roles** â†’ **Create role**
2. **Trusted entity:** AWS service â†’ **Lambda** â†’ Next
3. **à¸ªà¸£à¹‰à¸²à¸‡ Custom Policy à¸à¹ˆà¸­à¸™** (à¹à¸—à¸™à¸—à¸µà¹ˆà¸ˆà¸°à¹ƒà¸Šà¹‰ Managed Policy à¸à¸§à¹‰à¸²à¸‡à¹†):
   - à¸„à¸¥à¸´à¸ **Create policy** (à¹€à¸›à¸´à¸” Tab à¹ƒà¸«à¸¡à¹ˆ)
   - à¹€à¸¥à¸·à¸­à¸ **JSON** tab â†’ à¸§à¸²à¸‡à¹‚à¸„à¹‰à¸”à¸™à¸µà¹‰:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "ReadFromRawBucket",
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:HeadObject"
      ],
      "Resource": "arn:aws:s3:::pipeline-raw-[à¸Šà¸·à¹ˆà¸­à¸„à¸¸à¸“]-*/*"
    },
    {
      "Sid": "WriteToProcessedBucket",
      "Effect": "Allow",
      "Action": [
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::pipeline-processed-[à¸Šà¸·à¹ˆà¸­à¸„à¸¸à¸“]-*/*"
    },
    {
      "Sid": "CloudWatchLogs",
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "arn:aws:logs:*:*:*"
    }
  ]
}
```

4. Policy name: `pipeline-lambda-policy` â†’ **Create policy**
5. à¸à¸¥à¸±à¸šà¹„à¸›à¸«à¸™à¹‰à¸² Create Role â†’ Refresh â†’ à¸„à¹‰à¸™à¸«à¸² `pipeline-lambda-policy` â†’ à¹€à¸¥à¸·à¸­à¸ â†’ Next
6. Role name: `pipeline-lambda-role` â†’ **Create role**

> ğŸ”’ **Security Note:** à¹€à¸£à¸²à¸£à¸°à¸šà¸¸ Resource ARN à¹à¸šà¸šà¹€à¸ˆà¸²à¸°à¸ˆà¸‡ à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰ `"Resource": "*"` à¹€à¸à¸£à¸²à¸°à¸–à¹‰à¸² Lambda à¸–à¸¹à¸ Compromise à¸ˆà¸°à¹„à¸”à¹‰à¹„à¸¡à¹ˆà¹€à¸‚à¹‰à¸² Resource à¸­à¸·à¹ˆà¸™à¹„à¸”à¹‰

---

### âœ… STEP 3 â€” à¸ªà¸£à¹‰à¸²à¸‡ Lambda Function

1. à¹€à¸‚à¹‰à¸² **Lambda** â†’ **Create function**
2. à¸à¸£à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:
   ```
   Function name: csv-validator
   Runtime: Python 3.12
   Architecture: x86_64
   Execution role: Use an existing role â†’ pipeline-lambda-role
   ```
3. à¸à¸” **Create function**
4. à¹ƒà¸™ **Code** tab â†’ à¹à¸—à¸™à¸—à¸µà¹ˆà¹‚à¸„à¹‰à¸”à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸”à¹‰à¸§à¸¢:

```python
import json
import boto3
import csv
import io
import logging
from datetime import datetime

# à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Logger
logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client('s3')

def lambda_handler(event, context):
    """
    à¸£à¸±à¸š S3 Event à¹€à¸¡à¸·à¹ˆà¸­à¸¡à¸µà¹„à¸Ÿà¸¥à¹Œà¸–à¸¹à¸ Upload
    à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ CSV à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡ à¹à¸¥à¸° Log à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¸„à¸±à¸
    """
    
    logger.info(f"Received event: {json.dumps(event)}")
    
    results = []
    
    for record in event['Records']:
        # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸Ÿà¸¥à¹Œà¸ˆà¸²à¸ Event
        bucket_name = record['s3']['bucket']['name']
        object_key = record['s3']['object']['key']
        file_size = record['s3']['object']['size']
        event_time = record['eventTime']
        
        logger.info(json.dumps({
            "event": "file_received",
            "bucket": bucket_name,
            "key": object_key,
            "size_bytes": file_size,
            "timestamp": event_time
        }))
        
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸™à¸²à¸¡à¸ªà¸à¸¸à¸¥à¹„à¸Ÿà¸¥à¹Œ
        if not object_key.lower().endswith('.csv'):
            logger.warning(f"File {object_key} is not a CSV file. Skipping.")
            results.append({
                "file": object_key,
                "status": "rejected",
                "reason": "Not a CSV file"
            })
            continue
        
        # à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œà¸ˆà¸²à¸ S3
        try:
            response = s3.get_object(Bucket=bucket_name, Key=object_key)
            content = response['Body'].read().decode('utf-8')
            
            # Parse CSV
            csv_reader = csv.DictReader(io.StringIO(content))
            rows = list(csv_reader)
            headers = csv_reader.fieldnames
            
            # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸£à¸·à¸­à¹€à¸›à¸¥à¹ˆà¸²
            if len(rows) == 0:
                logger.warning(f"File {object_key} is empty (no data rows)")
                results.append({
                    "file": object_key,
                    "status": "rejected",
                    "reason": "Empty file"
                })
                continue
            
            # à¸ªà¸£à¸¸à¸›à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸Ÿà¸¥à¹Œ
            summary = {
                "event": "file_validated",
                "file": object_key,
                "bucket": bucket_name,
                "status": "valid",
                "total_rows": len(rows),
                "headers": list(headers) if headers else [],
                "file_size_bytes": file_size,
                "processed_at": datetime.now().isoformat()
            }
            
            logger.info(json.dumps(summary))
            
            results.append({
                "file": object_key,
                "status": "valid",
                "rows": len(rows),
                "headers": list(headers) if headers else []
            })
            
        except Exception as e:
            logger.error(json.dumps({
                "event": "processing_error",
                "file": object_key,
                "error": str(e)
            }))
            results.append({
                "file": object_key,
                "status": "error",
                "reason": str(e)
            })
    
    logger.info(f"Processing complete. Results: {json.dumps(results)}")
    
    return {
        "statusCode": 200,
        "body": json.dumps({
            "message": f"Processed {len(results)} file(s)",
            "results": results
        })
    }
```

5. à¸à¸” **Deploy**

**à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Timeout:**
- **Configuration** tab â†’ **General configuration** â†’ **Edit**
- Timeout: **30 seconds** (default 3 à¸§à¸´à¸™à¸²à¸—à¸µà¸ªà¸±à¹‰à¸™à¹€à¸à¸´à¸™à¹„à¸›à¸ªà¸³à¸«à¸£à¸±à¸š CSV à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ)
- Memory: **256 MB**
- à¸à¸” **Save**

---

### âœ… STEP 4 â€” à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² S3 Event Trigger

1. à¸¢à¸±à¸‡à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ Lambda â†’ **Configuration** tab â†’ **Triggers** â†’ **Add trigger**
2. à¹€à¸¥à¸·à¸­à¸ **S3**
3. à¸à¸£à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:
   ```
   Bucket: pipeline-raw-[à¸Šà¸·à¹ˆà¸­à¸„à¸¸à¸“]
   Event types: All object create events
   Prefix: uploads/          â† à¸ªà¸³à¸„à¸±à¸à¸¡à¸²à¸! à¸£à¸±à¸šà¹€à¸‰à¸à¸²à¸°à¸ˆà¸²à¸ folder à¸™à¸µà¹‰
   Suffix: .csv              â† à¸£à¸±à¸šà¹€à¸‰à¸à¸²à¸°à¹„à¸Ÿà¸¥à¹Œ .csv
   ```
4. âœ… Acknowledge recursive warning â†’ **Add**

> âš ï¸ **à¸ªà¸³à¸„à¸±à¸à¸¡à¸²à¸:** à¸à¸²à¸£à¸•à¸±à¹‰à¸‡ Prefix `uploads/` à¸—à¸³à¹ƒà¸«à¹‰ Lambda trigger à¹€à¸‰à¸à¸²à¸°à¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ folder à¸™à¸µà¹‰à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ Recursive Loop à¹ƒà¸™à¸à¸£à¸“à¸µà¸—à¸µà¹ˆà¹€à¸‚à¸µà¸¢à¸™à¸à¸¥à¸±à¸šà¸¡à¸²à¸¢à¸±à¸‡ Bucket à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™

---

### âœ… STEP 5 â€” à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œ CSV à¸—à¸”à¸ªà¸­à¸š

à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œ `sample_sales.csv` à¹ƒà¸™à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡:

```csv
order_id,customer_name,product,quantity,price,date
ORD001,à¸ªà¸¡à¸Šà¸²à¸¢ à¹ƒà¸ˆà¸”à¸µ,Widget A,5,250.00,2024-01-15
ORD002,à¸ªà¸¡à¸«à¸à¸´à¸‡ à¸£à¸±à¸à¹€à¸£à¸µà¸¢à¸™,Widget B,2,450.00,2024-01-15
ORD003,à¸§à¸´à¸Šà¸±à¸¢ à¸¡à¸²à¸™à¸°,Widget A,10,250.00,2024-01-16
ORD004,à¸™à¸´à¸”à¸² à¸ªà¸¸à¸‚à¹ƒà¸ˆ,Widget C,1,1200.00,2024-01-16
ORD005,à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¹Œ à¸”à¸µà¸‡à¸²à¸¡,Widget B,3,450.00,2024-01-17
```

---

### âœ… STEP 6 â€” à¸—à¸”à¸ªà¸­à¸š Pipeline

**à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ:**
1. à¹€à¸‚à¹‰à¸² S3 â†’ `pipeline-raw-...` â†’ à¹€à¸‚à¹‰à¸² folder `uploads/`
2. à¸à¸” **Upload** â†’ Add files â†’ à¹€à¸¥à¸·à¸­à¸ `sample_sales.csv` â†’ **Upload**

**à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š CloudWatch Logs:**
1. à¹€à¸‚à¹‰à¸² **CloudWatch** â†’ **Log groups** â†’ `/aws/lambda/csv-validator`
2. à¸à¸” Log Stream à¸¥à¹ˆà¸²à¸ªà¸¸à¸”
3. à¸„à¸§à¸£à¹€à¸«à¹‡à¸™ Log à¹à¸šà¸šà¸™à¸µà¹‰:

```json
{
  "event": "file_validated",
  "file": "uploads/sample_sales.csv",
  "status": "valid",
  "total_rows": 5,
  "headers": ["order_id", "customer_name", "product", "quantity", "price", "date"],
  "file_size_bytes": 312
}
```

**à¸—à¸”à¸ªà¸­à¸šà¸à¸£à¸“à¸µ Error:**
1. à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆ CSV à¹€à¸Šà¹ˆà¸™ `test.txt` à¹„à¸›à¸—à¸µà¹ˆ `uploads/`
2. à¸”à¸¹ Log â€” à¸„à¸§à¸£à¹€à¸«à¹‡à¸™ `"status": "rejected", "reason": "Not a CSV file"`

---

### âœ… STEP 7 â€” à¸—à¸”à¸ªà¸­à¸š Lambda à¹‚à¸”à¸¢à¸•à¸£à¸‡ (Manual Test)

à¸à¹ˆà¸­à¸™ Trigger à¸ˆà¸²à¸ S3 à¸„à¸§à¸£à¸—à¸”à¸ªà¸­à¸š Lambda à¹‚à¸”à¸¢à¸•à¸£à¸‡à¸à¹ˆà¸­à¸™:

1. à¹ƒà¸™ Lambda â†’ **Test** tab â†’ **Create new event**
2. Event name: `test-s3-event`
3. à¸§à¸²à¸‡ JSON à¸™à¸µà¹‰à¹€à¸›à¹‡à¸™ Event:

```json
{
  "Records": [
    {
      "eventVersion": "2.1",
      "eventSource": "aws:s3",
      "eventTime": "2024-01-15T10:00:00.000Z",
      "eventName": "ObjectCreated:Put",
      "s3": {
        "bucket": {
          "name": "pipeline-raw-[à¸Šà¸·à¹ˆà¸­à¸„à¸¸à¸“]-[random]"
        },
        "object": {
          "key": "uploads/sample_sales.csv",
          "size": 312
        }
      }
    }
  ]
}
```

4. à¸à¸” **Test** â†’ à¸”à¸¹à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¹ƒà¸™ Execution results

---

### ğŸ“ à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡ Week 1

1. **à¹à¸à¹‰à¹„à¸‚ Lambda** à¹ƒà¸«à¹‰à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹„à¸Ÿà¸¥à¹Œà¸¡à¸µ Column `order_id` à¸­à¸¢à¸¹à¹ˆà¸«à¸£à¸·à¸­à¹€à¸›à¸¥à¹ˆà¸² à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¹ƒà¸«à¹‰ Reject
2. **à¹€à¸à¸´à¹ˆà¸¡ Metric:** à¹ƒà¸Šà¹‰ `boto3` à¸ªà¹ˆà¸‡ Custom Metric à¹„à¸› CloudWatch à¹€à¸à¸·à¹ˆà¸­à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆ Valid/Invalid à¸•à¹ˆà¸­à¸§à¸±à¸™
3. **à¸—à¸”à¸¥à¸­à¸‡** Upload à¹„à¸Ÿà¸¥à¹Œ CSV à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ (1,000 rows) à¹à¸¥à¹‰à¸§à¸”à¸¹à¸§à¹ˆà¸² Duration à¹€à¸à¸´à¹ˆà¸¡à¸‚à¸¶à¹‰à¸™à¹€à¸—à¹ˆà¸²à¹„à¸«à¸£à¹ˆ

### â“ à¸„à¸³à¸–à¸²à¸¡à¸—à¹‰à¸²à¸¢ LAB Week 1

1. à¸—à¸³à¹„à¸¡ S3 Event Trigger à¸–à¸¶à¸‡à¹„à¸¡à¹ˆà¸£à¸±à¸šà¸›à¸£à¸°à¸à¸±à¸™à¸§à¹ˆà¸²à¸ˆà¸° Trigger **à¸„à¸£à¸±à¹‰à¸‡à¹€à¸”à¸µà¸¢à¸§** à¹€à¸ªà¸¡à¸­? à¹à¸¥à¹‰à¸§à¸ˆà¸°à¸­à¸­à¸à¹à¸šà¸š Lambda à¹ƒà¸«à¹‰à¸£à¸­à¸‡à¸£à¸±à¸š Duplicate Events à¹„à¸”à¹‰à¸¢à¸±à¸‡à¹„à¸‡? (Idempotency)
2. Lambda à¸¡à¸µ Concurrency Limit à¹€à¸—à¹ˆà¸²à¹„à¸«à¸£à¹ˆà¹ƒà¸™ Default? à¸–à¹‰à¸² Upload à¹„à¸Ÿà¸¥à¹Œà¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™ 1,000 à¹„à¸Ÿà¸¥à¹Œà¸ˆà¸°à¹€à¸à¸´à¸”à¸­à¸°à¹„à¸£à¸‚à¸¶à¹‰à¸™?
3. Prefix `uploads/` à¹ƒà¸™ S3 Trigger à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ Recursive Loop à¹„à¸”à¹‰à¸ˆà¸£à¸´à¸‡à¸«à¸£à¸·à¸­à¹€à¸›à¸¥à¹ˆà¸²? à¸¡à¸µà¸à¸£à¸“à¸µà¸—à¸µà¹ˆà¸¢à¸±à¸‡à¹€à¸à¸´à¸” Loop à¹„à¸”à¹‰à¹„à¸«à¸¡?

---

---

# ğŸ“… WEEK 2 â€” Message Queue with SQS

> **à¸«à¸±à¸§à¸‚à¹‰à¸­:** à¹€à¸à¸´à¹ˆà¸¡ SQS à¹€à¸à¸·à¹ˆà¸­ Decouple à¸£à¸°à¸šà¸šà¹à¸¥à¸°à¸£à¸­à¸‡à¸£à¸±à¸š High Volume

---

## ğŸ“ à¸§à¸±à¸•à¸–à¸¸à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰
- à¸­à¸˜à¸´à¸šà¸²à¸¢ Decoupling à¹à¸¥à¸°à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œà¸‚à¸­à¸‡ Message Queue à¹„à¸”à¹‰
- à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸„à¸§à¸²à¸¡à¹à¸•à¸à¸•à¹ˆà¸²à¸‡à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ Standard Queue à¸à¸±à¸š FIFO Queue
- à¹€à¸Šà¸·à¹ˆà¸­à¸¡ Lambda (Producer) â†’ SQS â†’ Lambda (Consumer)
- à¸ˆà¸±à¸”à¸à¸²à¸£ Dead Letter Queue (DLQ) à¹€à¸à¸·à¹ˆà¸­à¸ˆà¸±à¸”à¸à¸²à¸£ Message à¸—à¸µà¹ˆ Process à¹„à¸¡à¹ˆà¸ªà¸³à¹€à¸£à¹‡à¸ˆ
- à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆ SQS Visibility Timeout à¹à¸¥à¸° Retry Mechanism

---

## ğŸ“– à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸à¸·à¹‰à¸™à¸à¸²à¸™ (Lecture â€” 30 à¸™à¸²à¸—à¸µ)

### à¸—à¸³à¹„à¸¡à¸•à¹‰à¸­à¸‡à¸¡à¸µ Queue?

à¸¥à¸­à¸‡à¸™à¸¶à¸à¸–à¸¶à¸‡à¸ªà¸–à¸²à¸™à¸à¸²à¸£à¸“à¹Œà¸™à¸µà¹‰: Upload à¹„à¸Ÿà¸¥à¹Œ 500 à¹„à¸Ÿà¸¥à¹Œà¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™ Lambda Validator à¸–à¸¹à¸ Trigger 500 à¸„à¸£à¸±à¹‰à¸‡à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™ à¸—à¸¸à¸à¸•à¸±à¸§à¸à¸¢à¸²à¸¢à¸²à¸¡à¹€à¸£à¸µà¸¢à¸ Service à¸–à¸±à¸”à¹„à¸›à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™ â†’ Service à¸–à¸±à¸”à¹„à¸›à¸£à¸±à¸šà¹„à¸¡à¹ˆà¹„à¸«à¸§ â†’ à¸£à¸°à¸šà¸šà¸¥à¹ˆà¸¡

**SQS à¹à¸à¹‰à¸›à¸±à¸à¸«à¸²à¸™à¸µà¹‰à¸”à¹‰à¸§à¸¢:**
```
500 files uploaded â†’ [SQS Queue] â†’ Consumer Lambda à¸­à¹ˆà¸²à¸™à¸—à¸µà¸¥à¸° 10 messages
                                    â†’ à¸„à¹ˆà¸­à¸¢à¹† à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸•à¸²à¸¡à¸„à¸§à¸²à¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–
                                    â†’ à¹„à¸¡à¹ˆ overflow à¸£à¸°à¸šà¸šà¸–à¸±à¸”à¹„à¸›
```

### Standard Queue vs FIFO Queue

| | Standard Queue | FIFO Queue |
|---|---|---|
| Ordering | Best-effort (à¹„à¸¡à¹ˆà¸£à¸±à¸šà¸›à¸£à¸°à¸à¸±à¸™) | à¸£à¸±à¸šà¸›à¸£à¸°à¸à¸±à¸™ First-In-First-Out |
| Throughput | Unlimited | 300â€“3,000 msg/sec |
| Delivery | At least once (à¸­à¸²à¸ˆ duplicate) | Exactly once |
| à¸£à¸²à¸„à¸² | à¸–à¸¹à¸à¸à¸§à¹ˆà¸² | à¹à¸à¸‡à¸à¸§à¹ˆà¸²à¹€à¸¥à¹‡à¸à¸™à¹‰à¸­à¸¢ |
| à¹ƒà¸Šà¹‰à¹€à¸¡à¸·à¹ˆà¸­ | à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸¥à¸³à¸”à¸±à¸šà¹„à¸¡à¹ˆà¸ªà¸³à¸„à¸±à¸ | à¸•à¹‰à¸­à¸‡à¸£à¸±à¸à¸©à¸²à¸¥à¸³à¸”à¸±à¸š |

> à¸ªà¸³à¸«à¸£à¸±à¸š Project à¸™à¸µà¹‰à¹ƒà¸Šà¹‰ **Standard Queue** à¹€à¸à¸£à¸²à¸°à¹€à¸£à¸²à¸ˆà¸°à¸—à¸³ Idempotent Processing

### Dead Letter Queue (DLQ) à¸„à¸·à¸­à¸­à¸°à¹„à¸£?

à¹€à¸¡à¸·à¹ˆà¸­ Consumer Lambda à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ Message à¸¥à¹‰à¸¡à¹€à¸«à¸¥à¸§à¸‹à¹‰à¸³à¹† (à¹€à¸à¸´à¸™ maxReceiveCount) SQS à¸ˆà¸°à¸¢à¹‰à¸²à¸¢ Message à¸™à¸±à¹‰à¸™à¹„à¸›à¸¢à¸±à¸‡ DLQ à¹à¸—à¸™à¸—à¸µà¹ˆà¸ˆà¸°à¸—à¸´à¹‰à¸‡ à¸—à¸³à¹ƒà¸«à¹‰à¹€à¸£à¸²à¸ªà¸²à¸¡à¸²à¸£à¸–:
- Debug à¸§à¹ˆà¸² Message à¹„à¸«à¸™à¸¡à¸µà¸›à¸±à¸à¸«à¸²
- à¸ªà¹ˆà¸‡à¹„à¸›à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹ƒà¸«à¸¡à¹ˆà¸—à¸µà¸«à¸¥à¸±à¸‡
- à¹à¸ˆà¹‰à¸‡à¹€à¸•à¸·à¸­à¸™ Operator à¹€à¸¡à¸·à¹ˆà¸­à¸¡à¸µ Message à¸„à¹‰à¸²à¸‡à¹ƒà¸™ DLQ

### Architecture à¸—à¸µà¹ˆà¸ˆà¸°à¸ªà¸£à¹‰à¸²à¸‡à¹ƒà¸™ Week 2

```
[S3 Event] â†’ [Lambda: csv-validator] â†’ [SQS: main-queue]
                                               â”‚
                                   SQS Trigger (Batch)
                                               â”‚
                                               â–¼
                                    [Lambda: csv-processor]
                                       - Parse CSV
                                       - à¸„à¸³à¸™à¸§à¸“ Summary
                                       - Save à¹„à¸›à¸¢à¸±à¸‡ S3 processed/
                                               â”‚
                                     (à¸–à¹‰à¸² Error > 3 à¸„à¸£à¸±à¹‰à¸‡)
                                               â–¼
                                    [SQS: dead-letter-queue]
```

---

## ğŸ”§ LAB Step-by-Step

---

### âœ… STEP 1 â€” à¸ªà¸£à¹‰à¸²à¸‡ Dead Letter Queue à¸à¹ˆà¸­à¸™

à¹€à¸ªà¸¡à¸­à¸ªà¸£à¹‰à¸²à¸‡ DLQ à¸à¹ˆà¸­à¸™ Main Queue:

1. à¹€à¸‚à¹‰à¸² **SQS** â†’ **Create queue**
2. à¸à¸£à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:
   ```
   Type: Standard
   Name: pipeline-dlq
   
   Configuration:
   Visibility timeout: 30 seconds
   Message retention period: 14 days   â† à¹€à¸à¹‡à¸šà¸™à¸²à¸™ à¹€à¸œà¸·à¹ˆà¸­ Debug
   Maximum message size: 256 KB
   Delivery delay: 0
   ```
3. à¸à¸” **Create queue**
4. à¸ˆà¸” **Queue ARN** à¹„à¸§à¹‰à¹ƒà¸Šà¹‰à¸•à¹ˆà¸­ (à¸£à¸¹à¸›à¹à¸šà¸š: `arn:aws:sqs:ap-southeast-1:123456789:pipeline-dlq`)

---

### âœ… STEP 2 â€” à¸ªà¸£à¹‰à¸²à¸‡ Main Queue

1. à¸à¸” **Create queue** à¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡
2. à¸à¸£à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:
   ```
   Type: Standard
   Name: pipeline-main-queue
   
   Configuration:
   Visibility timeout: 60 seconds      â† à¸•à¹‰à¸­à¸‡à¸¡à¸²à¸à¸à¸§à¹ˆà¸² Lambda timeout
   Message retention period: 4 days
   Maximum message size: 256 KB
   Delivery delay: 0
   Receive message wait time: 20 seconds  â† Long Polling à¸›à¸£à¸°à¸«à¸¢à¸±à¸” Cost
   ```
3. **Dead-letter queue section:**
   - Enabled: âœ…
   - Dead-letter queue: à¹€à¸¥à¸·à¸­à¸ `pipeline-dlq`
   - Maximum receives: `3` (à¸¥à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆ 3 à¸„à¸£à¸±à¹‰à¸‡à¹à¸¥à¹‰à¸§à¸„à¹ˆà¸­à¸¢à¸ªà¹ˆà¸‡ DLQ)
4. à¸à¸” **Create queue**

> ğŸ’¡ **Long Polling à¸„à¸·à¸­à¸­à¸°à¹„à¸£?** à¹à¸—à¸™à¸—à¸µà¹ˆ SQS à¸ˆà¸°à¸•à¸­à¸šà¸à¸¥à¸±à¸šà¸—à¸±à¸™à¸—à¸µà¸§à¹ˆà¸² "à¹„à¸¡à¹ˆà¸¡à¸µ Message" (Short Polling â†’ à¹€à¸ªà¸µà¸¢à¹€à¸‡à¸´à¸™à¹à¸•à¹ˆà¹€à¸›à¸¥à¹ˆà¸²) Long Polling à¸ˆà¸°à¸£à¸­à¸™à¸²à¸™à¸ªà¸¹à¸‡à¸ªà¸¸à¸” 20 à¸§à¸´à¸™à¸²à¸—à¸µà¸à¹ˆà¸­à¸™à¸•à¸­à¸š â€” à¸¥à¸”à¸„à¹ˆà¸²à¹ƒà¸Šà¹‰à¸ˆà¹ˆà¸²à¸¢à¹„à¸”à¹‰à¸¡à¸²à¸

---

### âœ… STEP 3 â€” à¸­à¸±à¸›à¹€à¸”à¸• IAM Policy

Lambda à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸ªà¸´à¸—à¸˜à¸´à¹Œà¸ªà¹ˆà¸‡/à¸£à¸±à¸š Message à¸ˆà¸²à¸ SQS:

1. à¹€à¸‚à¹‰à¸² **IAM** â†’ **Policies** â†’ à¸„à¹‰à¸™à¸«à¸² `pipeline-lambda-policy` â†’ **Edit**
2. à¹€à¸à¸´à¹ˆà¸¡ Statement à¹ƒà¸«à¸¡à¹ˆà¹€à¸‚à¹‰à¸²à¹„à¸›à¹ƒà¸™ JSON:

```json
{
  "Sid": "SQSPermissions",
  "Effect": "Allow",
  "Action": [
    "sqs:SendMessage",
    "sqs:ReceiveMessage",
    "sqs:DeleteMessage",
    "sqs:GetQueueAttributes",
    "sqs:ChangeMessageVisibility"
  ],
  "Resource": [
    "arn:aws:sqs:ap-southeast-1:[ACCOUNT-ID]:pipeline-main-queue",
    "arn:aws:sqs:ap-southeast-1:[ACCOUNT-ID]:pipeline-dlq"
  ]
}
```

3. à¸à¸” **Save changes**

---

### âœ… STEP 4 â€” à¸­à¸±à¸›à¹€à¸”à¸• Lambda Validator à¹ƒà¸«à¹‰à¸ªà¹ˆà¸‡ Message à¹„à¸› SQS

à¹à¸à¹‰à¹„à¸‚ `csv-validator` Lambda:

```python
import json
import boto3
import csv
import io
import logging
from datetime import datetime

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client('s3')
sqs = boto3.client('sqs', region_name='ap-southeast-1')

# à¹ƒà¸ªà¹ˆ Queue URL à¸‚à¸­à¸‡à¸„à¸¸à¸“
QUEUE_URL = 'https://sqs.ap-southeast-1.amazonaws.com/[ACCOUNT-ID]/pipeline-main-queue'
PROCESSED_BUCKET = 'pipeline-processed-[à¸Šà¸·à¹ˆà¸­à¸„à¸¸à¸“]-[random]'

def lambda_handler(event, context):
    logger.info(f"Received {len(event['Records'])} S3 event(s)")
    
    results = []
    
    for record in event['Records']:
        bucket_name = record['s3']['bucket']['name']
        object_key = record['s3']['object']['key']
        file_size = record['s3']['object']['size']
        
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸™à¸²à¸¡à¸ªà¸à¸¸à¸¥
        if not object_key.lower().endswith('.csv'):
            logger.warning(f"Skipping non-CSV file: {object_key}")
            continue
        
        try:
            # à¸­à¹ˆà¸²à¸™à¹à¸¥à¸°à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹„à¸Ÿà¸¥à¹Œ
            response = s3.get_object(Bucket=bucket_name, Key=object_key)
            content = response['Body'].read().decode('utf-8')
            
            csv_reader = csv.DictReader(io.StringIO(content))
            rows = list(csv_reader)
            headers = list(csv_reader.fieldnames) if csv_reader.fieldnames else []
            
            if len(rows) == 0:
                logger.warning(f"Empty file: {object_key}")
                continue
            
            # à¸ªà¸£à¹‰à¸²à¸‡ Message à¸ªà¸³à¸«à¸£à¸±à¸š SQS
            message = {
                "job_id": f"job-{datetime.now().strftime('%Y%m%d%H%M%S')}-{object_key.split('/')[-1]}",
                "source_bucket": bucket_name,
                "source_key": object_key,
                "destination_bucket": PROCESSED_BUCKET,
                "file_size_bytes": file_size,
                "row_count": len(rows),
                "headers": headers,
                "submitted_at": datetime.now().isoformat()
            }
            
            # à¸ªà¹ˆà¸‡ Message à¹„à¸› SQS
            sqs_response = sqs.send_message(
                QueueUrl=QUEUE_URL,
                MessageBody=json.dumps(message),
                MessageAttributes={
                    'job_type': {
                        'DataType': 'String',
                        'StringValue': 'csv_processing'
                    }
                }
            )
            
            logger.info(json.dumps({
                "event": "message_queued",
                "job_id": message["job_id"],
                "message_id": sqs_response['MessageId'],
                "file": object_key,
                "rows": len(rows)
            }))
            
            results.append({
                "file": object_key,
                "status": "queued",
                "job_id": message["job_id"],
                "message_id": sqs_response['MessageId']
            })
            
        except Exception as e:
            logger.error(json.dumps({
                "event": "error",
                "file": object_key,
                "error": str(e),
                "error_type": type(e).__name__
            }))
            results.append({"file": object_key, "status": "error", "reason": str(e)})
    
    return {"statusCode": 200, "body": json.dumps({"results": results})}
```

à¸à¸” **Deploy**

---

### âœ… STEP 5 â€” à¸ªà¸£à¹‰à¸²à¸‡ Lambda Consumer (csv-processor)

1. **Lambda** â†’ **Create function**
2. à¸à¸£à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:
   ```
   Function name: csv-processor
   Runtime: Python 3.12
   Execution role: pipeline-lambda-role
   ```
3. à¸§à¸²à¸‡à¹‚à¸„à¹‰à¸”:

```python
import json
import boto3
import csv
import io
import logging
from datetime import datetime
from collections import defaultdict

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client('s3')

def lambda_handler(event, context):
    """
    Consumer Lambda â€” à¸­à¹ˆà¸²à¸™ Message à¸ˆà¸²à¸ SQS à¹à¸¥à¸°à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ CSV
    """
    logger.info(f"Processing {len(event['Records'])} SQS message(s)")
    
    processed = []
    failed = []
    
    for record in event['Records']:
        message_id = record['messageId']
        receipt_handle = record['receiptHandle']
        
        try:
            body = json.loads(record['body'])
            job_id = body['job_id']
            source_bucket = body['source_bucket']
            source_key = body['source_key']
            dest_bucket = body['destination_bucket']
            
            logger.info(json.dumps({
                "event": "processing_started",
                "job_id": job_id,
                "message_id": message_id
            }))
            
            # à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œà¸ˆà¸²à¸ S3
            response = s3.get_object(Bucket=source_bucket, Key=source_key)
            content = response['Body'].read().decode('utf-8')
            
            # Parse CSV
            csv_reader = csv.DictReader(io.StringIO(content))
            rows = list(csv_reader)
            
            # à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥: à¸„à¸³à¸™à¸§à¸“ Summary Statistics
            summary = calculate_summary(rows, job_id, source_key)
            
            # à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¹„à¸›à¸¢à¸±à¸‡ S3
            filename = source_key.split('/')[-1].replace('.csv', '')
            output_key = f"processed/{datetime.now().strftime('%Y/%m/%d')}/{filename}_summary.json"
            
            s3.put_object(
                Bucket=dest_bucket,
                Key=output_key,
                Body=json.dumps(summary, ensure_ascii=False, indent=2),
                ContentType='application/json'
            )
            
            logger.info(json.dumps({
                "event": "processing_complete",
                "job_id": job_id,
                "output_key": output_key,
                "total_rows": summary['total_rows'],
                "total_revenue": summary.get('total_revenue', 0)
            }))
            
            processed.append(job_id)
            
        except Exception as e:
            logger.error(json.dumps({
                "event": "processing_failed",
                "message_id": message_id,
                "error": str(e),
                "error_type": type(e).__name__
            }))
            # à¸–à¹‰à¸² raise Exception â†’ SQS à¸ˆà¸° retry à¹à¸¥à¸°à¸ªà¹ˆà¸‡ DLQ à¸«à¸¥à¸±à¸‡ 3 à¸„à¸£à¸±à¹‰à¸‡
            failed.append(message_id)
            raise  # Re-raise à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰ SQS à¸£à¸¹à¹‰à¸§à¹ˆà¸² Message à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸–à¸¹à¸ Process à¸ªà¸³à¹€à¸£à¹‡à¸ˆ
    
    logger.info(f"Batch complete: {len(processed)} processed, {len(failed)} failed")
    
    return {
        "batchItemFailures": [
            {"itemIdentifier": mid} for mid in failed
        ]
    }


def calculate_summary(rows, job_id, source_key):
    """à¸„à¸³à¸™à¸§à¸“ Summary Statistics à¸ˆà¸²à¸ CSV rows"""
    
    if not rows:
        return {"job_id": job_id, "source": source_key, "total_rows": 0}
    
    summary = {
        "job_id": job_id,
        "source_file": source_key,
        "processed_at": datetime.now().isoformat(),
        "total_rows": len(rows),
        "columns": list(rows[0].keys())
    }
    
    # à¸–à¹‰à¸²à¸¡à¸µ Column à¸•à¸±à¸§à¹€à¸¥à¸‚ à¸¥à¸­à¸‡à¸„à¸³à¸™à¸§à¸“ Stats
    numeric_columns = {}
    for col in rows[0].keys():
        try:
            values = [float(row[col]) for row in rows if row[col]]
            if values:
                numeric_columns[col] = {
                    "min": min(values),
                    "max": max(values),
                    "sum": round(sum(values), 2),
                    "avg": round(sum(values) / len(values), 2),
                    "count": len(values)
                }
        except (ValueError, TypeError):
            pass  # à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆà¸•à¸±à¸§à¹€à¸¥à¸‚ à¸‚à¹‰à¸²à¸¡à¹„à¸›
    
    if numeric_columns:
        summary["numeric_stats"] = numeric_columns
    
    # à¸™à¸±à¸šà¸„à¹ˆà¸²à¸‹à¹‰à¸³à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸° Column (à¸ªà¸³à¸«à¸£à¸±à¸š Categorical)
    categorical_stats = {}
    for col in rows[0].keys():
        if col not in numeric_columns:
            counts = defaultdict(int)
            for row in rows:
                counts[row[col]] += 1
            categorical_stats[col] = dict(counts)
    
    if categorical_stats:
        summary["categorical_stats"] = categorical_stats
    
    return summary
```

4. à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²:
   - Timeout: **60 seconds**
   - Memory: **256 MB**
5. à¸à¸” **Deploy**

---

### âœ… STEP 6 â€” à¹€à¸Šà¸·à¹ˆà¸­à¸¡ SQS à¸à¸±à¸š Lambda Consumer

1. à¹ƒà¸™ Lambda `csv-processor` â†’ **Configuration** â†’ **Triggers** â†’ **Add trigger**
2. à¹€à¸¥à¸·à¸­à¸ **SQS**
3. à¸à¸£à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:
   ```
   SQS queue: pipeline-main-queue
   Batch size: 5               â† à¸­à¹ˆà¸²à¸™à¸„à¸£à¸±à¹‰à¸‡à¸¥à¸° 5 messages
   Batch window: 10 seconds    â† à¸£à¸­à¸ªà¸°à¸ªà¸¡ messages à¸ªà¸¹à¸‡à¸ªà¸¸à¸” 10 à¸§à¸´à¸™à¸²à¸—à¸µ
   Report batch item failures: âœ… Enable
   ```
4. à¸à¸” **Add**

> ğŸ’¡ **Batch Item Failures à¸„à¸·à¸­à¸­à¸°à¹„à¸£?** à¹à¸—à¸™à¸—à¸µà¹ˆà¸–à¹‰à¸² Message à¹ƒà¸”à¹ƒà¸™ Batch à¸¥à¹‰à¸¡à¹€à¸«à¸¥à¸§à¸ˆà¸° retry à¸—à¸±à¹‰à¸‡ Batch à¹ƒà¸«à¸¡à¹ˆà¸«à¸¡à¸” à¹€à¸£à¸²à¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸šà¹€à¸‰à¸à¸²à¸° Message ID à¸—à¸µà¹ˆà¸¥à¹‰à¸¡à¹€à¸«à¸¥à¸§ SQS à¸ˆà¸° retry à¹€à¸‰à¸à¸²à¸°à¸­à¸±à¸™ à¸™à¸±à¹‰à¸™ à¸›à¸£à¸°à¸«à¸¢à¸±à¸”à¸¡à¸²à¸

---

### âœ… STEP 7 â€” à¸—à¸”à¸ªà¸­à¸š End-to-End Pipeline

1. **à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”** `sample_sales.csv` à¹„à¸›à¸¢à¸±à¸‡ `uploads/` à¹ƒà¸™ S3 Bucket à¹à¸£à¸
2. **à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š** SQS Console â†’ `pipeline-main-queue` â†’ Messages available à¸ˆà¸°à¸‚à¸¶à¹‰à¸™à¸Šà¸±à¹ˆà¸§à¸„à¸£à¸²à¸§à¹à¸¥à¹‰à¸§à¸«à¸²à¸¢à¹„à¸›
3. **à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š** CloudWatch Logs:
   - `/aws/lambda/csv-validator` â†’ à¸„à¸§à¸£à¹€à¸«à¹‡à¸™ `"event": "message_queued"`
   - `/aws/lambda/csv-processor` â†’ à¸„à¸§à¸£à¹€à¸«à¹‡à¸™ `"event": "processing_complete"`
4. **à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š** S3 `pipeline-processed-...` â†’ à¹€à¸‚à¹‰à¸² folder `processed/` â†’ à¸„à¸§à¸£à¹€à¸«à¹‡à¸™à¹„à¸Ÿà¸¥à¹Œ `..._summary.json`
5. à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ JSON à¸¡à¸²à¸”à¸¹à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ

---

### âœ… STEP 8 â€” à¸—à¸”à¸ªà¸­à¸š Dead Letter Queue

à¸—à¸”à¸ªà¸­à¸šà¸§à¹ˆà¸² DLQ à¸—à¸³à¸‡à¸²à¸™à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡:

1. à¹à¸à¹‰ `csv-processor` à¹ƒà¸«à¹‰ Raise Error à¹€à¸ªà¸¡à¸­à¸Šà¸±à¹ˆà¸§à¸„à¸£à¸²à¸§:
```python
def lambda_handler(event, context):
    raise Exception("Simulated failure for DLQ testing")
```
2. Deploy à¹à¸¥à¸°à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ CSV à¹ƒà¸«à¸¡à¹ˆ
3. à¸£à¸­à¸›à¸£à¸°à¸¡à¸²à¸“ 2â€“3 à¸™à¸²à¸—à¸µ (SQS à¸ˆà¸° retry 3 à¸„à¸£à¸±à¹‰à¸‡à¸•à¸²à¸¡ maxReceiveCount)
4. à¹€à¸‚à¹‰à¸² SQS â†’ `pipeline-dlq` â†’ **Messages available** à¸„à¸§à¸£à¹€à¸à¸´à¹ˆà¸¡à¸‚à¸¶à¹‰à¸™
5. à¸à¸” **Send and receive messages** â†’ **Poll for messages** â†’ à¸”à¸¹à¹€à¸™à¸·à¹‰à¸­à¸«à¸² Message
6. à¹à¸à¹‰à¹‚à¸„à¹‰à¸” Lambda à¸à¸¥à¸±à¸šà¹€à¸›à¹‡à¸™à¸›à¸à¸•à¸´à¹à¸¥à¹‰à¸§ Deploy

---

### ğŸ“ à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡ Week 2

1. **à¹€à¸à¸´à¹ˆà¸¡ CloudWatch Alarm** à¹à¸ˆà¹‰à¸‡à¹€à¸•à¸·à¸­à¸™à¹€à¸¡à¸·à¹ˆà¸­ `pipeline-dlq` à¸¡à¸µ Message à¸¡à¸²à¸à¸à¸§à¹ˆà¸² 0 (à¹à¸ªà¸”à¸‡à¸§à¹ˆà¸²à¸¡à¸µ Processing Error)
2. **à¸—à¸”à¸ªà¸­à¸š High Volume:** à¹€à¸‚à¸µà¸¢à¸™ Script à¸­à¸±à¸›à¹‚à¸«à¸¥à¸” CSV 20 à¹„à¸Ÿà¸¥à¹Œà¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™ à¸”à¸¹à¸§à¹ˆà¸² SQS à¸ˆà¸±à¸”à¸à¸²à¸£ Queue à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£
3. **à¹€à¸à¸´à¹ˆà¸¡ Message Deduplication ID:** à¹à¸à¹‰à¹‚à¸„à¹‰à¸”à¹ƒà¸«à¹‰à¹„à¸¡à¹ˆà¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸Ÿà¸¥à¹Œà¹€à¸”à¸´à¸¡à¸‹à¹‰à¸³ (Idempotency Key)

### â“ à¸„à¸³à¸–à¸²à¸¡à¸—à¹‰à¸²à¸¢ LAB Week 2

1. à¸–à¹‰à¸²à¸•à¸±à¹‰à¸‡ Visibility Timeout à¸ªà¸±à¹‰à¸™à¸à¸§à¹ˆà¸² Lambda Timeout à¸ˆà¸°à¹€à¸à¸´à¸”à¸­à¸°à¹„à¸£à¸‚à¸¶à¹‰à¸™?
2. SQS Standard Queue à¸­à¸²à¸ˆ Deliver Message à¸¡à¸²à¸à¸à¸§à¹ˆà¸² 1 à¸„à¸£à¸±à¹‰à¸‡ â€” à¹€à¸£à¸²à¸­à¸­à¸à¹à¸šà¸š `csv-processor` à¹ƒà¸«à¹‰à¸£à¸­à¸‡à¸£à¸±à¸šà¸à¸£à¸“à¸µà¸™à¸µà¹‰à¹„à¸”à¹‰à¸¢à¸±à¸‡à¹„à¸‡? (Hint: à¸”à¸¹à¸—à¸µà¹ˆ output_key)
3. Long Polling à¸à¸±à¸š Short Polling à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™à¸¢à¸±à¸‡à¹„à¸‡à¹ƒà¸™à¹à¸‡à¹ˆ Cost à¹à¸¥à¸° Latency?

---

---

# ğŸ“… WEEK 3 â€” Orchestration with Step Functions

> **à¸«à¸±à¸§à¸‚à¹‰à¸­:** à¹à¸—à¸™à¸—à¸µà¹ˆ Chain Lambda à¸•à¸£à¸‡à¹† à¸”à¹‰à¸§à¸¢ State Machine à¸—à¸µà¹ˆà¸”à¸¹à¹à¸¥à¹„à¸”à¹‰à¸‡à¹ˆà¸²à¸¢

---

## ğŸ“ à¸§à¸±à¸•à¸–à¸¸à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰
- à¸­à¸˜à¸´à¸šà¸²à¸¢ Orchestration vs Choreography à¹ƒà¸™ Microservices à¹„à¸”à¹‰
- à¸ªà¸£à¹‰à¸²à¸‡ Step Functions State Machine à¸”à¹‰à¸§à¸¢ Amazon States Language (ASL)
- à¹€à¸Šà¸·à¹ˆà¸­à¸¡ Lambda à¸«à¸¥à¸²à¸¢à¸•à¸±à¸§à¹€à¸‚à¹‰à¸² Workflow à¸—à¸µà¹ˆà¸¡à¸µ Error Handling
- à¹ƒà¸Šà¹‰ Parallel State à¹à¸¥à¸° Choice State
- Monitor Execution History à¹ƒà¸™ Step Functions Console

---

## ğŸ“– à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸à¸·à¹‰à¸™à¸à¸²à¸™ (Lecture â€” 30 à¸™à¸²à¸—à¸µ)

### Orchestration vs Choreography

**Choreography (à¹à¸šà¸šà¸—à¸µà¹ˆà¸—à¸³à¹ƒà¸™ Week 1â€“2):**
```
Lambda A â”€â”€â”€â”€à¸ªà¹ˆà¸‡ Eventâ”€â”€â”€â”€> SQS â”€â”€â”€â”€triggerâ”€â”€â”€â”€> Lambda B
Lambda B â”€â”€â”€â”€à¸ªà¹ˆà¸‡ Eventâ”€â”€â”€â”€> SNS â”€â”€â”€â”€triggerâ”€â”€â”€â”€> Lambda C
```
- à¹à¸•à¹ˆà¸¥à¸° Service à¸£à¸¹à¹‰à¸ˆà¸±à¸à¸à¸±à¸™à¸™à¹‰à¸­à¸¢à¸—à¸µà¹ˆà¸ªà¸¸à¸”
- à¸¢à¸²à¸à¸—à¸µà¹ˆà¸ˆà¸° Debug à¹€à¸¡à¸·à¹ˆà¸­à¸¡à¸µà¸›à¸±à¸à¸«à¸² ("à¹„à¸Ÿà¸¥à¹ˆà¸­à¸‡à¸«à¸™à¹„à¸›à¹„à¸«à¸™?")
- à¹„à¸¡à¹ˆà¸¡à¸µà¸—à¸µà¹ˆà¹€à¸”à¸µà¸¢à¸§à¸—à¸µà¹ˆà¹€à¸«à¹‡à¸™ Flow à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”

**Orchestration (Step Functions):**
```
Step Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Lambda A
              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Lambda B (à¸–à¹‰à¸² A à¸ªà¸³à¹€à¸£à¹‡à¸ˆ)
              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Lambda C (à¸–à¹‰à¸² B à¸ªà¸³à¹€à¸£à¹‡à¸ˆ)
              â”€â”€Errorâ”€â”€â”€> Notify (à¸–à¹‰à¸²à¸¡à¸µ Error)
```
- à¸¡à¸µà¸—à¸µà¹ˆà¹€à¸”à¸µà¸¢à¸§à¸—à¸µà¹ˆà¸„à¸§à¸šà¸„à¸¸à¸¡ Flow à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
- à¹€à¸«à¹‡à¸™ Visual Diagram à¹à¸¥à¸° Execution History
- Error Handling à¹à¸¥à¸° Retry à¹ƒà¸™à¸—à¸µà¹ˆà¹€à¸”à¸µà¸¢à¸§
- Timeout à¹à¸•à¹ˆà¸¥à¸° Step à¹„à¸”à¹‰

### Amazon States Language (ASL)

ASL à¹€à¸›à¹‡à¸™ JSON-based Language à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸³à¸«à¸™à¸” State Machine:

```json
{
  "Comment": "à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡ State Machine",
  "StartAt": "Step1",
  "States": {
    "Step1": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:...:function:my-lambda",
      "Next": "Step2",
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError"
        }
      ]
    },
    "Step2": {
      "Type": "Task",
      "Resource": "...",
      "End": true
    },
    "HandleError": {
      "Type": "Fail",
      "Error": "ProcessingFailed"
    }
  }
}
```

### State Types à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸

| State Type | à¹ƒà¸Šà¹‰à¸—à¸³à¸­à¸°à¹„à¸£ | à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡ |
|-----------|-----------|---------|
| Task | à¹€à¸£à¸µà¸¢à¸ Lambda / Service | à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸Ÿà¸¥à¹Œ |
| Choice | à¹à¸¢à¸à¹€à¸ªà¹‰à¸™à¸—à¸²à¸‡à¸•à¸²à¸¡à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚ | à¸–à¹‰à¸² rows > 1000 à¸—à¸³ A à¹„à¸¡à¹ˆà¸‡à¸±à¹‰à¸™à¸—à¸³ B |
| Parallel | à¸—à¸³à¸«à¸¥à¸²à¸¢ Tasks à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™ | Transform + Validate à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™ |
| Wait | à¸£à¸­à¹€à¸§à¸¥à¸² | à¸£à¸­ 5 à¸™à¸²à¸—à¸µà¸à¹ˆà¸­à¸™ Retry |
| Succeed | à¸ˆà¸šà¸ªà¸³à¹€à¸£à¹‡à¸ˆ | End State |
| Fail | à¸ˆà¸šà¹à¸šà¸š Error | Error State |
| Pass | à¸ªà¹ˆà¸‡à¸•à¹ˆà¸­à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸—à¸³à¸­à¸°à¹„à¸£ | Debug / Testing |

### Architecture à¸—à¸µà¹ˆà¸ˆà¸°à¸ªà¸£à¹‰à¸²à¸‡à¹ƒà¸™ Week 3

```
[SQS Consumer Lambda] â”€startsâ”€> [Step Functions: ProcessCSVWorkflow]
                                          â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â–¼             â–¼             â–¼
                      [Parse CSV]  [Validate Schema] [Check Size]
                      (Parallel States â€” à¸—à¸³à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™)
                            â”‚
                            â–¼
                      [Choice: Valid?]
                         /         \
                    Yes              No
                     â”‚               â”‚
              [Transform Data]  [Move to failed/]
                     â”‚
                     â–¼
               [Save Results]
                     â”‚
                     â–¼
                [Notify SNS]   â† à¸ˆà¸°à¸—à¸³à¹ƒà¸™ Week 4
                     â”‚
                     â–¼
                  [Success]
```

---

## ğŸ”§ LAB Step-by-Step

---

### âœ… STEP 1 â€” à¸ªà¸£à¹‰à¸²à¸‡ Lambda Functions à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸•à¹ˆà¸¥à¸° Step

à¹€à¸£à¸²à¸ˆà¸°à¸ªà¸£à¹‰à¸²à¸‡ Lambda 4 à¸•à¸±à¸§à¹à¸¢à¸à¸à¸±à¸™ à¹à¸•à¹ˆà¸¥à¸°à¸•à¸±à¸§à¸—à¸³à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆà¹€à¸”à¸µà¸¢à¸§ (Single Responsibility):

**Lambda 1: parse-csv**

1. **Lambda** â†’ **Create function** â†’ `parse-csv` â†’ Python 3.12 â†’ `pipeline-lambda-role`

```python
import json
import boto3
import csv
import io
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)
s3 = boto3.client('s3')

def lambda_handler(event, context):
    """
    Step 1: à¸­à¹ˆà¸²à¸™à¹à¸¥à¸° Parse à¹„à¸Ÿà¸¥à¹Œ CSV à¸ˆà¸²à¸ S3
    Input: {"source_bucket": "...", "source_key": "...", "job_id": "..."}
    Output: à¹€à¸à¸´à¹ˆà¸¡ rows à¹à¸¥à¸° headers à¹€à¸‚à¹‰à¸²à¹„à¸›à¹ƒà¸™ event
    """
    source_bucket = event['source_bucket']
    source_key = event['source_key']
    job_id = event['job_id']
    
    logger.info(f"Parsing {source_key} for job {job_id}")
    
    response = s3.get_object(Bucket=source_bucket, Key=source_key)
    content = response['Body'].read().decode('utf-8')
    
    csv_reader = csv.DictReader(io.StringIO(content))
    rows = list(csv_reader)
    headers = list(csv_reader.fieldnames) if csv_reader.fieldnames else []
    
    logger.info(f"Parsed {len(rows)} rows, {len(headers)} columns")
    
    # à¸ªà¹ˆà¸‡à¸•à¹ˆà¸­à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸›à¸¢à¸±à¸‡ Step à¸–à¸±à¸”à¹„à¸›
    return {
        **event,  # à¸ªà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸”à¸´à¸¡à¸•à¹ˆà¸­à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
        "rows": rows,
        "headers": headers,
        "row_count": len(rows),
        "parse_status": "success"
    }
```

**Lambda 2: validate-schema**

1. à¸ªà¸£à¹‰à¸²à¸‡ Lambda `validate-schema` â†’ Python 3.12 â†’ `pipeline-lambda-role`

```python
import json
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

# à¸à¸³à¸«à¸™à¸” Schema à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£
REQUIRED_COLUMNS = ['order_id', 'customer_name', 'product', 'quantity', 'price', 'date']

def lambda_handler(event, context):
    """
    Step 2: à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸² CSV à¸¡à¸µ Column à¸„à¸£à¸šà¸–à¹‰à¸§à¸™à¸•à¸²à¸¡ Schema
    """
    headers = event.get('headers', [])
    job_id = event['job_id']
    
    missing_columns = [col for col in REQUIRED_COLUMNS if col not in headers]
    extra_columns = [col for col in headers if col not in REQUIRED_COLUMNS]
    
    is_valid = len(missing_columns) == 0
    
    logger.info(json.dumps({
        "job_id": job_id,
        "is_valid": is_valid,
        "missing_columns": missing_columns,
        "extra_columns": extra_columns,
        "headers_found": headers
    }))
    
    return {
        **event,
        "schema_valid": is_valid,
        "missing_columns": missing_columns,
        "extra_columns": extra_columns,
        "validation_status": "passed" if is_valid else "failed"
    }
```

**Lambda 3: transform-data**

1. à¸ªà¸£à¹‰à¸²à¸‡ Lambda `transform-data` â†’ Python 3.12 â†’ `pipeline-lambda-role`

```python
import json
import boto3
import logging
from datetime import datetime
from collections import defaultdict

logger = logging.getLogger()
logger.setLevel(logging.INFO)
s3 = boto3.client('s3')

def lambda_handler(event, context):
    """
    Step 3: à¹à¸›à¸¥à¸‡à¹à¸¥à¸°à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    """
    rows = event['rows']
    job_id = event['job_id']
    dest_bucket = event['destination_bucket']
    source_key = event['source_key']
    
    logger.info(f"Transforming {len(rows)} rows for job {job_id}")
    
    # à¸„à¸³à¸™à¸§à¸“ Revenue
    total_revenue = 0
    product_summary = defaultdict(lambda: {"quantity": 0, "revenue": 0, "orders": 0})
    
    transformed_rows = []
    for row in rows:
        try:
            qty = float(row.get('quantity', 0))
            price = float(row.get('price', 0))
            revenue = qty * price
            total_revenue += revenue
            
            product = row.get('product', 'Unknown')
            product_summary[product]['quantity'] += qty
            product_summary[product]['revenue'] += revenue
            product_summary[product]['orders'] += 1
            
            transformed_rows.append({
                **row,
                'revenue': round(revenue, 2),
                'processed_at': datetime.now().isoformat()
            })
        except (ValueError, TypeError) as e:
            logger.warning(f"Error processing row {row}: {e}")
    
    result = {
        "job_id": job_id,
        "source_file": source_key,
        "processed_at": datetime.now().isoformat(),
        "summary": {
            "total_rows": len(rows),
            "total_revenue": round(total_revenue, 2),
            "average_order_value": round(total_revenue / len(rows), 2) if rows else 0,
            "product_breakdown": {k: {**v, "revenue": round(v["revenue"], 2)} 
                                  for k, v in product_summary.items()}
        },
        "data": transformed_rows
    }
    
    # à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸›à¸¢à¸±à¸‡ S3
    filename = source_key.split('/')[-1].replace('.csv', '')
    output_key = f"processed/{datetime.now().strftime('%Y/%m/%d')}/{filename}_result.json"
    
    s3.put_object(
        Bucket=dest_bucket,
        Key=output_key,
        Body=json.dumps(result, ensure_ascii=False, indent=2),
        ContentType='application/json'
    )
    
    logger.info(json.dumps({
        "event": "transform_complete",
        "job_id": job_id,
        "total_revenue": result['summary']['total_revenue'],
        "output_key": output_key
    }))
    
    return {
        **event,
        "transform_status": "success",
        "output_key": output_key,
        "total_revenue": result['summary']['total_revenue'],
        "summary": result['summary']
    }
```

**Lambda 4: move-to-failed**

1. à¸ªà¸£à¹‰à¸²à¸‡ Lambda `move-to-failed` â†’ Python 3.12 â†’ `pipeline-lambda-role`

```python
import json
import boto3
import logging
from datetime import datetime

logger = logging.getLogger()
logger.setLevel(logging.INFO)
s3 = boto3.client('s3')

def lambda_handler(event, context):
    """
    à¹€à¸¡à¸·à¹ˆà¸­ Schema Invalid â€” à¸¢à¹‰à¸²à¸¢à¹„à¸Ÿà¸¥à¹Œà¹„à¸›à¸¢à¸±à¸‡ failed/ folder à¸à¸£à¹‰à¸­à¸¡ error log
    """
    source_bucket = event['source_bucket']
    source_key = event['source_key']
    job_id = event['job_id']
    missing_columns = event.get('missing_columns', [])
    
    # Copy à¹„à¸›à¸¢à¸±à¸‡ failed/ folder
    filename = source_key.split('/')[-1]
    failed_key = f"failed/{datetime.now().strftime('%Y/%m/%d')}/{filename}"
    
    s3.copy_object(
        Bucket=source_bucket,
        CopySource={'Bucket': source_bucket, 'Key': source_key},
        Key=failed_key,
        Metadata={
            'job_id': job_id,
            'reason': f"Missing columns: {', '.join(missing_columns)}",
            'failed_at': datetime.now().isoformat()
        },
        MetadataDirective='REPLACE'
    )
    
    logger.warning(json.dumps({
        "event": "file_moved_to_failed",
        "job_id": job_id,
        "original_key": source_key,
        "failed_key": failed_key,
        "reason": f"Missing columns: {missing_columns}"
    }))
    
    return {
        **event,
        "failed_key": failed_key,
        "move_status": "success"
    }
```

Deploy à¸—à¸¸à¸ Lambda

---

### âœ… STEP 2 â€” à¸­à¸±à¸›à¹€à¸”à¸• IAM Policy

Lambda à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸ªà¸´à¸—à¸˜à¸´à¹Œà¹€à¸£à¸µà¸¢à¸ Step Functions à¹à¸¥à¸° Copy Object à¹ƒà¸™ S3:

à¹€à¸à¸´à¹ˆà¸¡ Statement à¹ƒà¸™ `pipeline-lambda-policy`:

```json
{
  "Sid": "S3CopyObject",
  "Effect": "Allow",
  "Action": ["s3:CopyObject"],
  "Resource": "arn:aws:s3:::pipeline-raw-[à¸Šà¸·à¹ˆà¸­à¸„à¸¸à¸“]-*/*"
},
{
  "Sid": "StepFunctionsStart",
  "Effect": "Allow",
  "Action": ["states:StartExecution"],
  "Resource": "*"
}
```

**à¸ªà¸£à¹‰à¸²à¸‡ IAM Role à¸ªà¸³à¸«à¸£à¸±à¸š Step Functions:**

1. **IAM** â†’ **Roles** â†’ **Create role**
2. Trusted entity: **AWS service** â†’ à¹€à¸¥à¸·à¹ˆà¸­à¸™à¸¥à¸‡à¸«à¸² **Step Functions** â†’ Next
3. à¸„à¹‰à¸™à¸«à¸² `AWSLambdaRole` (à¸­à¸™à¸¸à¸à¸²à¸•à¹ƒà¸«à¹‰ Step Functions à¹€à¸£à¸µà¸¢à¸ Lambda) â†’ à¹€à¸à¸´à¹ˆà¸¡
4. Role name: `pipeline-stepfunctions-role` â†’ Create

---

### âœ… STEP 3 â€” à¸ªà¸£à¹‰à¸²à¸‡ State Machine

1. à¹€à¸‚à¹‰à¸² **Step Functions** â†’ **State machines** â†’ **Create state machine**
2. à¹€à¸¥à¸·à¸­à¸ **Write your workflow in code**
3. Type: **Standard** (Express à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸š High Volume / Short Duration)
4. à¸§à¸²à¸‡ ASL JSON à¸™à¸µà¹‰:

```json
{
  "Comment": "CSV Processing Pipeline â€” Week 3 LAB",
  "StartAt": "ParseCSV",
  "States": {
    "ParseCSV": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:parse-csv",
      "TimeoutSeconds": 60,
      "Retry": [
        {
          "ErrorEquals": ["Lambda.ServiceException", "Lambda.AWSLambdaException", "Lambda.SdkClientException"],
          "IntervalSeconds": 2,
          "MaxAttempts": 3,
          "BackoffRate": 2
        }
      ],
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError",
          "ResultPath": "$.error"
        }
      ],
      "Next": "ValidateSchema"
    },
    
    "ValidateSchema": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:validate-schema",
      "TimeoutSeconds": 30,
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError",
          "ResultPath": "$.error"
        }
      ],
      "Next": "CheckValidation"
    },
    
    "CheckValidation": {
      "Type": "Choice",
      "Choices": [
        {
          "Variable": "$.schema_valid",
          "BooleanEquals": true,
          "Next": "TransformData"
        },
        {
          "Variable": "$.schema_valid",
          "BooleanEquals": false,
          "Next": "MoveToFailed"
        }
      ],
      "Default": "HandleError"
    },
    
    "TransformData": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:transform-data",
      "TimeoutSeconds": 120,
      "Retry": [
        {
          "ErrorEquals": ["States.ALL"],
          "IntervalSeconds": 5,
          "MaxAttempts": 2,
          "BackoffRate": 2
        }
      ],
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError",
          "ResultPath": "$.error"
        }
      ],
      "Next": "ProcessingComplete"
    },
    
    "MoveToFailed": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:move-to-failed",
      "TimeoutSeconds": 30,
      "Next": "ValidationFailed"
    },
    
    "ProcessingComplete": {
      "Type": "Succeed",
      "Comment": "Pipeline completed successfully"
    },
    
    "ValidationFailed": {
      "Type": "Fail",
      "Error": "ValidationFailed",
      "Cause": "CSV schema validation failed â€” file moved to failed folder"
    },
    
    "HandleError": {
      "Type": "Fail",
      "Error": "ProcessingError",
      "Cause": "An error occurred during pipeline processing"
    }
  }
}
```

5. **à¹à¸—à¸™à¸—à¸µà¹ˆ `[ACCOUNT-ID]`** à¸”à¹‰à¸§à¸¢ AWS Account ID à¸ˆà¸£à¸´à¸‡à¸‚à¸­à¸‡à¸„à¸¸à¸“ (à¸«à¸²à¹„à¸”à¹‰à¸ˆà¸²à¸ AWS Console à¸šà¸™à¸‚à¸§à¸²)
6. à¸à¸” **Next** â†’ State machine name: `ProcessCSVWorkflow`
7. Execution role: `pipeline-stepfunctions-role`
8. à¸à¸” **Create state machine**

---

### âœ… STEP 4 â€” à¸­à¸±à¸›à¹€à¸”à¸• SQS Consumer à¹ƒà¸«à¹‰à¹€à¸£à¸´à¹ˆà¸¡ Step Functions

à¹à¸à¹‰à¹„à¸‚ Lambda `csv-processor` (à¸—à¸µà¹ˆà¸ªà¸£à¹‰à¸²à¸‡à¹ƒà¸™ Week 2) à¹ƒà¸«à¹‰ Start Step Functions Execution à¹à¸—à¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹€à¸­à¸‡:

```python
import json
import boto3
import logging
from datetime import datetime

logger = logging.getLogger()
logger.setLevel(logging.INFO)

sfn = boto3.client('stepfunctions', region_name='ap-southeast-1')

# à¹ƒà¸ªà¹ˆ State Machine ARN à¸‚à¸­à¸‡à¸„à¸¸à¸“
STATE_MACHINE_ARN = 'arn:aws:states:ap-southeast-1:[ACCOUNT-ID]:stateMachine:ProcessCSVWorkflow'

def lambda_handler(event, context):
    """
    à¸­à¹ˆà¸²à¸™ Message à¸ˆà¸²à¸ SQS à¹à¸¥à¸°à¹€à¸£à¸´à¹ˆà¸¡ Step Functions Execution
    """
    logger.info(f"Processing {len(event['Records'])} SQS message(s)")
    
    batch_failures = []
    
    for record in event['Records']:
        message_id = record['messageId']
        
        try:
            body = json.loads(record['body'])
            job_id = body['job_id']
            
            # à¹€à¸•à¸£à¸µà¸¢à¸¡ Input à¸ªà¸³à¸«à¸£à¸±à¸š Step Functions
            sfn_input = {
                "job_id": job_id,
                "source_bucket": body['source_bucket'],
                "source_key": body['source_key'],
                "destination_bucket": body['destination_bucket'],
                "submitted_at": body['submitted_at'],
                "row_count_estimate": body.get('row_count', 0)
            }
            
            # Start Step Functions Execution
            response = sfn.start_execution(
                stateMachineArn=STATE_MACHINE_ARN,
                name=f"execution-{job_id}",  # à¸•à¹‰à¸­à¸‡ Unique
                input=json.dumps(sfn_input)
            )
            
            logger.info(json.dumps({
                "event": "execution_started",
                "job_id": job_id,
                "execution_arn": response['executionArn'],
                "message_id": message_id
            }))
            
        except Exception as e:
            logger.error(f"Failed to start execution for message {message_id}: {e}")
            batch_failures.append({"itemIdentifier": message_id})
    
    return {"batchItemFailures": batch_failures}
```

à¸à¸” **Deploy**

---

### âœ… STEP 5 â€” à¸—à¸”à¸ªà¸­à¸š State Machine

**à¸—à¸”à¸ªà¸­à¸šà¹‚à¸”à¸¢à¸•à¸£à¸‡à¸ˆà¸²à¸ Step Functions Console:**

1. à¹€à¸‚à¹‰à¸² **Step Functions** â†’ `ProcessCSVWorkflow` â†’ **Start execution**
2. Input:
```json
{
  "job_id": "test-job-001",
  "source_bucket": "pipeline-raw-[à¸Šà¸·à¹ˆà¸­à¸„à¸¸à¸“]-[random]",
  "source_key": "uploads/sample_sales.csv",
  "destination_bucket": "pipeline-processed-[à¸Šà¸·à¹ˆà¸­à¸„à¸¸à¸“]-[random]",
  "submitted_at": "2024-01-15T10:00:00"
}
```
3. à¸à¸” **Start execution**

**à¸”à¸¹ Visual Execution:**
- à¸ˆà¸°à¹€à¸«à¹‡à¸™ Diagram à¹à¸ªà¸”à¸‡ State à¸—à¸µà¹ˆà¸à¸³à¸¥à¸±à¸‡ Execute à¸­à¸¢à¸¹à¹ˆ (à¸ªà¸µà¸Ÿà¹‰à¸² = à¸à¸³à¸¥à¸±à¸‡à¸—à¸³à¸‡à¸²à¸™, à¸ªà¸µà¹€à¸‚à¸µà¸¢à¸§ = à¸ªà¸³à¹€à¸£à¹‡à¸ˆ, à¸ªà¸µà¹à¸”à¸‡ = à¸¥à¹‰à¸¡à¹€à¸«à¸¥à¸§)
- à¸à¸”à¹à¸•à¹ˆà¸¥à¸° State à¹€à¸à¸·à¹ˆà¸­à¸”à¸¹ Input/Output

**à¸—à¸”à¸ªà¸­à¸š Validation Failed Path:**
1. à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œ `bad_schema.csv` à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µ Column `order_id`:
```csv
name,product,qty,amount
Alice,Widget A,5,250
```
2. Upload à¸‚à¸¶à¹‰à¸™ S3 `uploads/`
3. Start execution à¸”à¹‰à¸§à¸¢ `source_key: "uploads/bad_schema.csv"`
4. à¸ªà¸±à¸‡à¹€à¸à¸• Flow à¸ˆà¸°à¹„à¸›à¹€à¸ªà¹‰à¸™ `MoveToFailed` â†’ `ValidationFailed`

---

### ğŸ“ à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡ Week 3

1. **à¹€à¸à¸´à¹ˆà¸¡ Parallel State:** à¸£à¸±à¸™ `validate-schema` à¹à¸¥à¸° **à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š File Size** à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™ (à¸ªà¸£à¹‰à¸²à¸‡ Lambda `check-file-size` à¸‡à¹ˆà¸²à¸¢à¹†)
2. **à¹€à¸à¸´à¹ˆà¸¡ Wait State:** à¸–à¹‰à¸² File Size > 1 MB à¹ƒà¸«à¹‰à¸£à¸­ 30 à¸§à¸´à¸™à¸²à¸—à¸µà¸à¹ˆà¸­à¸™ Transform (à¸ˆà¸³à¸¥à¸­à¸‡ Rate Limiting)
3. **Map State:** à¸–à¹‰à¸² CSV à¸¡à¸µà¸«à¸¥à¸²à¸¢ Products à¹ƒà¸«à¹‰à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹à¸•à¹ˆà¸¥à¸° Product à¹à¸šà¸š Parallel à¸”à¹‰à¸§à¸¢ Map State

### â“ à¸„à¸³à¸–à¸²à¸¡à¸—à¹‰à¸²à¸¢ LAB Week 3

1. Standard Workflow à¸à¸±à¸š Express Workflow à¹ƒà¸™ Step Functions à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™à¸¢à¸±à¸‡à¹„à¸‡? à¹€à¸¥à¸·à¸­à¸à¹ƒà¸Šà¹‰à¸­à¸°à¹„à¸£à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸«à¸£à¹ˆ?
2. Retry à¹ƒà¸™ ASL à¸—à¸³à¸‡à¸²à¸™à¸¢à¸±à¸‡à¹„à¸‡? `BackoffRate: 2` à¸«à¸¡à¸²à¸¢à¸„à¸§à¸²à¸¡à¸§à¹ˆà¸²à¸­à¸°à¹„à¸£?
3. à¸–à¹‰à¸² Execution à¸«à¸™à¸¶à¹ˆà¸‡à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸²à¸™à¸²à¸™à¸¡à¸²à¸ à¸ˆà¸° Cancel à¹„à¸”à¹‰à¸¢à¸±à¸‡à¹„à¸‡? à¸¡à¸µ Side Effects à¸­à¸°à¹„à¸£à¸šà¹‰à¸²à¸‡?

---

---

# ğŸ“… WEEK 4 â€” Notification & Recap

> **à¸«à¸±à¸§à¸‚à¹‰à¸­:** à¹€à¸à¸´à¹ˆà¸¡ SNS à¹à¸ˆà¹‰à¸‡à¹€à¸•à¸·à¸­à¸™ + CloudWatch Dashboard + à¸ªà¸£à¸¸à¸› Architecture à¹à¸¥à¸° Cost

---

## ğŸ“ à¸§à¸±à¸•à¸–à¸¸à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰
- à¸ªà¸£à¹‰à¸²à¸‡ SNS Topic à¹à¸¥à¸°à¸ªà¹ˆà¸‡ Email/SMS Notification
- à¹€à¸Šà¸·à¹ˆà¸­à¸¡ SNS à¹€à¸‚à¹‰à¸²à¸à¸±à¸š Step Functions (à¸ˆà¸š Pipeline à¸”à¹‰à¸§à¸¢à¸à¸²à¸£à¹à¸ˆà¹‰à¸‡à¹€à¸•à¸·à¸­à¸™)
- à¸ªà¸£à¹‰à¸²à¸‡ CloudWatch Dashboard à¹à¸ªà¸”à¸‡ Metrics à¸‚à¸­à¸‡à¸—à¸¸à¸ Service
- à¸•à¸±à¹‰à¸‡ CloudWatch Alarm à¹à¸ˆà¹‰à¸‡à¹€à¸•à¸·à¸­à¸™à¹€à¸¡à¸·à¹ˆà¸­à¸¡à¸µ Pipeline Failure
- à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ Architecture à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹à¸¥à¸° Cost Estimation

---

## ğŸ“– à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸à¸·à¹‰à¸™à¸à¸²à¸™ (Lecture â€” 20 à¸™à¸²à¸—à¸µ)

### Amazon SNS (Simple Notification Service) à¸„à¸·à¸­à¸­à¸°à¹„à¸£?

SNS à¹€à¸›à¹‡à¸™ Pub/Sub Messaging Service â€” à¸œà¸¹à¹‰à¸ªà¹ˆà¸‡ (Publisher) à¸ªà¹ˆà¸‡ Message à¹„à¸›à¸¢à¸±à¸‡ Topic à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¸§à¹ˆà¸²à¹ƒà¸„à¸£à¸£à¸±à¸š à¸œà¸¹à¹‰à¸£à¸±à¸š (Subscriber) à¸£à¸±à¸š Message à¸ˆà¸²à¸ Topic à¸—à¸µà¹ˆ Subscribe à¹„à¸§à¹‰

```
[Lambda à¸ªà¹ˆà¸‡à¸œà¸¥] â”€â”€publishâ”€â”€> [SNS Topic: pipeline-notifications]
                                          â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â–¼                â–¼                â–¼
                    [Email Sub]     [SMS Sub]      [Lambda Sub]
                  (à¸ªà¹ˆà¸‡ Email)      (à¸ªà¹ˆà¸‡ SMS)     (trigger à¸­à¸·à¹ˆà¸™)
```

**SNS vs SQS:**
| | SNS | SQS |
|---|---|---|
| Pattern | Push (Pub/Sub) | Pull (Queue) |
| Delivery | Fan-out (à¸—à¸¸à¸ Subscriber) | One consumer per message |
| Retention | à¹„à¸¡à¹ˆà¹€à¸à¹‡à¸š | à¹€à¸à¹‡à¸šà¸•à¸²à¸¡à¸—à¸µà¹ˆà¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² |
| Use case | Notification, Fan-out | Work Queue, Decoupling |

### Architecture à¸—à¸µà¹ˆà¸ˆà¸°à¸ªà¸£à¹‰à¸²à¸‡à¹ƒà¸™ Week 4

```
[Step Functions: TransformData] 
              â”‚
              â–¼
     [Lambda: send-notification]
              â”‚
              â–¼
       [SNS Topic: pipeline-notifications]
         â”‚              â”‚
         â–¼              â–¼
      [Email]         [SQS]  â† à¸ªà¸³à¸«à¸£à¸±à¸š Integration à¸­à¸·à¹ˆà¸™à¹† à¹ƒà¸™à¸­à¸™à¸²à¸„à¸•

[CloudWatch] â”€â”€â”€â”€ Monitor â”€â”€â”€â”€> à¸—à¸¸à¸ Service à¹ƒà¸™ Pipeline
                                 + Dashboard + Alarms
```

---

## ğŸ”§ LAB Step-by-Step

---

### âœ… STEP 1 â€” à¸ªà¸£à¹‰à¸²à¸‡ SNS Topic

1. à¹€à¸‚à¹‰à¸² **SNS** â†’ **Topics** â†’ **Create topic**
2. à¸à¸£à¸­à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:
   ```
   Type: Standard
   Name: pipeline-notifications
   Display name: CSV Pipeline
   ```
3. à¸à¸” **Create topic**
4. à¸ˆà¸” **Topic ARN** à¹„à¸§à¹‰ (à¸£à¸¹à¸›à¹à¸šà¸š: `arn:aws:sns:ap-southeast-1:...:pipeline-notifications`)

**à¸ªà¸£à¹‰à¸²à¸‡ Email Subscription:**
1. à¸à¸” Topic à¸—à¸µà¹ˆà¸ªà¸£à¹‰à¸²à¸‡ â†’ **Create subscription**
2. à¸à¸£à¸­à¸:
   ```
   Protocol: Email
   Endpoint: [à¹ƒà¸ªà¹ˆ Email à¸‚à¸­à¸‡à¸„à¸¸à¸“]
   ```
3. à¸à¸” **Create subscription**
4. à¹€à¸Šà¹‡à¸„ Email â†’ à¸à¸” **Confirm subscription** à¹ƒà¸™ Email à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸£à¸±à¸š
5. à¸à¸¥à¸±à¸šà¸¡à¸²à¸—à¸µà¹ˆ SNS Console â†’ Status à¸„à¸§à¸£à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹€à¸›à¹‡à¸™ `Confirmed`

---

### âœ… STEP 2 â€” à¸­à¸±à¸›à¹€à¸”à¸• IAM Policy à¹€à¸à¸´à¹ˆà¸¡ SNS

à¹€à¸à¸´à¹ˆà¸¡à¹ƒà¸™ `pipeline-lambda-policy`:

```json
{
  "Sid": "SNSPublish",
  "Effect": "Allow",
  "Action": ["sns:Publish"],
  "Resource": "arn:aws:sns:ap-southeast-1:[ACCOUNT-ID]:pipeline-notifications"
}
```

---

### âœ… STEP 3 â€” à¸ªà¸£à¹‰à¸²à¸‡ Lambda Notification

1. **Lambda** â†’ **Create function** â†’ `send-notification` â†’ Python 3.12 â†’ `pipeline-lambda-role`

```python
import json
import boto3
import logging
from datetime import datetime

logger = logging.getLogger()
logger.setLevel(logging.INFO)

sns = boto3.client('sns', region_name='ap-southeast-1')

SNS_TOPIC_ARN = 'arn:aws:sns:ap-southeast-1:[ACCOUNT-ID]:pipeline-notifications'

def lambda_handler(event, context):
    """
    à¸ªà¹ˆà¸‡ Notification à¹€à¸¡à¸·à¹ˆà¸­ Pipeline à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ (à¸ªà¸³à¹€à¸£à¹‡à¸ˆà¸«à¸£à¸·à¸­à¸¥à¹‰à¸¡à¹€à¸«à¸¥à¸§)
    """
    job_id = event.get('job_id', 'unknown')
    status = event.get('transform_status', event.get('move_status', 'unknown'))
    source_key = event.get('source_key', 'unknown')
    summary = event.get('summary', {})
    
    is_success = status == 'success'
    
    # à¸ªà¸£à¹‰à¸²à¸‡ Message
    if is_success:
        subject = f"âœ… Pipeline à¸ªà¸³à¹€à¸£à¹‡à¸ˆ â€” {source_key.split('/')[-1]}"
        message = build_success_message(job_id, source_key, summary, event)
    else:
        subject = f"âŒ Pipeline à¸¥à¹‰à¸¡à¹€à¸«à¸¥à¸§ â€” {source_key.split('/')[-1]}"
        message = build_failure_message(job_id, source_key, event)
    
    # à¸ªà¹ˆà¸‡ SNS
    response = sns.publish(
        TopicArn=SNS_TOPIC_ARN,
        Subject=subject[:100],  # Subject à¸ˆà¸³à¸à¸±à¸” 100 à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£
        Message=message,
        MessageAttributes={
            'job_id': {'DataType': 'String', 'StringValue': job_id},
            'status': {'DataType': 'String', 'StringValue': 'success' if is_success else 'failed'}
        }
    )
    
    logger.info(json.dumps({
        "event": "notification_sent",
        "job_id": job_id,
        "status": "success" if is_success else "failed",
        "message_id": response['MessageId']
    }))
    
    return {
        **event,
        "notification_status": "sent",
        "notification_message_id": response['MessageId']
    }


def build_success_message(job_id, source_key, summary, event):
    filename = source_key.split('/')[-1]
    total_rows = summary.get('total_rows', event.get('row_count', 0))
    total_revenue = summary.get('total_revenue', 0)
    output_key = event.get('output_key', 'N/A')
    
    return f"""
ğŸ‰ CSV Pipeline à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!
{'='*50}

ğŸ“ à¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥: {filename}
ğŸ†” Job ID: {job_id}
â° à¹€à¸ªà¸£à¹‡à¸ˆà¹€à¸¡à¸·à¹ˆà¸­: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC

ğŸ“Š à¸ªà¸£à¸¸à¸›à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ:
   - à¸ˆà¸³à¸™à¸§à¸™à¹à¸–à¸§: {total_rows:,} à¹à¸–à¸§
   - à¸£à¸²à¸¢à¹„à¸”à¹‰à¸£à¸§à¸¡: {total_revenue:,.2f} à¸šà¸²à¸—

ğŸ’¾ à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸šà¸±à¸™à¸—à¸¶à¸à¸—à¸µà¹ˆ:
   {output_key}

---
AWS Data Pipeline Notification
    """.strip()


def build_failure_message(job_id, source_key, event):
    filename = source_key.split('/')[-1]
    missing_cols = event.get('missing_columns', [])
    failed_key = event.get('failed_key', 'N/A')
    
    return f"""
âš ï¸ CSV Pipeline à¸à¸šà¸›à¸±à¸à¸«à¸²

ğŸ“ à¹„à¸Ÿà¸¥à¹Œ: {filename}
ğŸ†” Job ID: {job_id}
â° à¹€à¸§à¸¥à¸²: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC

âŒ à¸ªà¸²à¹€à¸«à¸•à¸¸: Schema à¹„à¸¡à¹ˆà¸•à¸£à¸‡à¸•à¸²à¸¡à¸—à¸µà¹ˆà¸à¸³à¸«à¸™à¸”
   Column à¸—à¸µà¹ˆà¸‚à¸²à¸”à¸«à¸²à¸¢: {', '.join(missing_cols) if missing_cols else 'N/A'}

ğŸ“‚ à¹„à¸Ÿà¸¥à¹Œà¸–à¸¹à¸à¸¢à¹‰à¸²à¸¢à¹„à¸›à¸—à¸µà¹ˆ: {failed_key}

---
à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹à¸¥à¸°à¹à¸à¹‰à¹„à¸‚à¹„à¸Ÿà¸¥à¹Œà¸à¹ˆà¸­à¸™à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹ƒà¸«à¸¡à¹ˆ
AWS Data Pipeline Notification
    """.strip()
```

à¸à¸” **Deploy**

---

### âœ… STEP 4 â€” à¸­à¸±à¸›à¹€à¸”à¸• State Machine à¹€à¸à¸´à¹ˆà¸¡ Notification Step

à¹à¸à¹‰à¹„à¸‚ State Machine `ProcessCSVWorkflow` â€” à¹€à¸à¸´à¹ˆà¸¡ `SendNotification` à¹à¸¥à¸° `SendFailureNotification`:

1. à¹€à¸‚à¹‰à¸² **Step Functions** â†’ `ProcessCSVWorkflow` â†’ **Edit**
2. à¹à¸—à¸™à¸—à¸µà¹ˆ ASL à¸”à¹‰à¸§à¸¢à¹€à¸§à¸­à¸£à¹Œà¸Šà¸±à¸™à¹ƒà¸«à¸¡à¹ˆ:

```json
{
  "Comment": "CSV Processing Pipeline â€” Week 4 (with Notifications)",
  "StartAt": "ParseCSV",
  "States": {
    "ParseCSV": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:parse-csv",
      "TimeoutSeconds": 60,
      "Retry": [
        {
          "ErrorEquals": ["Lambda.ServiceException", "Lambda.AWSLambdaException"],
          "IntervalSeconds": 2,
          "MaxAttempts": 3,
          "BackoffRate": 2
        }
      ],
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError",
          "ResultPath": "$.error"
        }
      ],
      "Next": "ValidateSchema"
    },

    "ValidateSchema": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:validate-schema",
      "TimeoutSeconds": 30,
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError",
          "ResultPath": "$.error"
        }
      ],
      "Next": "CheckValidation"
    },

    "CheckValidation": {
      "Type": "Choice",
      "Choices": [
        {
          "Variable": "$.schema_valid",
          "BooleanEquals": true,
          "Next": "TransformData"
        },
        {
          "Variable": "$.schema_valid",
          "BooleanEquals": false,
          "Next": "MoveToFailed"
        }
      ],
      "Default": "HandleError"
    },

    "TransformData": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:transform-data",
      "TimeoutSeconds": 120,
      "Retry": [
        {
          "ErrorEquals": ["States.ALL"],
          "IntervalSeconds": 5,
          "MaxAttempts": 2,
          "BackoffRate": 2
        }
      ],
      "Catch": [
        {
          "ErrorEquals": ["States.ALL"],
          "Next": "HandleError",
          "ResultPath": "$.error"
        }
      ],
      "Next": "SendSuccessNotification"
    },

    "SendSuccessNotification": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:send-notification",
      "TimeoutSeconds": 30,
      "Next": "ProcessingComplete"
    },

    "MoveToFailed": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:move-to-failed",
      "TimeoutSeconds": 30,
      "Next": "SendFailureNotification"
    },

    "SendFailureNotification": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:ap-southeast-1:[ACCOUNT-ID]:function:send-notification",
      "TimeoutSeconds": 30,
      "Next": "ValidationFailed"
    },

    "ProcessingComplete": {
      "Type": "Succeed"
    },

    "ValidationFailed": {
      "Type": "Fail",
      "Error": "ValidationFailed",
      "Cause": "Schema validation failed"
    },

    "HandleError": {
      "Type": "Fail",
      "Error": "ProcessingError",
      "Cause": "Unexpected error during pipeline processing"
    }
  }
}
```

3. à¸à¸” **Save** â†’ **Save anyway** (à¸–à¹‰à¸²à¸¡à¸µ warning)

---

### âœ… STEP 5 â€” à¸ªà¸£à¹‰à¸²à¸‡ CloudWatch Dashboard

1. à¹€à¸‚à¹‰à¸² **CloudWatch** â†’ **Dashboards** â†’ **Create dashboard**
2. Name: `csv-pipeline-dashboard` â†’ **Create dashboard**
3. à¹€à¸à¸´à¹ˆà¸¡ Widgets à¸•à¸²à¸¡à¸¥à¸³à¸”à¸±à¸š:

**Widget 1: Lambda Invocations Overview (Line Chart)**
- Add widget â†’ Line
- Metric: Lambda â†’ By Function Name
- à¹€à¸¥à¸·à¸­à¸: `csv-validator`, `csv-processor`, `parse-csv`, `transform-data`, `send-notification`
- Metric name: Invocations
- Title: "Lambda Invocations"

**Widget 2: Lambda Errors (Line Chart)**
- à¹€à¸«à¸¡à¸·à¸­à¸™à¸à¸±à¸™ à¹à¸•à¹ˆ Metric: Errors
- Title: "Lambda Errors"

**Widget 3: SQS Queue Depth (Line Chart)**
- Metric: SQS â†’ ApproximateNumberOfMessagesVisible
- Queue: pipeline-main-queue
- Title: "SQS Queue Depth"

**Widget 4: DLQ Messages (Number)**
- Metric: SQS â†’ ApproximateNumberOfMessagesVisible
- Queue: pipeline-dlq
- Title: "Dead Letter Queue"

**Widget 5: Step Functions Executions (Line Chart)**
- Metric: Step Functions â†’ ExecutionsStarted, ExecutionsSucceeded, ExecutionsFailed
- Title: "Step Functions Executions"

4. à¸à¸” **Save dashboard**

---

### âœ… STEP 6 â€” à¸•à¸±à¹‰à¸‡ CloudWatch Alarms

**Alarm 1: Lambda Error Rate à¸ªà¸¹à¸‡**

1. **CloudWatch** â†’ **Alarms** â†’ **Create alarm**
2. Select metric â†’ Lambda â†’ By Function Name â†’ `transform-data` â†’ Errors
3. à¸à¸³à¸«à¸™à¸”:
   ```
   Statistic: Sum
   Period: 5 minutes
   Threshold: Greater than 2
   Datapoints to alarm: 1 out of 1
   ```
4. Notification: à¸ªà¹ˆà¸‡à¹„à¸› SNS Topic `pipeline-notifications`
5. Alarm name: `pipeline-transform-errors`

**Alarm 2: DLQ à¸¡à¸µ Messages**

1. Create alarm â†’ SQS â†’ `pipeline-dlq` â†’ ApproximateNumberOfMessagesVisible
2. à¸à¸³à¸«à¸™à¸”:
   ```
   Statistic: Maximum
   Period: 1 minute  
   Threshold: Greater than 0
   ```
3. Notification: à¸ªà¹ˆà¸‡à¹„à¸› SNS Topic à¹€à¸”à¸´à¸¡
4. Alarm name: `pipeline-dlq-messages`

**Alarm 3: Step Functions Failures**

1. Create alarm â†’ Step Functions â†’ `ProcessCSVWorkflow` â†’ ExecutionsFailed
2. à¸à¸³à¸«à¸™à¸”:
   ```
   Statistic: Sum
   Period: 5 minutes
   Threshold: Greater than 0
   ```
3. Alarm name: `pipeline-execution-failures`

---

### âœ… STEP 7 â€” End-to-End Test à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ

à¸—à¸”à¸ªà¸­à¸š Full Pipeline à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”:

**Test Case 1: Happy Path (à¹„à¸Ÿà¸¥à¹Œà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡)**
1. à¸­à¸±à¸›à¹‚à¸«à¸¥à¸” `sample_sales.csv` à¹„à¸›à¸—à¸µà¹ˆ `uploads/`
2. à¸£à¸­ ~30 à¸§à¸´à¸™à¸²à¸—à¸µ
3. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š:
   - [ ] CloudWatch Logs: `csv-validator` â†’ queued
   - [ ] CloudWatch Logs: `csv-processor` â†’ execution started
   - [ ] Step Functions: Execution à¸ªà¸³à¹€à¸£à¹‡à¸ˆ (à¸ªà¸µà¹€à¸‚à¸µà¸¢à¸§à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”)
   - [ ] S3 `processed/`: à¸¡à¸µà¹„à¸Ÿà¸¥à¹Œ `_result.json`
   - [ ] Email: à¹„à¸”à¹‰à¸£à¸±à¸š Notification âœ…

**Test Case 2: Schema Error**
1. à¸­à¸±à¸›à¹‚à¸«à¸¥à¸” CSV à¸—à¸µà¹ˆà¸‚à¸²à¸” Column
2. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š:
   - [ ] Step Functions: à¹„à¸›à¹€à¸ªà¹‰à¸™ `MoveToFailed`
   - [ ] S3: à¹„à¸Ÿà¸¥à¹Œà¸–à¸¹à¸à¸¢à¹‰à¸²à¸¢à¹„à¸› `failed/`
   - [ ] Email: à¹„à¸”à¹‰à¸£à¸±à¸š Notification âŒ à¸à¸£à¹‰à¸­à¸¡à¸ªà¸²à¹€à¸«à¸•à¸¸

**Test Case 3: à¸—à¸”à¸ªà¸­à¸š DLQ**
1. à¸—à¸³à¹ƒà¸«à¹‰ `parse-csv` Lambda Error à¸Šà¸±à¹ˆà¸§à¸„à¸£à¸²à¸§
2. à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ
3. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š:
   - [ ] SQS DLQ: à¸¡à¸µ Message
   - [ ] CloudWatch Alarm: à¸–à¸¹à¸ Trigger
   - [ ] Email Alert: à¹„à¸”à¹‰à¸£à¸±à¸š Alarm notification

---

### ğŸ“ à¹à¸šà¸šà¸à¸¶à¸à¸«à¸±à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡ Week 4

1. **à¹€à¸à¸´à¹ˆà¸¡ SMS Notification:** à¸ªà¸£à¹‰à¸²à¸‡ SNS Subscription à¹à¸šà¸š SMS (à¹ƒà¸Šà¹‰à¹€à¸šà¸­à¸£à¹Œà¸¡à¸·à¸­à¸–à¸·à¸­à¸ˆà¸£à¸´à¸‡)
2. **à¸ªà¸£à¹‰à¸²à¸‡ CloudWatch Metric Filter:** à¸”à¸¶à¸‡ Custom Metric à¸ˆà¸²à¸ Log à¹€à¸Šà¹ˆà¸™ à¸™à¸±à¸š `"event": "file_validated"` à¸•à¹ˆà¸­à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡
3. **à¹€à¸à¸´à¹ˆà¸¡ S3 Lifecycle Policy:** à¸•à¸±à¹‰à¸‡à¹ƒà¸«à¹‰à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸™ `failed/` à¸–à¸¹à¸à¸¥à¸šà¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´à¸«à¸¥à¸±à¸‡ 30 à¸§à¸±à¸™

### â“ à¸„à¸³à¸–à¸²à¸¡à¸—à¹‰à¸²à¸¢ LAB Week 4

1. SNS Subscription à¹à¸šà¸š Email à¸à¸±à¸šà¹à¸šà¸š SQS à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™à¸¢à¸±à¸‡à¹„à¸‡à¹ƒà¸™à¹à¸‡à¹ˆ Reliability? à¸­à¸°à¹„à¸£à¹€à¸à¸´à¸”à¸‚à¸¶à¹‰à¸™à¸–à¹‰à¸² Email Server Down?
2. CloudWatch Alarm à¸¡à¸µà¸à¸µà¹ˆ State? à¹à¸•à¹ˆà¸¥à¸° State à¸«à¸¡à¸²à¸¢à¸„à¸§à¸²à¸¡à¸§à¹ˆà¸²à¸­à¸°à¹„à¸£?
3. à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ Monitor Latency à¸‚à¸­à¸‡ Pipeline à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” (à¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆ Upload à¸ˆà¸™à¸–à¸¶à¸‡ Email à¸ªà¹ˆà¸‡) à¸ˆà¸°à¸§à¸±à¸”à¹„à¸”à¹‰à¸¢à¸±à¸‡à¹„à¸‡?

---

---

# ğŸ—ï¸ Final Architecture & Summary

---

## Architecture Diagram (à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS Data Pipeline Architecture                â”‚
â”‚                      (All Serverless, Free Tier)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[User]
  â”‚  Upload CSV
  â–¼
[S3: raw-uploads/]                         [CloudWatch]
  â”‚  ObjectCreated Event                        â”‚
  â”‚                                    Monitors everything
  â–¼                                             â”‚
[Lambda: csv-validator]  â”€â”€Logsâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚
  â”‚  Validate + Send Message                    â”‚
  â–¼                                             â”‚
[SQS: pipeline-main-queue]  â”€Metricsâ”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚
  â”‚  (DLQ: pipeline-dlq)                        â”‚
  â”‚  SQS Trigger                                â”‚
  â–¼                                             â”‚
[Lambda: csv-processor]  â”€â”€Logsâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚
  â”‚  Start Execution                            â”‚
  â–¼                                             â”‚
[Step Functions: ProcessCSVWorkflow]  â”€â”€â”€â”€â”€> Execution Metrics
  â”‚
  â”œâ”€[Lambda: parse-csv]
  â”‚
  â”œâ”€[Lambda: validate-schema]
  â”‚         â”‚
  â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
  â”‚  Valid    Invalid
  â”‚    â”‚         â”‚
  â”‚    â–¼         â–¼
  â”‚ [Lambda:  [Lambda:
  â”‚  transform] move-to-failed]
  â”‚    â”‚         â”‚
  â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
  â”‚         â”‚
  â–¼         â–¼
[Lambda: send-notification]
  â”‚
  â–¼
[SNS: pipeline-notifications]
  â”‚              â”‚
  â–¼              â–¼
[Email]       [SMS / Lambda / SQS ...]

Output: [S3: processed/YYYY/MM/DD/*.json]
Failed: [S3: raw-uploads/failed/YYYY/MM/DD/*.csv]
```

---

## Services à¸ªà¸£à¸¸à¸›à¸—à¸±à¹‰à¸‡ 4 à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ

| AWS Service | à¹ƒà¸Šà¹‰à¸—à¸³à¸­à¸°à¹„à¸£ | à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ | Free Tier |
|-------------|-----------|---------|-----------|
| S3 | à¸£à¸±à¸šà¹„à¸Ÿà¸¥à¹Œà¸”à¸´à¸š, à¹€à¸à¹‡à¸šà¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ | Week 1 | 5 GB / à¹€à¸”à¸·à¸­à¸™ |
| Lambda | Business Logic à¸—à¸¸à¸ Step | Week 1â€“4 | 1M invocations / à¹€à¸”à¸·à¸­à¸™ |
| IAM | à¸ˆà¸±à¸”à¸à¸²à¸£à¸ªà¸´à¸—à¸˜à¸´à¹Œ (Least Privilege) | Week 1 | à¸Ÿà¸£à¸µà¹€à¸ªà¸¡à¸­ |
| SQS | Decouple + Queue | Week 2 | 1M requests / à¹€à¸”à¸·à¸­à¸™ |
| Step Functions | Orchestrate Workflow | Week 3 | 4,000 transitions / à¹€à¸”à¸·à¸­à¸™ |
| SNS | Notification | Week 4 | 1M publishes / à¹€à¸”à¸·à¸­à¸™ |
| CloudWatch | Monitor + Alert | Week 4 | 5 GB logs / à¹€à¸”à¸·à¸­à¸™ |

---

## Cost Estimation (à¸«à¸¥à¸±à¸‡ Free Tier à¸«à¸¡à¸”)

à¸ªà¸¡à¸¡à¸•à¸´ Pipeline à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸Ÿà¸¥à¹Œ 100 à¹„à¸Ÿà¸¥à¹Œ/à¸§à¸±à¸™:

| Service | Estimated Usage | Estimated Cost |
|---------|----------------|----------------|
| S3 | 1 GB storage + transfers | ~$0.02/à¹€à¸”à¸·à¸­à¸™ |
| Lambda | 5 functions Ã— 100 files Ã— 30 days = 15,000 invocations | à¸Ÿà¸£à¸µ (à¸•à¹ˆà¸³à¸à¸§à¹ˆà¸² 1M) |
| SQS | 100 messages/à¸§à¸±à¸™ = 3,000/à¹€à¸”à¸·à¸­à¸™ | à¸Ÿà¸£à¸µ (à¸•à¹ˆà¸³à¸à¸§à¹ˆà¸² 1M) |
| Step Functions | 100 executions Ã— 7 transitions = 700/à¹€à¸”à¸·à¸­à¸™ | à¸Ÿà¸£à¸µ (à¸•à¹ˆà¸³à¸à¸§à¹ˆà¸² 4,000) |
| SNS | 100 emails/à¸§à¸±à¸™ = 3,000/à¹€à¸”à¸·à¸­à¸™ | à¸Ÿà¸£à¸µ (à¸•à¹ˆà¸³à¸à¸§à¹ˆà¸² 1,000 â€” à¸–à¹‰à¸²à¹€à¸à¸´à¸™ ~$0.002/notification) |
| CloudWatch | Logs + Metrics | ~$0.50/à¹€à¸”à¸·à¸­à¸™ |
| **à¸£à¸§à¸¡** | | **< $1/à¹€à¸”à¸·à¸­à¸™** |

> ğŸ’° Pipeline à¹à¸šà¸šà¸™à¸µà¹‰à¸–à¸¹à¸à¸¡à¸²à¸à¹€à¸à¸£à¸²à¸° Serverless â€” à¸ˆà¹ˆà¸²à¸¢à¸•à¸²à¸¡à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸ˆà¸£à¸´à¸‡ à¹„à¸¡à¹ˆà¸¡à¸µà¸„à¹ˆà¸² Server à¸•à¸¥à¸­à¸”à¹€à¸§à¸¥à¸²

---

## Design Patterns à¸—à¸µà¹ˆà¹€à¸£à¸µà¸¢à¸™à¸¡à¸²à¸•à¸¥à¸­à¸” 4 à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ

### 1. Event-Driven Pattern (Week 1)
```
Trigger â”€â”€> Function â”€â”€> Output
```
à¹ƒà¸Šà¹‰à¹€à¸¡à¸·à¹ˆà¸­: à¸•à¹‰à¸­à¸‡à¸•à¸­à¸šà¸ªà¸™à¸­à¸‡à¸•à¹ˆà¸­à¹€à¸«à¸•à¸¸à¸à¸²à¸£à¸“à¹Œà¸—à¸µà¹ˆà¹€à¸à¸´à¸”à¸‚à¸¶à¹‰à¸™à¹‚à¸”à¸¢à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´

### 2. Message Queue Pattern (Week 2)
```
Producer â”€â”€> Queue â”€â”€> Consumer
```
à¹ƒà¸Šà¹‰à¹€à¸¡à¸·à¹ˆà¸­: à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ Decouple à¸£à¸°à¸šà¸š à¸«à¸£à¸·à¸­à¸ˆà¸±à¸”à¸à¸²à¸£ Load à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸ªà¸¡à¹ˆà¸³à¹€à¸ªà¸¡à¸­

### 3. Orchestration Pattern (Week 3)
```
Orchestrator â”€â”€> Step A â”€â”€> Step B â”€â”€> Step C
```
à¹ƒà¸Šà¹‰à¹€à¸¡à¸·à¹ˆà¸­: à¸¡à¸µ Workflow à¸«à¸¥à¸²à¸¢à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆà¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™ à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ Error Handling à¸£à¸§à¸¡à¸¨à¸¹à¸™à¸¢à¹Œ

### 4. Fan-out Notification Pattern (Week 4)
```
Event â”€â”€> SNS â”€â”€> [Email, SMS, Lambda, SQS, ...]
```
à¹ƒà¸Šà¹‰à¹€à¸¡à¸·à¹ˆà¸­: à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹à¸ˆà¹‰à¸‡à¹€à¸•à¸·à¸­à¸™à¸«à¸¥à¸²à¸¢ Channel à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™

---

## ğŸš€ Next Steps â€” à¸•à¹ˆà¸­à¸¢à¸­à¸”à¸ˆà¸²à¸à¸—à¸µà¹ˆà¹€à¸£à¸µà¸¢à¸™

- **EventBridge:** à¹à¸—à¸™ S3 Events à¸”à¹‰à¸§à¸¢ EventBridge Rules à¸ªà¸³à¸«à¸£à¸±à¸š Multi-Source Events
- **Kinesis Data Streams:** à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¸š Real-time Stream (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆ File-based)
- **Glue + Athena:** Query à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸™ S3 à¸”à¹‰à¸§à¸¢ SQL à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ Load à¹€à¸‚à¹‰à¸² Database
- **DynamoDB:** à¹€à¸à¹‡à¸š Job Status à¹à¸¥à¸° Metadata à¹ƒà¸«à¹‰ Query à¹„à¸”à¹‰
- **API Gateway:** à¹€à¸à¸´à¹ˆà¸¡ REST API à¹€à¸à¸·à¹ˆà¸­ Trigger Pipeline à¸«à¸£à¸·à¸­à¸”à¸¹à¸ªà¸–à¸²à¸™à¸° Job
- **CDK (Cloud Development Kit):** à¹€à¸‚à¸µà¸¢à¸™ Infrastructure à¹€à¸›à¹‡à¸™ Code à¹à¸—à¸™à¸à¸²à¸£à¸„à¸¥à¸´à¸ Console
- **X-Ray:** Distributed Tracing à¸”à¸¹ Latency à¸‚à¸­à¸‡à¹à¸•à¹ˆà¸¥à¸° Step

---

## ğŸ† à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸—à¸³à¹„à¸”à¹‰à¹à¸¥à¹‰à¸§à¸«à¸¥à¸±à¸‡à¹€à¸£à¸µà¸¢à¸™à¸„à¸£à¸š 4 à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ

- âœ… à¸¡à¸µ Data Pipeline à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´ 100% Serverless â€” à¹„à¸¡à¹ˆà¸¡à¸µ Server à¸”à¸¹à¹à¸¥à¹€à¸¥à¸¢
- âœ… à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆ 4 Design Patterns à¸ªà¸³à¸„à¸±à¸à¹ƒà¸™à¸£à¸°à¸šà¸š Distributed
- âœ… à¹€à¸‚à¸µà¸¢à¸™ Lambda Python à¸—à¸µà¹ˆà¸¡à¸µ Structured Logging à¹à¸¥à¸° Error Handling
- âœ… à¸­à¸­à¸à¹à¸šà¸š IAM Policy à¹à¸šà¸š Least Privilege à¹„à¸”à¹‰
- âœ… Debug à¸”à¹‰à¸§à¸¢ CloudWatch Logs à¹à¸¥à¸° Step Functions Visual Execution
- âœ… à¸à¸£à¹‰à¸­à¸¡à¸ªà¸­à¸š **AWS Cloud Practitioner** à¸«à¸£à¸·à¸­ **AWS Solutions Architect Associate**

---

*à¹€à¸­à¸à¸ªà¸²à¸£à¸™à¸µà¹‰à¸ˆà¸±à¸”à¸—à¸³à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸à¸²à¸£à¸ªà¸­à¸™ AWS Cloud Â· Free Tier Â· 4 à¸ªà¸±à¸›à¸”à¸²à¸«à¹Œ*
